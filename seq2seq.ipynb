{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seq2seq.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "FnS-GXJOJOY2",
        "colab_type": "text"
      },
      "source": [
        "Tensorflow 1.4.0 is required.\n",
        "This is based on [NMT Tutorial](https://github.com/tensorflow/nmt)."
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {
        "id": "caxRbRVkDdhp",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import copy\n",
        "from __future__ import print_function\n",
        "from tensorflow.python.layers import core as layers_core\n",
        "from tensorflow.python.platform import gfile\n",
        "import matplotlib.pyplot as plt\n",
        "from random import randint\n",
        "\n",
        "## Note for me. You've summarized Seq2Seq at http://d.hatena.ne.jp/higepon/20171210/1512887715."
      ],
      "cell_type": "code",
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g-ZP7_08WtPv",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "source": [
        "tf.__version__\n",
        "!mkdir \"./saved_model2\"\n",
        "!mkdir \"./saved_model\"\n",
        "\n",
        "\n",
        "!ls -la ./saved_model2\n",
        "\n",
        "\n",
        "# TODO\n",
        "# inferernce „ÅØ batch „Åò„ÇÉ„Å™„Åè„Å¶ËâØ„ÅÑ„ÅÆ„ÅßË®àÁÆóÈáèÊ∏õ„Çâ„Åõ„Çã"
      ],
      "cell_type": "code",
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C16WwvYwGVBm",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "source": [
        "# TODO\n",
        "# Make 2 models possible\n",
        "# Make them as methods\n",
        "# Change first part to use small hparams for debug\n",
        "# We could pass sequence_length to tf.nn.dynamic_rnn for better performance.\n",
        "# Maybe extract hparams\n",
        "# „É¢„Éá„É´ „Å©„ÅÜ„ÇÑ„Å£„Å¶ reload „Åô„Çã„ÅãÔºü\n",
        "# support beam infer\n",
        "# attention default\n",
        "# model_path „Å´„Çà„Çã reload „ÅåÂãï„ÅÑ„Å¶Âæó„Å™„ÅÑ„ÄÇ\n",
        "\n",
        "test_hparams = tf.contrib.training.HParams(\n",
        "    batch_size=3,\n",
        "    encoder_length=5,\n",
        "    decoder_length=5,\n",
        "    num_units=6,\n",
        "    vocab_size=9,\n",
        "    embedding_size=8,\n",
        "    learning_rate = 0.01,\n",
        "    max_gradient_norm = 5.0,\n",
        "    beam_width =9,\n",
        "    use_attention = False,\n",
        "    num_train_steps = 150,\n",
        "    debug_verbose = False\n",
        ")\n",
        "\n",
        "test_attention_hparams = copy.deepcopy(test_hparams)\n",
        "test_attention_hparams.use_attention = True\n",
        "\n",
        "real_hparams = tf.contrib.training.HParams(\n",
        "    batch_size=25, # of tweets should be devidable by batch_size\n",
        "    encoder_length=20, \n",
        "    decoder_length=20,\n",
        "    num_units=1024,\n",
        "    vocab_size=500,\n",
        "    embedding_size=256,\n",
        "    learning_rate = 0.01,\n",
        "    max_gradient_norm = 5.0,\n",
        "    beam_width =9,\n",
        "    use_attention = False,\n",
        "    num_train_steps = 100,\n",
        "    debug_verbose = True\n",
        ")\n",
        "\n",
        "# Model path\n",
        "model_path = \"./saved_model/twitter\"\n",
        "\n",
        "# Symbol for start decode process.\n",
        "tgt_sos_id = 0\n",
        "\n",
        "# Symbol for end of decode process.\n",
        "tgt_eos_id = 1\n",
        "\n",
        "pad_id = 2\n",
        "\n",
        "unk_id = 3"
      ],
      "cell_type": "code",
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DFEYKvBwL3Nm",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "source": [
        "\n",
        "# For debug purpose.\n",
        "tf.reset_default_graph()\n",
        "\n",
        "class ChatbotModel:\n",
        "  def __init__(self, sess, hparams, model_path, scope='ChatbotModel'):\n",
        "    self.sess = sess\n",
        "    # todo remove\n",
        "    self.hparams = hparams\n",
        "    \n",
        "    # todo\n",
        "    self.model_path = model_path\n",
        "    self.name = scope\n",
        "\n",
        "    self.encoder_inputs, encoder_outputs, encoder_state, embedding_encoder = self._build_encoder(hparams, scope)\n",
        "    self.decoder_inputs, self.decoder_lengths, self.replies, self.beam_replies, logits, self.infer_logits = self._build_decoder(hparams, embedding_encoder, encoder_state, encoder_outputs, scope)\n",
        "\n",
        "    self.reward = tf.placeholder(tf.float32, name=\"reward\")\n",
        "    self.target_labels, self.loss, self.global_step, self.train_op = self._build_optimizer(hparams, logits)\n",
        "    \n",
        "    # Initialize saver after model created\n",
        "    self.saver = tf.train.Saver(tf.global_variables())\n",
        "\n",
        "    ckpt = tf.train.get_checkpoint_state(self.model_path)\n",
        "    if ckpt:\n",
        "      last_model = ckpt.model_checkpoint_path\n",
        "      self.saver.restore(self.sess, last_model)\n",
        "      print(\"loaded \" + last_model)\n",
        "    else:\n",
        "      self.sess.run(tf.global_variables_initializer())\n",
        "      print(\"created fresh model.\")\n",
        "      \n",
        "  def train(self, encoder_inputs, target_labels, decoder_inputs, decoder_lengths, reward = 1.0):\n",
        "    feed_dict = {\n",
        "        self.encoder_inputs: encoder_inputs,\n",
        "        self.target_labels: target_labels,\n",
        "        self.decoder_inputs: decoder_inputs,\n",
        "        self.decoder_lengths: decoder_lengths,\n",
        "        # For normal Seq2Seq reward is always 1.\n",
        "        self.reward: reward \n",
        "    }    \n",
        "    _, loss_value, global_step = self.sess.run([self.train_op, self.loss, self.global_step], feed_dict=feed_dict)\n",
        "    return loss_value, global_step\n",
        "\n",
        "  def train_with_reward(self, standard_seq2seq_model, encoder_inputs, target_labels, decoder_inputs, decoder_lengths, dull_responses):\n",
        "    infered_replies = self.infer(encoder_inputs)\n",
        "    standard_seq2seq_encoder_inputs =[]\n",
        "    for reply in infered_replies:\n",
        "      if len(reply) <= self.hparams.encoder_length:\n",
        "        standard_seq2seq_encoder_inputs.append(np.append(reply, ([pad_id] * (self.hparams.encoder_length - len(reply)))))\n",
        "      else:\n",
        "        raise Exception(\"Infered reply is not suppose to be longer than encoder_input\")\n",
        "    standard_seq2seq_encoder_inputs = np.transpose(np.array(standard_seq2seq_encoder_inputs))\n",
        "    reward1 = standard_seq2seq_model.reward_ease_of_answering(standard_seq2seq_encoder_inputs, dull_responses)\n",
        "    reward2 = 0 # todo\n",
        "    reward3 = 0 # todo\n",
        "    reward = 0.25 * reward1 + 0.25 * reward2 + 0.5 * reward3\n",
        "    print(\"reward\", reward)\n",
        "    return self.train(encoder_inputs, target_labels, decoder_inputs, decoder_lengths, reward)\n",
        "  \n",
        "  def infer(self, encoder_inputs):\n",
        "    inference_feed_dict = {\n",
        "        self.encoder_inputs: encoder_inputs,\n",
        "    }\n",
        "    replies = self.sess.run(self.replies, feed_dict=inference_feed_dict)\n",
        "    return replies\n",
        "  \n",
        "  def infer_beam_search(self, encoder_inputs):\n",
        "    inference_feed_dict = {\n",
        "        self.encoder_inputs: encoder_inputs,\n",
        "    }    \n",
        "    replies = self.sess.run(self.beam_replies, feed_dict=inference_feed_dict)\n",
        "    return replies\n",
        "  \n",
        "  ## todo model_path\n",
        "  def save(self):\n",
        "      self.saver.save(self.sess, \"{}/{}\".format(self.model_path, self.name), global_step=self.global_step)\n",
        "      \n",
        "  def log_prob(self, encoder_inputs, expected_output):\n",
        "    \"\"\"Return sum of log probability of given one specific expected_output for sencoder_inputs.\n",
        "\n",
        "    Args:\n",
        "        encoder_inputs: [encoder_length, batch_size], eg) tweets\n",
        "        expected_output: [1, decoder_length or less than decoder_length], eg) One reply.\n",
        "\n",
        "    Returns:\n",
        "        Return log probablity of expected output for given encoder inputs.\n",
        "        eg) sum of log probability of reply \"Good\" when given [\"How are you?\", \"What's up?\"]\n",
        "    \"\"\"\n",
        "    inference_feed_dict = {\n",
        "      self.encoder_inputs: encoder_inputs,\n",
        "    }\n",
        "    \n",
        "    # Logits\n",
        "    #   logits_value: [batch_size, actual_decoder_length, vocab_size]\n",
        "    logits_batch_value = self.sess.run(self.infer_logits, feed_dict=inference_feed_dict)\n",
        "\n",
        "    sum_p = []\n",
        "    # For each batch: [actual_decoder_length, vocab_size]\n",
        "    for logits in logits_batch_value:\n",
        "      p = 1\n",
        "      # Note that expected_output and logits don't always have same length, but zip takes care of the case.\n",
        "      for word_id, logit in zip(expected_output, logits):\n",
        "        # Apply softmax first, see definition of softmax.\n",
        "        norm = (self._softmax(logit))[word_id]\n",
        "        p *= norm\n",
        "      p = np.log(p)\n",
        "      sum_p.append(p)\n",
        "    ret = np.sum(sum_p) / len(sum_p)\n",
        "    return ret  \n",
        "  \n",
        "  def reward_ease_of_answering(self, encoder_inputs, expected_outputs):\n",
        "    \"\"\" Return reward for ease of answering. See Deep Reinforcement Learning for Dialogue Generation for more details.\n",
        "\n",
        "    Args:\n",
        "        encoder_inputs: [encoder_length, batch_size], eg) tweets\n",
        "        expected_outputs: [number of pre-defined dull responses, decoder_length or less than decoder_length]. eg) [[\"I'm\", \"Good\"], [\"fine\"]]\n",
        "\n",
        "    Returns:\n",
        "        Return reward for ease of answering.\n",
        "        Note that this can be calcualated by calling log_prob function for each dull response,\n",
        "        but this function is more efficient because this calculated the reward at once.\n",
        "    \"\"\"    \n",
        "    inference_feed_dict = {\n",
        "      self.encoder_inputs: encoder_inputs,\n",
        "    }\n",
        "    \n",
        "    # Logits\n",
        "    #   logits_value: [batch_size, actual_decoder_length, vocab_size]\n",
        "    logits_batch_value = self.sess.run(self.infer_logits, feed_dict=inference_feed_dict)\n",
        "\n",
        "    batch_sum_p = []\n",
        "    # For each batch: [actual_decoder_length, vocab_size]\n",
        "    for logits in logits_batch_value:\n",
        "      sum_p = []\n",
        "      for expected_output in expected_outputs:\n",
        "        p = 1\n",
        "        # Note that expected_output and logits don't always have same length, but zip takes care of the case.\n",
        "        for word_id, logit in zip(expected_output, logits):\n",
        "          # Apply softmax first, see definition of softmax.\n",
        "          norm = (self._softmax(logit))[word_id]\n",
        "          p *= norm\n",
        "        p = np.log(p) / len(expected_output)\n",
        "        sum_p.append(p)\n",
        "      one_batch_p = np.sum(sum_p)\n",
        "      batch_sum_p.append(one_batch_p)\n",
        "    ret = np.sum(batch_sum_p) / len(batch_sum_p)\n",
        "    return -ret  \n",
        "      \n",
        "    \n",
        "  @staticmethod\n",
        "  def _softmax(x):\n",
        "      return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
        "      \n",
        "  def _build_optimizer(self, hparams, logits):\n",
        "    # Target labels\n",
        "    #   As described in doc for sparse_softmax_cross_entropy_with_logits,\n",
        "    #   labels should be [batch_size, decoder_lengths] instead of [batch_size, decoder_lengths, vocab_size].\n",
        "    #   So labels should have indices instead of vocab_size classes.\n",
        "    target_labels = tf.placeholder(tf.int32, shape=(hparams.batch_size, hparams.decoder_length), name=\"target_labels\")\n",
        "\n",
        "    # Loss\n",
        "    crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "        labels=target_labels, logits=logits)\n",
        "\n",
        "    loss = tf.reduce_sum(crossent / tf.to_float(hparams.batch_size))\n",
        "    # Adjust loss with reward.\n",
        "    loss = tf.multiply(loss, self.reward)\n",
        "    \n",
        "    # Train\n",
        "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
        "\n",
        "    # Calculate and clip gradients\n",
        "    params = tf.trainable_variables()\n",
        "    gradients = tf.gradients(loss, params)\n",
        "    clipped_gradients, _ = tf.clip_by_global_norm(\n",
        "        gradients, hparams.max_gradient_norm)\n",
        "\n",
        "    # Optimization\n",
        "    optimizer = tf.train.AdamOptimizer(hparams.learning_rate)\n",
        "    train_op = optimizer.apply_gradients(\n",
        "        zip(clipped_gradients, params), global_step=global_step)\n",
        "    return target_labels, loss, global_step, train_op\n",
        "  \n",
        "  def _build_encoder(self, hparams, scope):\n",
        "    # Encoder\n",
        "    #   encoder_inputs: [encoder_length, batch_size]\n",
        "    #   This is time major where encoder_length comes first instead of batch_size.\n",
        "    encoder_inputs = tf.placeholder(tf.int32, shape=(hparams.encoder_length, hparams.batch_size), name=\"encoder_inputs\")\n",
        "    \n",
        "    # Embedding\n",
        "    #   We originally didn't share embbedding between encoder and decoder.\n",
        "    #   But now we share it. It makes much easier to calculate rewards.\n",
        "    #   Matrix for embedding: [vocab_size, embedding_size]\n",
        "    with tf.variable_scope(scope):\n",
        "      embedding_encoder = tf.get_variable(\"embedding_encoder\", [hparams.vocab_size, hparams.embedding_size])\n",
        "\n",
        "    # Look up embedding:\n",
        "    #   encoder_inputs: [encoder_length, batch_size]\n",
        "    #   encoder_emb_inputs: [encoder_length, batch_size, embedding_size]\n",
        "    encoder_emb_inputs = tf.nn.embedding_lookup(embedding_encoder, encoder_inputs)\n",
        "\n",
        "    # LSTM cell.\n",
        "    with tf.variable_scope(scope):\n",
        "      encoder_cell = tf.nn.rnn_cell.BasicLSTMCell(hparams.num_units)\n",
        "\n",
        "    # Run Dynamic RNN\n",
        "    #   encoder_outputs: [encoder_length, batch_size, num_units]\n",
        "    #   encoder_state: [batch_size, num_units], this is final state of the cell for each batch.\n",
        "    with tf.variable_scope(scope):\n",
        "      encoder_outputs, encoder_state = tf.nn.dynamic_rnn(encoder_cell, encoder_emb_inputs, time_major=True, dtype=tf.float32)\n",
        "      \n",
        "    return encoder_inputs, encoder_outputs, encoder_state, embedding_encoder\n",
        "  \n",
        "  def _build_greedy_inference(self, hparams, embedding_encoder, decoder_cell, initial_state, projection_layer):\n",
        "    # Greedy decoder\n",
        "    inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n",
        "        embedding_encoder,\n",
        "        tf.fill([hparams.batch_size], tgt_sos_id), tgt_eos_id)\n",
        "    \n",
        "    inference_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
        "        decoder_cell, inference_helper, initial_state,\n",
        "        output_layer=projection_layer)\n",
        "\n",
        "    # len(infered_reply) is lte encoder_length, because we are targetting tweeet (140 for each tweet)\n",
        "    # Also by doing this, we can pass the reply to other seq2seq w/o shorten it.\n",
        "    maximum_iterations = hparams.encoder_length\n",
        "\n",
        "    # Dynamic decoding\n",
        "    outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
        "        inference_decoder, maximum_iterations=maximum_iterations)\n",
        "    replies = outputs.sample_id\n",
        "    \n",
        "    # We use infer_logits instead of logits when calculating log_prob, because infer_logits doesn't require decoder_lengths input.\n",
        "    infer_logits = outputs.rnn_output\n",
        "    return infer_logits, replies\n",
        "    \n",
        "  def _build_training_decoder(self, hparams, encoder_state, encoder_outputs, decoder_cell, decoder_emb_inputs, decoder_lengths, projection_layer):\n",
        "    # Decoder with helper:\n",
        "    #   decoder_emb_inputs: [decoder_length, batch_size, embedding_size]\n",
        "    #   decoder_length: [batch_size] vector, which represents each target sequence length.\n",
        "    training_helper = tf.contrib.seq2seq.TrainingHelper(decoder_emb_inputs, decoder_lengths, time_major=True)\n",
        "\n",
        "    # See https://github.com/tensorflow/tensorflow/issues/11904\n",
        "    if hparams.use_attention:\n",
        "      # Attention\n",
        "      # encoder_outputs is time major, so transopse it to batch major.\n",
        "      # attention_encoder_outputs: [batch_size, encoder_length, num_units]\n",
        "      attention_encoder_outputs = tf.transpose(encoder_outputs, [1, 0, 2])\n",
        "\n",
        "      # Create an attention mechanism\n",
        "      attention_mechanism = tf.contrib.seq2seq.LuongAttention(\n",
        "          hparams.num_units,\n",
        "          attention_encoder_outputs,\n",
        "          memory_sequence_length=None) ###### todo\n",
        "\n",
        "      wrapped_decoder_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
        "          decoder_cell, attention_mechanism,\n",
        "          attention_layer_size=hparams.num_units)\n",
        "\n",
        "      initial_state = wrapped_decoder_cell.zero_state(hparams.batch_size, tf.float32).clone(cell_state=encoder_state)\n",
        "    else:\n",
        "      wrapped_decoder_cell = decoder_cell\n",
        "      initial_state = encoder_state    \n",
        "      \n",
        "    # Decoder and decode\n",
        "    training_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
        "        wrapped_decoder_cell, training_helper, initial_state,\n",
        "        output_layer=projection_layer)  \n",
        "    \n",
        "    # Dynamic decoding\n",
        "    #   final_outputs.rnn_output: [batch_size, decoder_length, vocab_size], list of RNN state.\n",
        "    #   final_outputs.sample_id: [batch_size, decoder_length], list of argmax of rnn_output.\n",
        "    #   final_state: [batch_size, num_units], list of final state of RNN on decode process.\n",
        "    #   final_sequence_lengths: [batch_size], list of each decoded sequence. \n",
        "    final_outputs, _final_state, _final_sequence_lengths = tf.contrib.seq2seq.dynamic_decode(training_decoder)\n",
        "\n",
        "    if hparams.debug_verbose:\n",
        "      print(\"rnn_output.shape=\", final_outputs.rnn_output.shape)\n",
        "      print(\"sample_id.shape=\", final_outputs.sample_id.shape)\n",
        "      print(\"final_state=\", _final_state)\n",
        "      print(\"final_sequence_lengths.shape=\", _final_sequence_lengths.shape)\n",
        "\n",
        "    logits = final_outputs.rnn_output     \n",
        "    return logits, wrapped_decoder_cell, initial_state\n",
        "  \n",
        "  def _build_beam_search_inference(self, hparams, embedding_encoder, encoder_state, encoder_outputs, decoder_cell, projection_layer):\n",
        "    # https://github.com/tensorflow/tensorflow/issues/11904\n",
        "    if hparams.use_attention: \n",
        "      # Attention\n",
        "      # encoder_outputs is time major, so transopse it to batch major.\n",
        "      # attention_encoder_outputs: [batch_size, encoder_length, num_units]\n",
        "      attention_encoder_outputs = tf.transpose(encoder_outputs, [1, 0, 2])\n",
        "      \n",
        "      tiled_encoder_outputs = tf.contrib.seq2seq.tile_batch(attention_encoder_outputs, multiplier=hparams.beam_width)\n",
        "      tiled_encoder_final_state = tf.contrib.seq2seq.tile_batch(encoder_state, multiplier=hparams.beam_width)\n",
        "\n",
        "      # Create an attention mechanism\n",
        "      attention_mechanism = tf.contrib.seq2seq.LuongAttention(\n",
        "            hparams.num_units, tiled_encoder_outputs, memory_sequence_length=None) ###### todo    \n",
        "      \n",
        "      wrapped_decoder_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
        "             decoder_cell, attention_mechanism,\n",
        "            attention_layer_size=hparams.num_units)        \n",
        "\n",
        "      decoder_initial_state = wrapped_decoder_cell.zero_state(dtype=tf.float32, batch_size=hparams.batch_size * hparams.beam_width)\n",
        "      decoder_initial_state = decoder_initial_state.clone(cell_state=tiled_encoder_final_state)\n",
        "      \n",
        "      # todo\n",
        "      #    X_seq_len = tf.contrib.seq2seq.tile_batch(X_seq_len, multiplier=BEAM_WIDTH)\n",
        "\n",
        "    else:\n",
        "      wrapped_decoder_cell = decoder_cell\n",
        "      decoder_initial_state = tf.contrib.seq2seq.tile_batch(encoder_state, multiplier=hparams.beam_width)\n",
        "\n",
        "    # len(infered_reply) is lte encoder_length, because we are targetting tweeet (140 for each tweet)\n",
        "    # Also by doing this, we can pass the reply to other seq2seq w/o shorten it.\n",
        "    maximum_iterations = hparams.encoder_length\n",
        "      \n",
        "    inference_decoder = tf.contrib.seq2seq.BeamSearchDecoder(\n",
        "            cell=wrapped_decoder_cell,\n",
        "            embedding=embedding_encoder,\n",
        "            start_tokens=tf.fill([hparams.batch_size], tgt_sos_id),\n",
        "            end_token=tgt_eos_id,\n",
        "            initial_state=decoder_initial_state,\n",
        "            beam_width=hparams.beam_width,\n",
        "            output_layer=projection_layer,\n",
        "            length_penalty_weight=0.0)\n",
        "\n",
        "    # Dynamic decoding\n",
        "    beam_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
        "        inference_decoder, maximum_iterations=maximum_iterations)\n",
        "    beam_replies = beam_outputs.predicted_ids       \n",
        "    return beam_replies\n",
        "  \n",
        "  def _build_decoder(self, hparams, embedding_encoder, encoder_state, encoder_outputs, scope):\n",
        "    # Decoder input\n",
        "    #   decoder_inputs: [decoder_length, batch_size]\n",
        "    #   decoder_lengths: [batch_size]\n",
        "    #   This is grand truth target inputs for training.\n",
        "    decoder_inputs = tf.placeholder(tf.int32, shape=(hparams.decoder_length, hparams.batch_size), name=\"decoder_inputs\")\n",
        "    decoder_lengths = tf.placeholder(tf.int32, shape=(hparams.batch_size), name=\"decoder_lengths\")\n",
        "\n",
        "    # Look up embedding:\n",
        "    #   decoder_inputs: [decoder_length, batch_size]\n",
        "    #   decoder_emb_inp: [decoder_length, batch_size, embedding_size]\n",
        "    decoder_emb_inputs = tf.nn.embedding_lookup(embedding_encoder, decoder_inputs)   \n",
        "    \n",
        "    # https://stackoverflow.com/questions/39573188/output-projection-in-seq2seq-model-tensorflow\n",
        "    # Internally, a neural network operates on dense vectors of some size,\n",
        "    # often 256, 512 or 1024 floats (let's say 512 for here). \n",
        "    # But at the end it needs to predict a word from the vocabulary which is often much larger,\n",
        "    # e.g., 40000 words. Output projection is the final linear layer that converts (projects) from the internal representation to the larger one.\n",
        "    # So, for example, it can consist of a 512 x 40000 parameter matrix and a 40000 parameter for the bias vector.\n",
        "    projection_layer = layers_core.Dense(hparams.vocab_size, use_bias=False)\n",
        "    \n",
        "    # We share this between training and inference.\n",
        "    decoder_cell = tf.nn.rnn_cell.BasicLSTMCell(hparams.num_units)\n",
        "\n",
        "    # Training graph\n",
        "    logits, wrapped_decoder_cell, initial_state = self._build_training_decoder(hparams, encoder_state, encoder_outputs, decoder_cell, decoder_emb_inputs, decoder_lengths, projection_layer)\n",
        "   \n",
        "    # Greedy Inference graph\n",
        "    infer_logits, replies = self._build_greedy_inference(hparams, embedding_encoder, wrapped_decoder_cell, initial_state, projection_layer)\n",
        "    \n",
        "    # Beam Search Inference graph\n",
        "    beam_replies = self._build_beam_search_inference(hparams, embedding_encoder, encoder_state, encoder_outputs, decoder_cell, projection_layer) \n",
        "\n",
        "    return decoder_inputs, decoder_lengths, replies, beam_replies, logits, infer_logits"
      ],
      "cell_type": "code",
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DQg8kU-2Dr-q",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "source": [
        "# Helper functions to test\n",
        "def make_test_training_data(hparams):\n",
        "  train_encoder_inputs = np.empty((hparams.encoder_length, hparams.batch_size), dtype=np.int)\n",
        "  training_target_labels = np.empty((hparams.batch_size, hparams.decoder_length), dtype=np.int)\n",
        "  training_decoder_inputs = np.empty((hparams.decoder_length, hparams.batch_size), dtype=np.int)\n",
        "\n",
        "  # We keep first tweet to validate inference.\n",
        "  first_tweet = None\n",
        "\n",
        "  for i in range(hparams.batch_size):\n",
        "    # Tweet\n",
        "    tweet = np.random.randint(low=0, high=hparams.vocab_size, size=hparams.encoder_length)\n",
        "    train_encoder_inputs[:, i] = tweet  \n",
        "  \n",
        "    # Reply\n",
        "    #   Note that low = 2, as 0 and 1 are reserved.\n",
        "    reply = np.random.randint(low=2, high=hparams.vocab_size, size=hparams.decoder_length - 1)\n",
        "  \n",
        "    training_target_label = np.concatenate((reply, np.array([tgt_eos_id])))\n",
        "    training_target_labels[i] = training_target_label\n",
        "  \n",
        "    training_decoder_input = np.concatenate(([tgt_sos_id], reply))\n",
        "    training_decoder_inputs[:, i] = training_decoder_input\n",
        "  \n",
        "    if i == 0:\n",
        "      first_tweet = tweet\n",
        "      if hparams.debug_verbose:\n",
        "        print(\"0th tweet={}\".format(tweet))\n",
        "        print(\"0th reply_with_eos_suffix={}\".format(training_target_label))\n",
        "        print(\"0th reply_with_sos_prefix={}\".format(training_decoder_input))\n",
        "\n",
        "    if hparams.debug_verbose:\n",
        "      print(\"Tweets\")\n",
        "      print(train_encoder_inputs)\n",
        "      print(\"Replies\")\n",
        "      print(training_target_labels)\n",
        "      print(training_decoder_inputs)\n",
        "  return first_tweet, train_encoder_inputs, training_target_labels, training_decoder_inputs\n",
        "\n",
        "def test_training(test_hparams, model):\n",
        "  if test_hparams.use_attention:\n",
        "    print(\"==== training model[attention] ====\")\n",
        "  else:\n",
        "    print(\"==== training model ====\")\n",
        "  first_tweet, train_encoder_inputs, training_target_labels, training_decoder_inputs = make_test_training_data(test_hparams)\n",
        "  # Train\n",
        "  x = []\n",
        "  y = []\n",
        "  for i in range(test_hparams.num_train_steps):\n",
        "    loss_value, global_step = model.train(train_encoder_inputs, training_target_labels, training_decoder_inputs, np.ones((test_hparams.batch_size), dtype=int) * test_hparams.decoder_length)\n",
        "    if i % 5 == 0 and test_hparams.debug_verbose:\n",
        "      print('.', end='')\n",
        "\n",
        "    if i % 15 == 0:\n",
        "      model.save()\n",
        "      x.append(global_step)\n",
        "      y.append(loss_value)\n",
        "      if test_hparams.debug_verbose:\n",
        "        print(\"loss={} step={}\".format(loss_value, global_step))\n",
        "\n",
        "  inference_encoder_inputs = np.empty((test_hparams.encoder_length, test_hparams.batch_size), dtype=np.int)\n",
        "  for i in range(test_hparams.batch_size):\n",
        "    inference_encoder_inputs[:, i] = first_tweet\n",
        "\n",
        "\n",
        "  # testing \n",
        "  log_prob54 = model.log_prob(inference_encoder_inputs, np.array([5, 4]))\n",
        "  log_prob65 = model.log_prob(inference_encoder_inputs, np.array([6, 5]))\n",
        "  print(\"log_prob for 54\", log_prob54)\n",
        "  print(\"log_prob for 65\", log_prob65)\n",
        "\n",
        "  reward = model.reward_ease_of_answering(inference_encoder_inputs, np.array([[5], [6]]))\n",
        "  print(\"reward=\", reward)\n",
        "  \n",
        "  if test_hparams.debug_verbose:\n",
        "    print(inference_encoder_inputs)\n",
        "  replies = model.infer(inference_encoder_inputs)\n",
        "  print(\"Infered replies\", replies[0])\n",
        "  print(\"Expected replies\", training_target_labels[0])\n",
        "  \n",
        "  beam_replies = model.infer_beam_search(inference_encoder_inputs)\n",
        "  print(\"Infered replies candidate0\", beam_replies[0][:,0])\n",
        "  print(\"Infered replies candidate1\", beam_replies[0][:,1])\n",
        "\n",
        "  if test_hparams.debug_verbose:    \n",
        "    plt.plot(x, y, label=\"Loss\")\n",
        "    plt.plot()\n",
        "    plt.xlabel(\"Loss\")\n",
        "    plt.ylabel(\"steps\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "  \n",
        "def test_two_models_training():\n",
        "  first_tweet, train_encoder_inputs, training_target_labels, training_decoder_inputs = make_test_training_data(test_hparams)\n",
        "\n",
        "  graph1= tf.Graph()\n",
        "  graph2 = tf.Graph()\n",
        "  graph3 = tf.Graph()\n",
        "\n",
        "  with graph1.as_default():\n",
        "    sess1 = tf.Session(graph=graph1)\n",
        "    model = ChatbotModel(sess1, test_hparams, model_path=\"./saved_model/hige\")\n",
        "    test_training(test_hparams, model)  \n",
        "\n",
        "  with graph2.as_default():\n",
        "    sess2 = tf.Session(graph=graph2)\n",
        "    model2 = ChatbotModel(sess2, test_hparams, model_path=\"./saved_model2/hige\")\n",
        "    test_training(test_hparams, model2)  \n",
        "    dull_responses = [[4, 6, 6], [5, 5]]\n",
        "    model2.train_with_reward(model, train_encoder_inputs, training_target_labels, training_decoder_inputs, np.ones((test_hparams.batch_size), dtype=int) * test_hparams.decoder_length, dull_responses)\n",
        "\n",
        "  with graph3.as_default():\n",
        "    sess3 = tf.Session(graph=graph3)\n",
        "    model3 = ChatbotModel(sess3, test_attention_hparams, model_path=\"./saved_model3/hige\")\n",
        "    test_training(test_attention_hparams, model3)  \n",
        "    \n",
        "  \n",
        "! rm -rf ./saved_model\n",
        "! mkdir ./saved_model\n",
        "! rm -rf ./saved_model2\n",
        "! mkdir ./saved_model2\n",
        "! rm -rf ./saved_model3\n",
        "! mkdir ./saved_model3\n",
        "\n",
        "# Fresh model\n",
        "test_two_models_training()\n",
        "\n",
        "# Saved model\n",
        "test_two_models_training()\n"
      ],
      "cell_type": "code",
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SB627B3UGIac",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "source": [
        "!pip install pydrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "cell_type": "code",
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kxeWpXO5FThm",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "source": [
        "def read_file_from_drive(file_name):\n",
        "  seq2seq_data_dir_id = \"146ZLldWXLDH0l9WbSUNFKi3nVK_HV0Sz\"\n",
        "  file_list = drive.ListFile({'q': \"'{}' in parents and trashed=false\".format(seq2seq_data_dir_id)}).GetList()\n",
        "  found = [file for file in file_list if file['title'] == file_name]\n",
        "  if found != []:\n",
        "    downloaded = drive.CreateFile({'id': found[0]['id']})\n",
        "    return downloaded.GetContentString()\n",
        "  else:\n",
        "    raise ValueError(\"file {} not found.\".format(file_name))\n",
        "\n",
        "def read_vocabulary_drive(vocabulary_path):\n",
        "  rev_vocab = []\n",
        "  rev_vocab.extend(read_file_from_drive(vocabulary_path).splitlines())\n",
        "  print(rev_vocab)\n",
        "  rev_vocab = [line.strip() for line in rev_vocab]\n",
        "  vocab = dict([(x, y) for (y, x) in enumerate(rev_vocab)])\n",
        "  return vocab, rev_vocab  \n",
        "  \n",
        "print(read_vocabulary_drive('vocab.txt'))"
      ],
      "cell_type": "code",
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_Np6GIJATTGg",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "source": [
        "def read_training_data_from_drive(file_name, max_line_len, pad_value):\n",
        "  ret = []\n",
        "  for line in read_file_from_drive(file_name).splitlines():\n",
        "    # padding\n",
        "    ids = [int(x) for x in line.split()]\n",
        "    if len(ids) > max_line_len:\n",
        "      ids = ids[:max_line_len]\n",
        "    else:\n",
        "      ids.extend([pad_value] * (max_line_len - len(ids)))\n",
        "    ret.append(ids)\n",
        "  return ret\n",
        "\n",
        "def words_to_ids(words, vocab):\n",
        "  ids = []\n",
        "  for word in words:\n",
        "    if word in vocab:\n",
        "      ids.append(vocab[word])\n",
        "    else:\n",
        "      ids.append(unk_id)\n",
        "  return ids\n",
        "\n",
        "def ids_to_words(ids, rev_vocab):\n",
        "  words = \"\"\n",
        "  for id in ids:\n",
        "    words += rev_vocab[id]\n",
        "  return words\n",
        "# For replies, we use decoder_lenght - 1, because we need to add eos/sos.\n",
        "replies = read_training_data_from_drive('tweets_train_dec_idx.txt', real_hparams.decoder_length - 1, pad_id)\n",
        "tweets = read_training_data_from_drive('tweets_train_enc_idx.txt', real_hparams.encoder_length, pad_id)\n",
        "print(\"tweets_shape=\", len(tweets))\n",
        "\n",
        "vocab, rev_vocab = read_vocabulary_drive('vocab.txt')\n",
        "\n",
        "\n"
      ],
      "cell_type": "code",
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9STF9lA6UVCG",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "source": [
        "sess = tf.Session()\n",
        "# Note that tweets data should be a matrix where each line has exact same length.\n",
        "tweets_dataset = tf.data.Dataset.from_tensor_slices(tweets)\n",
        "replies_dataset = tf.data.Dataset.from_tensor_slices(replies)\n",
        "\n",
        "tweets_transposed = tweets_dataset.batch(real_hparams.batch_size).map(lambda x: tf.transpose(x))\n",
        "replies_with_eos_suffix = replies_dataset.map(lambda x: tf.concat([x, [tgt_eos_id]], axis=0)).batch(real_hparams.batch_size)\n",
        "replies_with_sos_prefix = replies_dataset.map(lambda x: tf.concat([[tgt_sos_id], x], axis=0)).batch(real_hparams.batch_size).map(lambda x: tf.transpose(x))\n",
        "\n",
        "print(\"tweets_example:\", sess.run(tweets_transposed.make_one_shot_iterator().get_next()))\n",
        "print(\"reply_with_eos_suffix_example:\", sess.run(replies_with_eos_suffix.make_one_shot_iterator().get_next()))\n",
        "print(\"reply_with_sos_prefix_example:\", sess.run(replies_with_sos_prefix.make_one_shot_iterator().get_next()))\n",
        "\n",
        "# Merge all using zip\n",
        "train_feed_data = tf.data.Dataset.zip((tweets_transposed, replies_with_eos_suffix, replies_with_sos_prefix))\n",
        "train_feed_data_value = sess.run(train_feed_data.make_one_shot_iterator().get_next())\n",
        "print(\"train_feed_data=\", train_feed_data_value[0])\n",
        "print(\"train_feed_data=\", train_feed_data_value[1])\n",
        "print(\"train_feed_data=\", train_feed_data_value[2])                                 \n",
        "                                 "
      ],
      "cell_type": "code",
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dagyd_fKE8lX",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "source": [
        "# Train using real data.\n",
        "#! rm -rf ./saved_model/real\n",
        "x = []\n",
        "y = []\n",
        "\n",
        "graph3= tf.Graph()\n",
        "with graph3.as_default():\n",
        "  sess3 = tf.Session(graph=graph3)\n",
        "  model3 = ChatbotModel(sess3, real_hparams, model_path=\"./saved_model/real\")\n",
        "  train_data_iterator = train_feed_data.repeat(1000).shuffle(500000).make_one_shot_iterator()\n",
        "    \n",
        "  for i in range(10): # real_hparams.num_train_steps):\n",
        "    train_data = sess3.run(train_data_iterator.get_next())\n",
        "    loss_value, global_step = model3.train(train_data[0], train_data[1], train_data[2], np.ones((real_hparams.batch_size), dtype=int) * real_hparams.decoder_length)\n",
        "\n",
        "    if i % 5 == 0 and real_hparams.debug_verbose:\n",
        "      print('.', end='')\n",
        "\n",
        "    if i % 15 == 0:\n",
        "      model3.save()\n",
        "      x.append(global_step)\n",
        "      y.append(loss_value)\n",
        "      if real_hparams.debug_verbose:\n",
        "        print(\"loss={} step={}\".format(loss_value, global_step))\n",
        "\n",
        "  # Calculate log_prob of www and \n",
        "  train_data = sess3.run(train_data_iterator.get_next())\n",
        "  print(\"probablity of www\", model3.log_prob(train_data[0], [vocab['www']]))\n",
        "  print(\"probablity of 00\", model3.log_prob(train_data[0], [vocab['00']]))\n",
        "\n"
      ],
      "cell_type": "code",
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y4auRWoHk9RW",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "source": [
        "\n",
        "plt.plot(x, y, label=\"Loss\")\n",
        "plt.plot()\n",
        "\n",
        "plt.xlabel(\"Loss\")\n",
        "plt.ylabel(\"steps\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "cell_type": "code",
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FsvxQU26XsOd",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "source": [
        "with graph3.as_default():\n",
        "  inference_encoder_inputs = np.empty((real_hparams.encoder_length, real_hparams.batch_size), dtype=np.int)\n",
        "  tweet = [\"„Éï„Ç©„É≠„Éº\", \"„ÅÇ„Çä„Åå„Å®„ÅÜ\", \"„É°„ÉÉ„Çª„Éº„Ç∏\", \"üò¢\", \"www\"]\n",
        "  tweet_ids = words_to_ids(tweet, vocab)\n",
        "  tweet_ids.extend([pad_id] * (real_hparams.encoder_length - len(tweet_ids)))\n",
        "  for i in range(real_hparams.batch_size):\n",
        "    inference_encoder_inputs[:, i] = np.array(tweet_ids, dtype=np.int) \n",
        "\n",
        "  replies = model3.infer(inference_encoder_inputs)\n",
        "  reply = replies[0].tolist()\n",
        "  print(\"Infered reply\", ids_to_words(reply, rev_vocab))\n",
        "  \n",
        "  beam_replies = model3.infer_beam_search(inference_encoder_inputs)\n",
        "  print(\"Infered replies candidate0\", ids_to_words(beam_replies[0][:,0], rev_vocab))\n",
        "  print(\"Infered replies candidate1\", ids_to_words(beam_replies[0][:,1], rev_vocab))"
      ],
      "cell_type": "code",
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AkwkmXfPPv57",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "source": [
        ""
      ],
      "cell_type": "code",
      "execution_count": 0,
      "outputs": []
    }
  ]
}
