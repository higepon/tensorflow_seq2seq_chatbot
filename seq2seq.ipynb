{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seq2seq.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "HWOxK9T5I8sb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Chatbot based on Seq2Seq Beam Search + Attention + Reinforcment Learning(Experimental)\n",
        "- Tensorflow 1.4.0+ is required.\n",
        "- This is based on [NMT Tutorial](https://github.com/tensorflow/nmt).\n",
        "- Experiment [notes](https://github.com/higepon/tensorflow_seq2seq_chatbot/wiki).\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "kK1r053SI2f9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Special commands should be located here.\n",
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "!apt-get -qq install -y mecab libmecab-dev mecab-ipadic mecab-ipadic-utf8\n",
        "\n",
        "!pip -q install git+https://github.com/mrahtz/easy-tf-log#egg=easy-tf-log[tf]\n",
        "!pip install pushbullet.py\n",
        "!pip install tweepy pyyaml\n",
        "!pip install mecab-python3\n",
        "\n",
        "def auth_google_drive():\n",
        "  # Generate creds for the Drive FUSE library.\n",
        "  if not os.path.exists('drive'):\n",
        "    from oauth2client.client import GoogleCredentials\n",
        "    creds = GoogleCredentials.get_application_default()\n",
        "    import getpass\n",
        "    !google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "    vcode = getpass.getpass()\n",
        "    !echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}  \n",
        "\n",
        "def mount_google_drive():\n",
        "  if not os.path.exists('drive'):\n",
        "    os.makedirs('drive', exist_ok=True)\n",
        "    !google-drive-ocamlfuse drive \n",
        "    \n",
        "def kill_docker():\n",
        "  !kill -9 -1  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "90XCqkUfbnUZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "response = urllib.request.urlopen(\"https://raw.githubusercontent.com/yaroslavvb/memory_util/master/memory_util.py\")\n",
        "open(\"memory_util.py\", \"wb\").write(response.read())\n",
        "import memory_util"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WE9v1UerJMRo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import copy as copy\n",
        "import datetime\n",
        "import hashlib\n",
        "import json\n",
        "import os\n",
        "import os.path\n",
        "import filecmp\n",
        "import random\n",
        "import re\n",
        "import shutil\n",
        "import importlib\n",
        "\n",
        "\n",
        "import MeCab\n",
        "import easy_tf_log\n",
        "import matplotlib.pyplot as plt\n",
        "import random as random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tweepy\n",
        "import yaml\n",
        "from easy_tf_log import tflog\n",
        "from google.colab import auth\n",
        "from google.colab import files\n",
        "import importlib\n",
        "from pushbullet import Pushbullet\n",
        "from tensorflow.python.layers import core as layers_core\n",
        "from tensorflow.python.platform import gfile\n",
        "\n",
        "# Generate auth tokens for Colab\n",
        "auth.authenticate_user()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OoMe73Z51zNk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#kill_docker()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mM1uEwbYJPJK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "auth_google_drive()\n",
        "mount_google_drive()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VuC6wyLvVUlk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import drive.tensorflow_seq2seq_chatbot.lib.chatbot_model as sq"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YPhaIY-BHV1Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def reload_modules():\n",
        "  !fusermount -u drive\n",
        "  !google-drive-ocamlfuse -cc drive \n",
        "  importlib.reload(sq)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fNrRD9yOFXM6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if sq.mode == sq.Mode.Test:\n",
        "    sq.test_distributed_one(enable_attention=False)\n",
        "    sq.test_distributed_one(enable_attention=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JrvS_DURF5Pq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tweet_small_hparams = copy.deepcopy(sq.base_hparams).override_from_dict(\n",
        "    {\n",
        "        'batch_size': 6,  # of tweets should be dividable by batch_size\n",
        "        'encoder_length': 8,\n",
        "        'decoder_length': 8,\n",
        "        'num_units': 256,\n",
        "        'num_layers': 2,\n",
        "        'vocab_size': 34,\n",
        "        'embedding_size': 40,\n",
        "        'beam_width': 2,  # for faster iteration, this should be 10\n",
        "        'num_train_steps': 200,\n",
        "        'model_path': sq.ModelDirectory.tweet_small.value,\n",
        "        'learning_rate': 0.05,\n",
        "        'use_attention': True,\n",
        "    })\n",
        "\n",
        "tweet_small_swapped_hparams = copy.deepcopy(\n",
        "    tweet_small_hparams).override_from_dict(\n",
        "    {'model_path': sq.ModelDirectory.tweet_small_swapped.value})\n",
        "\n",
        "if sq.mode == sq.Mode.Test:\n",
        "    tweets_path = \"tweets_small.txt\"\n",
        "    sq.TrainDataGenerator(tweets_path, tweet_small_hparams).remove_generated()\n",
        "    trainer = sq.Trainer()\n",
        "    trainer.train_seq2seq(tweet_small_hparams, tweets_path,\n",
        "                          [\"おはようございます。寒いですね。\", \"さて帰ろう。明日は早い。\", \"今回もよろしくです。\"])\n",
        "    sq.test_tweets_small_swapped(tweet_small_swapped_hparams)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tzh2rhEPguJ9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tweet_large_hparams = copy.deepcopy(sq.base_hparams).override_from_dict(\n",
        "    {\n",
        "        # In typical seq2seq chatbot\n",
        "        # num_layers=3, learning_rate=0.5, batch_size=64, vocab=20000-100000, learning_rate decay is 0.99, which is taken care as default parameter in AdamOptimizer.\n",
        "        'batch_size': 64,  # of tweets should be dividable by batch_size\n",
        "        'encoder_length': 28,\n",
        "        'decoder_length': 28,\n",
        "        'num_units': 1024,\n",
        "        'num_layers': 3,\n",
        "        'vocab_size': 60000,\n",
        "    # conversations.txt actually has about 70K uniq words.\n",
        "        'embedding_size': 1024,\n",
        "        'beam_width': 2,  # for faster iteration, this should be 10\n",
        "        'num_train_steps': 1000000,\n",
        "        'model_path': sq.ModelDirectory.tweet_large.value,\n",
        "        'learning_rate': 0.5,\n",
        "    # For vocab_size 50000, num_layers 3, num_units 1024, tweet_large, starting learning_rate 0.05 works well, change it t0 0.01 at perplexity 800, changed it to 0.005 at 200.\n",
        "        'learning_rate_decay': 0.99,\n",
        "        'use_attention': True,\n",
        "        # testing new restore learning rate and no USERNAME TOKEN\n",
        "    })\n",
        "\n",
        "tweet_large_swapped_hparams = copy.deepcopy(\n",
        "    tweet_large_hparams).override_from_dict(\n",
        "    {\n",
        "        'model_path': sq.ModelDirectory.tweet_large_swapped.value\n",
        "    })\n",
        "\n",
        "#Shell.save_model_in_drive(tweet_large_hparams.model_path)\n",
        "\n",
        "if sq.mode == sq.Mode.TrainSeq2Seq:\n",
        "    print(\"train seq2seq\")\n",
        "    sq.test_tweets_large(tweet_large_hparams)\n",
        "elif sq.mode == sq.Mode.TrainSeq2SeqSwapped:\n",
        "    print(\"train seq2seq swapped\")\n",
        "    sq.test_tweets_large_swapped(tweet_large_swapped_hparams)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "swhAySRidGr-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ls -Sl model/conversations_large*"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uWLrKkcC3TXM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "  \n",
        "reload_modules()\n",
        "\n",
        "conversations_large_hparams = copy.deepcopy(sq.base_hparams).override_from_dict(\n",
        "    {\n",
        "        # In typical seq2seq chatbot\n",
        "        # num_layers=3, learning_rate=0.5, batch_size=64, vocab=20000-100000, learning_rate decay is 0.99, which is taken care as default parameter in AdamOptimizer.\n",
        "        'batch_size': 64,  # of tweets should be dividable by batch_size\n",
        "        'encoder_length': 28,\n",
        "        'decoder_length': 28,\n",
        "        'num_units': 1024,\n",
        "        'num_layers': 3,\n",
        "        'vocab_size': 60000,\n",
        "    # conversations.txt actually has about 70K uniq words.\n",
        "        'embedding_size': 1024,\n",
        "        'beam_width': 2,  # for faster iteration, this should be 10\n",
        "        'num_train_steps': 0,\n",
        "        'model_path': sq.ModelDirectory.conversations_large.value,\n",
        "        'learning_rate': 0.5,\n",
        "    # For vocab_size 50000, num_layers 3, num_units 1024, tweet_large, starting learning_rate 0.05 works well, change it t0 0.01 at perplexity 800, changed it to 0.005 at 200.\n",
        "        'learning_rate_decay': 0.99,\n",
        "        'use_attention': True,\n",
        "\n",
        "    })\n",
        "\n",
        "conversations_large_rl_hparams = copy.deepcopy(\n",
        "    conversations_large_hparams).override_from_dict(\n",
        "    {\n",
        "        'model_path': sq.ModelDirectory.conversations_large_rl.value,\n",
        "        'num_train_steps': 12000,\n",
        "        'beam_width': 3,\n",
        "    })\n",
        "\n",
        "\n",
        "conversations_large_backward_hparams = copy.deepcopy(\n",
        "    conversations_large_hparams).override_from_dict(\n",
        "    {\n",
        "        'model_path': sq.ModelDirectory.conversations_large_backward.value,\n",
        "        'num_train_steps': 0,        \n",
        "    })\n",
        "\n",
        "\n",
        "conversations_txt = \"conversations_large.txt\"\n",
        "sq.Shell.download_file_if_necessary(conversations_txt)\n",
        "sq.ConversationTrainDataGenerator().generate(conversations_txt)\n",
        "\n",
        "with memory_util.capture_stderr() as stderr:\n",
        "    try:\n",
        "        trainer =sq.Trainer()\n",
        "        valid_tweets = [\"さて福岡行ってきます！\", \"誰か飲みに行こう\", \"熱でてるけど、でもなんか食べなきゃーと思ってアイス買おうとしたの\",\n",
        "              \"今日のドラマ面白そう！\", \"お腹すいたー\", \"おやすみ～\", \"おはようございます。寒いですね。\",\n",
        "              \"さて帰ろう。明日は早い。\", \"今回もよろしくです。\", \"ばいとおわ！\"]\n",
        "        trainer.train_seq2seq(conversations_large_hparams,\n",
        "                              \"conversations_large_seq2seq.txt\",\n",
        "                              valid_tweets, should_clean_saved_model=False)\n",
        "        trainer.train_seq2seq_swapped(conversations_large_backward_hparams,\n",
        "                                      \"conversations_large_seq2seq.txt\",\n",
        "                                      [\"この難にでも応用可能なひどいやつ\", \"おはようございます。明日はよろしくおねがいします。\"], vocab_path=\"conversations_large_seq2seq_vocab.txt\", should_clean_saved_model=False)\n",
        "\n",
        "        sq.Shell.copy_saved_model(conversations_large_hparams, conversations_large_rl_hparams)\n",
        "        sq.Trainer().train_rl(conversations_large_rl_hparams,\n",
        "                                conversations_large_hparams,\n",
        "                                conversations_large_backward_hparams,\n",
        "                                \"conversations_large_seq2seq.txt\",\n",
        "\n",
        "                                \"conversations_large_rl.txt\",\n",
        "                                valid_tweets)\n",
        "    except Exception as e:\n",
        "        print(stderr.getvalue())\n",
        "        raise (e)\n",
        "\n",
        "!ls - lSh\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T_q-Ns9hiHMB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# N.B: This would fail if we try to download logs in the previous cell.\n",
        "# My guess is tflog is somehow locking the log file when running the cell.\n",
        "#download_logs()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}