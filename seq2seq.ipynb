{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seq2seq.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/higepon/tensorflow_seq2seq_chatbot/blob/master/seq2seq.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "HWOxK9T5I8sb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Chatbot based on Seq2Seq Beam Search + Attention + Reinforcment Learning(Experimental)\n",
        "- Tensorflow 1.4.0+ is required.\n",
        "- This is based on [NMT Tutorial](https://github.com/tensorflow/nmt).\n",
        "- Experiment [notes](https://github.com/higepon/tensorflow_seq2seq_chatbot/wiki).\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "kK1r053SI2f9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Special commands should be located here.\n",
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "!apt-get -qq install -y mecab libmecab-dev mecab-ipadic mecab-ipadic-utf8\n",
        "\n",
        "!pip -q install git+https://github.com/mrahtz/easy-tf-log#egg=easy-tf-log[tf]\n",
        "!pip install pushbullet.py\n",
        "!pip install tweepy pyyaml\n",
        "!pip install mecab-python3\n",
        "\n",
        "def auth_google_drive():\n",
        "  # Generate creds for the Drive FUSE library.\n",
        "  if not os.path.exists('drive'):\n",
        "    from oauth2client.client import GoogleCredentials\n",
        "    creds = GoogleCredentials.get_application_default()\n",
        "    import getpass\n",
        "    !google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "    vcode = getpass.getpass()\n",
        "    !echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}  \n",
        "\n",
        "def mount_google_drive():\n",
        "  if not os.path.exists('drive'):\n",
        "    os.makedirs('drive', exist_ok=True)\n",
        "    !google-drive-ocamlfuse drive \n",
        "    \n",
        "def kill_docker():\n",
        "  !kill -9 -1  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "90XCqkUfbnUZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "response = urllib.request.urlopen(\"https://raw.githubusercontent.com/yaroslavvb/memory_util/master/memory_util.py\")\n",
        "open(\"memory_util.py\", \"wb\").write(response.read())\n",
        "import memory_util"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WE9v1UerJMRo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import copy as copy\n",
        "import datetime\n",
        "import hashlib\n",
        "import json\n",
        "import os\n",
        "import os.path\n",
        "import random\n",
        "import re\n",
        "import shutil\n",
        "from enum import Enum, auto\n",
        "\n",
        "import MeCab\n",
        "import easy_tf_log\n",
        "import matplotlib.pyplot as plt\n",
        "import random as random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tweepy\n",
        "import yaml\n",
        "from easy_tf_log import tflog\n",
        "from google.colab import auth\n",
        "from google.colab import files\n",
        "from pushbullet import Pushbullet\n",
        "from tensorflow.python.layers import core as layers_core\n",
        "from tensorflow.python.platform import gfile\n",
        "\n",
        "# Generate auth tokens for Colab\n",
        "auth.authenticate_user()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OoMe73Z51zNk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#kill_docker()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KapXwLNkJtH-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def client_id():\n",
        "    clients = {'dfc1d5b22ba03430800179d23e522f6f': 'client1',\n",
        "               'f8e857a2d792038820ebb2ae8d803f7c': 'client2',\n",
        "               '7628f983785173edabbde501ef8f781d': 'client3'}\n",
        "    with open('/content/datalab/adc.json') as json_data:\n",
        "        d = json.load(json_data)\n",
        "        email = d['id_token']['email'].encode('utf-8')\n",
        "        return clients[hashlib.md5(email).hexdigest()]\n",
        "\n",
        "\n",
        "print(client_id())\n",
        "current_client_id = client_id()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mM1uEwbYJPJK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "auth_google_drive()\n",
        "mount_google_drive()\n",
        "\n",
        "drive_path = 'drive/seq2seq_data'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bPLkjCHPSyGx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Mode(Enum):\n",
        "    Test = auto()\n",
        "    TrainSeq2Seq = auto()\n",
        "    TrainSeq2SeqSwapped = auto()\n",
        "    TrainRL = auto()\n",
        "    TweetBot = auto()\n",
        "    \n",
        "\n",
        "\n",
        "mode = Mode.Test\n",
        "#mode = Mode.TrainSeq2Seq\n",
        "#mode = Mode.TrainSeq2SeqSwapped\n",
        "#mode = Mode.TrainRL\n",
        "#mode = Mode.TweetBot\n",
        "\n",
        "\n",
        "class Shell:\n",
        "    @staticmethod\n",
        "    def download_file_if_necessary(file_name):\n",
        "        if os.path.exists(file_name):\n",
        "            return\n",
        "        print(\"downloading {}...\".format(file_name))\n",
        "        shutil.copy2(os.path.join(drive_path, file_name), file_name)\n",
        "        print(\"downloaded\")\n",
        "\n",
        "    @staticmethod\n",
        "    def download_model_data_if_necessary(model_path):\n",
        "        if not os.path.exists(model_path):\n",
        "            os.makedirs(model_path)\n",
        "        print(\"Downloading model files...\")\n",
        "        src_dir = os.path.join(drive_path, model_path)\n",
        "        Shell.copy_all_files(src_dir, model_path)\n",
        "        print(\"done\")\n",
        "\n",
        "    @staticmethod\n",
        "    def copy_all_files(src_dir, dst_dir):\n",
        "        if os.path.exists(src_dir):\n",
        "            for f in os.listdir(src_dir):\n",
        "                shutil.copy2(os.path.join(src_dir, f), os.path.join(dst_dir, f))\n",
        "\n",
        "    @staticmethod\n",
        "    def remove_all_files(target_dir):\n",
        "        for f in Shell.listdir(target_dir):\n",
        "            os.remove(f)\n",
        "\n",
        "    @staticmethod\n",
        "    def remove_matched_files(target_dir, pattern):\n",
        "        for f in Shell.listdir(target_dir):\n",
        "            if re.match(pattern, f):\n",
        "                os.remove(f)\n",
        "\n",
        "    @staticmethod\n",
        "    def download_logs(path):\n",
        "        for f in Shell.listdir(path):\n",
        "            if re.match('.*events', f):\n",
        "                files.download(f)\n",
        "\n",
        "    @staticmethod\n",
        "    def remove_saved_model(hparams):\n",
        "        os.makedirs(hparams.model_path, exist_ok=True)\n",
        "        Shell.remove_all_files(hparams.model_path)\n",
        "        os.makedirs(os.path.join(drive_path, hparams.model_path), exist_ok=True)\n",
        "        Shell.remove_all_files(os.path.join(drive_path, hparams.model_path))\n",
        "\n",
        "    @staticmethod\n",
        "    def copy_saved_model(src_hparams, dst_hparams):\n",
        "        Shell.copy_all_files(src_hparams.model_path, dst_hparams.model_path)\n",
        "        # rm tf.logs from source so that it wouldn't be mixed in dest tf.logs.\n",
        "        Shell.remove_matched_files(dst_hparams.model_path, \".*events.*\")\n",
        "\n",
        "    @staticmethod\n",
        "    def listdir(target_dir):\n",
        "        for dir_path, _, file_names in os.walk(target_dir):\n",
        "            for f in file_names:\n",
        "                yield os.path.abspath(os.path.join(dir_path, f))\n",
        "\n",
        "    @staticmethod\n",
        "    def list_model_file(path):\n",
        "        f = open('{}/checkpoint'.format(path))\n",
        "        text = f.read()\n",
        "        f.close()\n",
        "        print(text)\n",
        "        m = re.match(r\".*ChatbotModel\\-(\\d+)\", text)\n",
        "        model_name = m.group(1)\n",
        "        files = [\"checkpoint\"]\n",
        "        files.extend([x for x in os.listdir(path) if\n",
        "                      re.search(model_name, x) or re.search('events.out', x)])\n",
        "        return files\n",
        "\n",
        "    @staticmethod\n",
        "    def save_model_in_drive(model_path):\n",
        "        path = os.path.join(drive_path, model_path)\n",
        "        os.makedirs(path, exist_ok=True)\n",
        "        Shell.remove_all_files(os.path.join(drive_path, model_path))\n",
        "        print(\"Saving model in Google Drive...\")\n",
        "        for file in Shell.list_model_file(model_path):\n",
        "            print(\"Saving \", file)\n",
        "            shutil.copy2(os.path.join(model_path, file),\n",
        "                         os.path.join(drive_path, model_path, file))\n",
        "        print(\"done\")\n",
        "\n",
        "\n",
        "config_path = 'config.yml'\n",
        "Shell.download_file_if_necessary(config_path)\n",
        "f = open(config_path, 'rt')\n",
        "push_key = yaml.load(f)['pushbullet']['api_key']\n",
        "\n",
        "pb = Pushbullet(push_key)\n",
        "\n",
        "# Note for myself.\n",
        "# You've summarized Seq2Seq\n",
        "# at http://d.hatena.ne.jp/higepon/20171210/1512887715.\n",
        "\n",
        "# If you see following error, it means your max(len(tweets of training set)) <  decoder_length.\n",
        "# This should be a bug somewhere in build_decoder, but couldn't find one yet.\n",
        "# You can workaround by setting hparams.decoder_length=max len of tweet in training set.\n",
        "# InvalidArgumentError: logits and labels must have the same first dimension, got logits shape [48,50] and labels shape [54]\n",
        "#\t [[Node: root/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits = SparseSoftmaxCrossEntropyWithLogits[T=DT_FLOAT, Tlabels=DT_INT32, \n",
        "\n",
        "print(tf.__version__)\n",
        "\n",
        "def info(message, hparams):\n",
        "    if hparams.debug_verbose:\n",
        "        print(message)\n",
        "\n",
        "\n",
        "def has_gpu0():\n",
        "    return tf.test.gpu_device_name() == \"/device:GPU:0\"\n",
        "\n",
        "class ModelDirectory(Enum):\n",
        "    tweet_large = 'model/tweet_large'\n",
        "    tweet_large_rl = 'model/tweet_large_rl'\n",
        "    tweet_large_swapped = 'model/tweet_large_swapped'\n",
        "    tweet_small = 'model/tweet_small'\n",
        "    tweet_small_swapped = 'model/tweet_small_swapped'\n",
        "    tweet_small_rl = 'model/tweet_small_rl'\n",
        "    conversations_small = 'model/conversations_small'\n",
        "    conversations_small_backward = 'model/conversations_small_backward'\n",
        "    conversations_small_rl = 'model/conversations_small_rl'\n",
        "    test_multiple1 = 'model/test_multiple1'\n",
        "    test_multiple2 = 'model/test_multiple2'\n",
        "    test_multiple3 = 'model/test_multiple3'\n",
        "    test_distributed = 'model/test_distributed'\n",
        "\n",
        "    @staticmethod\n",
        "    def create_all_directories():\n",
        "        for d in ModelDirectory:\n",
        "            os.makedirs(d.value, exist_ok=True)\n",
        "\n",
        "\n",
        "# todo\n",
        "# collect all initializer\n",
        "ModelDirectory.create_all_directories()\n",
        "\n",
        "base_hparams = tf.contrib.training.HParams(\n",
        "    machine=current_client_id,\n",
        "    batch_size=3,\n",
        "    num_units=6,\n",
        "    num_layers=2,\n",
        "    vocab_size=9,\n",
        "    embedding_size=8,\n",
        "    learning_rate=0.01,\n",
        "    learning_rate_decay=0.99,\n",
        "    use_attention=False,\n",
        "    encoder_length=5,\n",
        "    decoder_length=5,\n",
        "    max_gradient_norm=5.0,\n",
        "    beam_width=2,\n",
        "    num_train_steps=100,\n",
        "    debug_verbose=False,\n",
        "    model_path='Please override model_directory',\n",
        "    sos_id=0,\n",
        "    eos_id=1,\n",
        "    pad_id=2,\n",
        "    unk_id=3,\n",
        "    sos_token=\"[SOS]\",\n",
        "    eos_token=\"[EOS]\",\n",
        "    pad_token=\"[PAD]\",\n",
        "    unk_token=\"[UNK]\",\n",
        ")\n",
        "\n",
        "test_hparams = copy.deepcopy(base_hparams).override_from_dict(\n",
        "    {'beam_width': 0, 'num_train_steps': 100, 'learning_rate': 0.5})\n",
        "\n",
        "test_attention_hparams = copy.deepcopy(test_hparams).override_from_dict(\n",
        "    {'use_attention': True})\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fNrRD9yOFXM6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def print_hparams(hparams):\n",
        "    result = {}\n",
        "    for key in ['machine', 'batch_size', 'num_units', 'num_layers',\n",
        "                'vocab_size',\n",
        "                'embedding_size', 'learning_rate', 'learning_rate_decay',\n",
        "                'use_attention', 'encoder_length', 'decoder_length',\n",
        "                'max_gradient_norm', 'beam_width', 'num_train_steps',\n",
        "                'model_path']:\n",
        "        result[key] = hparams.get(key)\n",
        "    print(result)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zCxiZgp9h8AH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# For debug purpose.\n",
        "tf.reset_default_graph()\n",
        "\n",
        "\n",
        "class ChatbotModel:\n",
        "    def __init__(self, sess, hparams, model_path, scope='ChatbotModel'):\n",
        "        self.sess = sess\n",
        "        # todo remove\n",
        "        self.hparams = hparams\n",
        "\n",
        "        # todo\n",
        "        self.model_path = model_path\n",
        "        self.scope = scope\n",
        "        # Sampled replies in previous session,\n",
        "        # this is necessary to back propagation.\n",
        "        self.sampled = tf.placeholder(tf.int32, name=\"sampled\")\n",
        "\n",
        "        # Used to store previously inferred by beam_search.\n",
        "        self.beam_predicted_ids = tf.placeholder(tf.int32,\n",
        "                                                 name=\"beam_predicted_ids\")\n",
        "        self.enc_inputs, self.enc_inputs_lengths, enc_outputs, enc_state, emb_encoder = self._build_encoder(\n",
        "            hparams, scope)\n",
        "\n",
        "        self.dec_inputs, self.dec_tgt_lengths, self._logits, self.sample_logits, self.sample_replies, self.sample_log_prob, self.infer_logits, self.replies, self.beam_replies, self.log_probs_beam_op = self._build_decoder(\n",
        "            hparams, self.enc_inputs_lengths, emb_encoder,\n",
        "            enc_state, enc_outputs)\n",
        "        self._log_probs = tf.nn.log_softmax(self.infer_logits)\n",
        "\n",
        "        self.reward = tf.placeholder(tf.float32, name=\"reward\")\n",
        "        self.tgt_labels, self.global_step, self.loss, self.train_op = self._build_seq2seq_optimizer(\n",
        "            hparams, self._logits)\n",
        "        self.rl_loss, self.rl_train_op = self._build_rl_optimizer(hparams)\n",
        "        self.beam_rl_loss, self.beam_rl_train_op = self._build_beam_rl_optimizer(\n",
        "            hparams)\n",
        "\n",
        "        self.train_loss_summary = tf.summary.scalar(\"loss\", self.loss)\n",
        "        self.val_loss_summary = tf.summary.scalar(\"validation_loss\",\n",
        "                                                  self.loss)\n",
        "        self.merged_summary = tf.summary.merge_all()\n",
        "\n",
        "        # Initialize saver after model created\n",
        "        self.saver = tf.train.Saver(tf.global_variables(), max_to_keep=1)\n",
        "\n",
        "    def train(self, enc_inputs, enc_inputs_lengths, target_labels,\n",
        "              dec_inputs, dec_target_lengths):\n",
        "\n",
        "        feed_dict = {\n",
        "            self.enc_inputs: enc_inputs,\n",
        "            self.enc_inputs_lengths: enc_inputs_lengths,\n",
        "            self.tgt_labels: target_labels,\n",
        "            self.dec_inputs: dec_inputs,\n",
        "            self.dec_tgt_lengths: dec_target_lengths,\n",
        "        }\n",
        "        _, global_step, summary = self.sess.run(\n",
        "            [self.train_op, self.global_step, self.train_loss_summary],\n",
        "            feed_dict=feed_dict)\n",
        "\n",
        "        return global_step, summary\n",
        "\n",
        "    def infer(self, enc_inputs, enc_inputs_lengths):\n",
        "        infer_feed_dic = {\n",
        "            self.enc_inputs: enc_inputs,\n",
        "            self.enc_inputs_lengths: enc_inputs_lengths,\n",
        "        }\n",
        "        return self.sess.run(self.replies, feed_dict=infer_feed_dic)\n",
        "\n",
        "    def log_probs(self, enc_inputs, enc_inputs_lengths):\n",
        "        print(\"enc_inputs.shape\", enc_inputs.shape)    \n",
        "        print(\"enc_inputs_lengths\", enc_inputs_lengths)\n",
        "        infer_feed_dic = {\n",
        "            self.enc_inputs: enc_inputs,\n",
        "            self.enc_inputs_lengths: enc_inputs_lengths,\n",
        "        }\n",
        "        return self.sess.run(self._log_probs, feed_dict=infer_feed_dic)\n",
        "\n",
        "    def infer_beam_search(self, enc_inputs, enc_inputs_lengths):\n",
        "        \"\"\"\n",
        "        :return: (replies: [batch_size, decoder_length, beam_size],\n",
        "                  logits: [batch_size, decoder_length, vocab_size]))\n",
        "        \"\"\"\n",
        "        infer_feed_dic = {\n",
        "            self.enc_inputs: enc_inputs,\n",
        "            self.enc_inputs_lengths: enc_inputs_lengths,\n",
        "        }\n",
        "        return self.sess.run([self.beam_replies, self.infer_logits], \n",
        "                             feed_dict=infer_feed_dic)\n",
        "\n",
        "    def sample(self, enc_inputs, enc_inputs_lengths):\n",
        "        infer_feed_dic = {\n",
        "            self.enc_inputs: enc_inputs,\n",
        "            self.enc_inputs_lengths: enc_inputs_lengths,\n",
        "        }\n",
        "\n",
        "        replies, logits = self.sess.run(\n",
        "            [self.sample_replies, self.sample_logits],\n",
        "            feed_dict=infer_feed_dic)\n",
        "        return replies, logits\n",
        "\n",
        "    def batch_loss(self, enc_inputs, enc_inputs_lengths, tgt_labels,\n",
        "                   dec_inputs, dec_tgt_lengths):\n",
        "        feed_dict = {\n",
        "            self.enc_inputs: enc_inputs,\n",
        "            self.enc_inputs_lengths: enc_inputs_lengths,\n",
        "            self.tgt_labels: tgt_labels,\n",
        "            self.dec_inputs: dec_inputs,\n",
        "            self.dec_tgt_lengths: dec_tgt_lengths,\n",
        "        }\n",
        "        return self.sess.run([self.loss, self.val_loss_summary],\n",
        "                             feed_dict=feed_dict)\n",
        "\n",
        "    def seq_len(self, seq):\n",
        "        try:\n",
        "            # length includes the first eos_id.\n",
        "            return seq.index(self.hparams.eos_id) + 1\n",
        "        except ValueError:\n",
        "            return self.hparams.encoder_length\n",
        "\n",
        "    def train_with_reward(self, enc_inputs, enc_inputs_lengths, sampled,\n",
        "                          reward):\n",
        "        feed_dict = {\n",
        "            self.enc_inputs: enc_inputs,\n",
        "            self.enc_inputs_lengths: enc_inputs_lengths,\n",
        "            self.sampled: sampled,\n",
        "            self.reward: reward\n",
        "        }\n",
        "\n",
        "        _, global_step = self.sess.run(\n",
        "            [self.rl_train_op, self.global_step],\n",
        "            feed_dict=feed_dict)\n",
        "        return global_step\n",
        "\n",
        "    def log_probs_beam(self, enc_inputs, enc_inputs_lengths,\n",
        "                       beam_predicted_ids):\n",
        "        \"\"\" Return log_probs_beam: [[batch_size, dec_length, beam_size]\n",
        "            This is used in rl training, where we need P_seq2seq(a| p_i, q_i)\n",
        "        \"\"\"\n",
        "        feed_dict = {\n",
        "            self.enc_inputs: enc_inputs,\n",
        "            self.enc_inputs_lengths: enc_inputs_lengths,\n",
        "            self.beam_predicted_ids: beam_predicted_ids,\n",
        "        }\n",
        "\n",
        "        return self.sess.run(self.log_probs_beam_op, feed_dict=feed_dict)\n",
        "\n",
        "    def train_beam_with_reward(self, enc_inputs, enc_inputs_lengths,\n",
        "                               beam_predicted_ids,\n",
        "                               reward):\n",
        "        feed_dict = {\n",
        "            self.enc_inputs: enc_inputs,\n",
        "            self.enc_inputs_lengths: enc_inputs_lengths,\n",
        "            self.beam_predicted_ids: beam_predicted_ids,\n",
        "            self.reward: reward\n",
        "        }\n",
        "\n",
        "        _, global_step = self.sess.run(\n",
        "            [self.beam_rl_train_op, self.global_step],\n",
        "            feed_dict=feed_dict)\n",
        "        return global_step\n",
        "\n",
        "    def reward_ease_of_answering(self, max_len, enc_inputs,\n",
        "                                 enc_inputs_lengths,\n",
        "                                 dull_responses):\n",
        "        \"\"\" Return reward for ease of answering.\n",
        "            See Deep Reinforcement Learning for Dialogue Generation\n",
        "            for more details.\n",
        "\n",
        "        Args:\n",
        "            enc_inputs: [encoder_length, batch_size], eg) tweets\n",
        "            dull_responses: [number of pre-defined dull responses,\n",
        "            decoder_length or less than decoder_length].\n",
        "            eg) [[\"I'm\", \"Good\"], [\"fine\"]]\n",
        "\n",
        "        Returns:\n",
        "            Return reward for ease of answering.\n",
        "        \"\"\"\n",
        "        inference_feed_dict = {\n",
        "            self.enc_inputs: enc_inputs,\n",
        "            self.enc_inputs_lengths: enc_inputs_lengths\n",
        "        }\n",
        "\n",
        "        # Logits\n",
        "        #   logits_value: [batch_size, actual_decoder_length, vocab_size]\n",
        "        logits_batch_value = self.sess.run(self.infer_logits,\n",
        "                                           feed_dict=inference_feed_dict)\n",
        "\n",
        "        # Note that encoder_inputs here is time major.\n",
        "        reward = np.ones((self.hparams.batch_size, max_len))\n",
        "        print(\n",
        "            \"constructing reward{}{}\".format(self.hparams.batch_size, max_len))\n",
        "        # For each batch: [actual_decoder_length, vocab_size]\n",
        "        for i, logits in enumerate(logits_batch_value):\n",
        "            p_array = []\n",
        "            for dull_response in dull_responses:\n",
        "                p = 1\n",
        "                # Note that dull_response and _logits don't\n",
        "                # always have same length, but zip takes care of the case.\n",
        "                for word_id, logit in zip(dull_response, logits):\n",
        "                    # Apply softmax first, see definition of softmax.\n",
        "                    norm = (self._softmax(logit))[word_id]\n",
        "                    p *= norm\n",
        "                # This is P(dull_response|encoder_input)\n",
        "                p = np.log(p) / len(dull_response)\n",
        "                p_array.append(p)\n",
        "            batch_p = np.sum(p_array) / len(dull_responses)\n",
        "            batch_reward = -batch_p\n",
        "            for j in range(max_len):\n",
        "                reward[i][j] = batch_reward\n",
        "        return reward\n",
        "\n",
        "    def save(self, model_path=None):\n",
        "        if model_path is None:\n",
        "            model_path = self.model_path\n",
        "        model_dir = \"{}/{}\".format(model_path, self.scope)\n",
        "        self.saver.save(self.sess, model_dir, global_step=self.global_step)\n",
        "\n",
        "    def restore(self):\n",
        "        ckpt = tf.train.get_checkpoint_state(self.model_path)\n",
        "        if ckpt:\n",
        "            last_model = ckpt.model_checkpoint_path\n",
        "            self.saver.restore(self.sess, last_model)\n",
        "            return True\n",
        "        else:\n",
        "            print(\"Created fresh model.\")\n",
        "            return False\n",
        "\n",
        "    @staticmethod\n",
        "    def _softmax(x):\n",
        "        return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
        "\n",
        "    def _build_rl_optimizer(self, hparams):\n",
        "        # todo mask the sampling results\n",
        "        sample_log_prob_shape = tf.shape(self.sample_log_prob)\n",
        "        reward_shape = tf.shape(self.reward)\n",
        "        reward_shape_print = tf.Print(reward_shape,\n",
        "                                      [reward_shape],\n",
        "                                      message=\"reward_shape\")\n",
        "        reward_print = tf.Print(self.reward,\n",
        "                                [self.reward],\n",
        "                                message=\"reward\")\n",
        "\n",
        "        asserts = [tf.assert_equal(sample_log_prob_shape[0],\n",
        "                                   reward_shape_print[0],\n",
        "                                   [self.sample_log_prob,\n",
        "                                    self.reward]),\n",
        "                   tf.assert_equal(sample_log_prob_shape[1],\n",
        "                                   reward_shape_print[1],\n",
        "                                   [self.sample_log_prob,\n",
        "                                    self.reward]), reward_print\n",
        "                   ]\n",
        "        with tf.control_dependencies(asserts):\n",
        "            loss = -tf.reduce_sum(\n",
        "                self.sample_log_prob * self.reward) / tf.to_float(\n",
        "                hparams.batch_size)\n",
        "        train_op = self._build_optimizer_with_loss(self.global_step, hparams,\n",
        "                                                   loss)\n",
        "        return loss, train_op\n",
        "\n",
        "    def _build_beam_rl_optimizer(self, hparams):\n",
        "        # todo mask the sampling results\n",
        "        beam_log_probs_shape = tf.shape(self.log_probs_beam_op)\n",
        "        beam_log_probs_shape_print = tf.Print(beam_log_probs_shape,\n",
        "                                              [beam_log_probs_shape,\n",
        "                                               tf.shape(self.infer_logits),\n",
        "                                               tf.shape(\n",
        "                                                   self.beam_predicted_ids),\n",
        "                                               tf.shape(self.beam_replies)],\n",
        "                                              \"beam_log_probs_shape\")\n",
        "        reward_shape = tf.shape(self.reward)\n",
        "        reward_shape_print = tf.Print(reward_shape, [reward_shape],\n",
        "                                      message=\"reward_shape\")\n",
        "        asserts = [tf.assert_equal(beam_log_probs_shape_print,\n",
        "                                   reward_shape_print,\n",
        "                                   [beam_log_probs_shape_print,\n",
        "                                    self.reward])]\n",
        "        with tf.control_dependencies(asserts):\n",
        "            loss = -tf.reduce_sum(\n",
        "                self.log_probs_beam_op * self.reward) / tf.to_float(\n",
        "                hparams.batch_size * hparams.beam_width)\n",
        "        train_op = self._build_optimizer_with_loss(self.global_step, hparams,\n",
        "                                                   loss)\n",
        "        return loss, train_op\n",
        "\n",
        "    def _build_optimizer_with_loss(self, global_step, hparams, loss):\n",
        "        params = tf.trainable_variables()\n",
        "        optimizer = tf.train.GradientDescentOptimizer(hparams.learning_rate)\n",
        "        gradients = tf.gradients(loss, params)\n",
        "        clipped_gradients, _ = tf.clip_by_global_norm(\n",
        "            gradients, hparams.max_gradient_norm)\n",
        "        with tf.device(self.available_device()):\n",
        "            train_op = optimizer.apply_gradients(\n",
        "                zip(clipped_gradients, params), global_step=global_step)\n",
        "        return train_op\n",
        "\n",
        "    def _build_seq2seq_optimizer(self, hparams, logits):\n",
        "        # Target labels\n",
        "        #   As described in doc for sparse_softmax_cross_entropy_with_logits,\n",
        "        #   labels should be [batch_size, decoder_target_lengths]\n",
        "        #   instead of [batch_size, decoder_target_lengths, vocab_size].\n",
        "        #   So labels should have indices instead of vocab_size classes.\n",
        "        tgt_labels = tf.placeholder(tf.int32, shape=(\n",
        "            hparams.batch_size, hparams.decoder_length), name=\"tgt_labels\")\n",
        "        # Loss\n",
        "        #   tgt_labels: [batch_size, decoder_length]\n",
        "        #   _logits: [batch_size, decoder_length, vocab_size]\n",
        "        #   crossent: [batch_size, decoder_length]\n",
        "        crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "            labels=tgt_labels, logits=logits)\n",
        "        tgt_weights = tf.sequence_mask(self.dec_tgt_lengths,\n",
        "                                       hparams.decoder_length,\n",
        "                                       dtype=logits.dtype)\n",
        "        crossent = crossent * tgt_weights\n",
        "        crossent_by_batch = tf.reduce_sum(crossent, axis=1)\n",
        "        loss = tf.reduce_sum(crossent_by_batch) / tf.to_float(\n",
        "            hparams.batch_size)\n",
        "        # Train\n",
        "        global_step = tf.get_variable(name=\"global_step\", shape=[],\n",
        "                                      dtype=tf.int32,\n",
        "                                      initializer=tf.constant_initializer(0),\n",
        "                                      trainable=False)\n",
        "        train_op = self._build_optimizer_with_loss(global_step, hparams, loss)\n",
        "        return tgt_labels, global_step, loss, train_op\n",
        "\n",
        "    @staticmethod\n",
        "    def available_device():\n",
        "        device = '/cpu:0'\n",
        "        if has_gpu0():\n",
        "            device = '/gpu:0'\n",
        "            print(\"$$$ GPU ENABLED $$$\")\n",
        "        return device\n",
        "\n",
        "    @staticmethod\n",
        "    def _build_encoder(hparams, scope):\n",
        "        # Encoder\n",
        "        #   enc_inputs: [encoder_length, batch_size]\n",
        "        #   This is time major where encoder_length comes\n",
        "        #   first instead of batch_size.\n",
        "        #   enc_inputs_lengths: [batch_size]\n",
        "        enc_inputs = tf.placeholder(tf.int32, shape=(\n",
        "            hparams.encoder_length, hparams.batch_size), name=\"enc_inputs\")\n",
        "        enc_inputs_lengths = tf.placeholder(tf.int32,\n",
        "                                            shape=hparams.batch_size,\n",
        "                                            name=\"enc_inputs_lengths\")\n",
        "\n",
        "        # Embedding\n",
        "        #   We originally didn't share embedding between encoder and decoder.\n",
        "        #   But now we share it. It makes much easier to calculate rewards.\n",
        "        #   Matrix for embedding: [vocab_size, embedding_size]\n",
        "        #   Should be shared between training and inference.\n",
        "        with tf.variable_scope(scope):\n",
        "            emb_encoder = tf.get_variable(\"emb_encoder\",\n",
        "                                          [hparams.vocab_size,\n",
        "                                           hparams.embedding_size])\n",
        "\n",
        "        # Look up embedding:\n",
        "        #   enc_inputs: [encoder_length, batch_size]\n",
        "        #   enc_emb_inputs: [encoder_length, batch_size, embedding_size]\n",
        "        enc_emb_inputs = tf.nn.embedding_lookup(emb_encoder, enc_inputs)\n",
        "\n",
        "        # LSTM cell.\n",
        "        with tf.variable_scope(scope):\n",
        "            # Should be shared between training and inference.\n",
        "            cells = []\n",
        "            for _ in range(hparams.num_layers):\n",
        "                cells.append(\n",
        "                    tf.nn.rnn_cell.BasicLSTMCell(hparams.num_units))\n",
        "            encoder_cell = tf.contrib.rnn.MultiRNNCell(cells)\n",
        "\n",
        "            # Run Dynamic RNN\n",
        "            #   enc_outputs: [encoder_length, batch_size, num_units]\n",
        "            #   enc_state: [batch_size, num_units],\n",
        "            #   this is final state of the cell for each batch.\n",
        "            enc_outputs, enc_state = tf.nn.dynamic_rnn(encoder_cell,\n",
        "                                                       enc_emb_inputs,\n",
        "                                                       time_major=True,\n",
        "                                                       dtype=tf.float32,\n",
        "                                                       sequence_length=enc_inputs_lengths)\n",
        "\n",
        "        return enc_inputs, enc_inputs_lengths, enc_outputs, enc_state, emb_encoder\n",
        "\n",
        "    @staticmethod\n",
        "    def _build_training_decoder(hparams, enc_inputs_lengths,\n",
        "                                enc_state, enc_outputs, dec_cell,\n",
        "                                dec_emb_inputs, dec_tgt_lengths,\n",
        "                                projection_layer, scope):\n",
        "\n",
        "        dynamic_batch_size = tf.shape(enc_inputs_lengths)[0]\n",
        "        initial_state, wrapped_dec_cell = ChatbotModel._attention_wrapper(\n",
        "            dec_cell, dynamic_batch_size, enc_inputs_lengths, enc_outputs,\n",
        "            enc_state, hparams, scope, reuse=False)\n",
        "\n",
        "        # Decoder with helper:\n",
        "        #   dec_emb_inputs: [decoder_length, batch_size, embedding_size]\n",
        "        #   dec_tgt_lengths: [batch_size] vector,\n",
        "        #   which represents each target sequence length.\n",
        "        with tf.variable_scope(scope):\n",
        "            training_helper = tf.contrib.seq2seq.TrainingHelper(\n",
        "                dec_emb_inputs,\n",
        "                dec_tgt_lengths,\n",
        "                time_major=True)\n",
        "\n",
        "        # Decoder and decode\n",
        "        with tf.variable_scope(scope):\n",
        "            with tf.variable_scope(\"training\"):\n",
        "                training_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
        "                    wrapped_dec_cell, training_helper, initial_state,\n",
        "                    output_layer=projection_layer)\n",
        "\n",
        "        # Dynamic decoding\n",
        "        #   final_outputs.rnn_output: [batch_size, decoder_length,\n",
        "        #                             vocab_size], list of RNN state.\n",
        "        #   final_outputs.sample_id: [batch_size, decoder_length],\n",
        "        #                            list of argmax of rnn_output.\n",
        "        #   final_state: [batch_size, num_units],\n",
        "        #                list of final state of RNN on decode process.\n",
        "        #   final_sequence_lengths: [batch_size], list of each decoded sequence.\n",
        "        with tf.variable_scope(scope):\n",
        "            final_outputs, _final_state, _final_sequence_lengths = tf.contrib.seq2seq.dynamic_decode(\n",
        "                training_decoder)\n",
        "\n",
        "        if hparams.debug_verbose:\n",
        "            print(\"rnn_output.shape=\", final_outputs.rnn_output.shape)\n",
        "            print(\"sample_id.shape=\", final_outputs.sample_id.shape)\n",
        "            print(\"final_state=\", _final_state)\n",
        "            print(\"final_sequence_lengths.shape=\",\n",
        "                  _final_sequence_lengths.shape)\n",
        "\n",
        "        logits = final_outputs.rnn_output\n",
        "        return logits, wrapped_dec_cell, initial_state\n",
        "\n",
        "    def _build_decoder(self, hparams, enc_inputs_lengths, embedding_encoder,\n",
        "                       enc_state, enc_outputs):\n",
        "        # Decoder input\n",
        "        #   dec_inputs: [decoder_length, batch_size]\n",
        "        #   dec_tgt_lengths: [batch_size]\n",
        "        #   This is grand truth target inputs for training.\n",
        "        dec_inputs = tf.placeholder(tf.int32, shape=(\n",
        "            hparams.decoder_length, hparams.batch_size), name=\"dec_inputs\")\n",
        "        dec_tgt_lengths = tf.placeholder(tf.int32,\n",
        "                                         shape=hparams.batch_size,\n",
        "                                         name=\"dec_tgt_lengths\")\n",
        "\n",
        "        # Look up embedding:\n",
        "        #   dec_inputs: [decoder_length, batch_size]\n",
        "        #   decoder_emb_inp: [decoder_length, batch_size, embedding_size]\n",
        "        dec_emb_inputs = tf.nn.embedding_lookup(embedding_encoder,\n",
        "                                                dec_inputs)\n",
        "\n",
        "        # https://stackoverflow.com/questions/39573188/output-projection-in-seq2seq-model-tensorflow\n",
        "        # Internally, a neural network operates on dense vectors of some size,\n",
        "        # often 256, 512 or 1024 floats (let's say 512 for here).\n",
        "        # But at the end it needs to predict a word\n",
        "        # from the vocabulary which is often much larger,\n",
        "        # e.g., 40000 words. Output projection is the final linear layer\n",
        "        # that converts (projects) from the internal representation\n",
        "        #  to the larger one.\n",
        "        # So, for example, it can consist of a 512 x 40000 parameter matrix\n",
        "        # and a 40000 parameter for the bias vector.\n",
        "        projection_layer = layers_core.Dense(hparams.vocab_size, use_bias=False)\n",
        "\n",
        "        # We share this between training and inference.\n",
        "        cells = []\n",
        "        for _ in range(hparams.num_layers):\n",
        "            cells.append(tf.nn.rnn_cell.BasicLSTMCell(hparams.num_units))\n",
        "        dec_cell = tf.contrib.rnn.MultiRNNCell(cells)\n",
        "\n",
        "        # Training graph\n",
        "        logits, wrapped_dec_cell, initial_state = self._build_training_decoder(\n",
        "            hparams, enc_inputs_lengths, enc_state, enc_outputs,\n",
        "            dec_cell, dec_emb_inputs, dec_tgt_lengths,\n",
        "            projection_layer, self.scope)\n",
        "\n",
        "        infer_logits, replies = self._build_greedy_inference(hparams,\n",
        "                                                             embedding_encoder,\n",
        "                                                             enc_state,\n",
        "                                                             enc_inputs_lengths,\n",
        "                                                             enc_outputs,\n",
        "                                                             dec_cell,\n",
        "                                                             projection_layer,\n",
        "                                                             self.scope)\n",
        "\n",
        "        # Beam Search Inference graph\n",
        "        beam_replies = self._build_beam_search_inference(hparams,\n",
        "                                                         enc_inputs_lengths,\n",
        "                                                         embedding_encoder,\n",
        "                                                         enc_state,\n",
        "                                                         enc_outputs,\n",
        "                                                         dec_cell,\n",
        "                                                         projection_layer,\n",
        "                                                         self.scope)\n",
        "\n",
        "        beam_log_probs = self._log_probs_beam(infer_logits,\n",
        "                                              self.beam_predicted_ids)\n",
        "\n",
        "        # Sample Inference graph\n",
        "        sample_logits, sample_replies = self._build_sample_inference(hparams,\n",
        "                                                                     embedding_encoder,\n",
        "                                                                     enc_state,\n",
        "                                                                     enc_inputs_lengths,\n",
        "                                                                     enc_outputs,\n",
        "                                                                     dec_cell,\n",
        "                                                                     projection_layer,\n",
        "                                                                     self.scope)\n",
        "        indices = self._convert_indices(self.sampled)\n",
        "        #        print_indices0 = tf.Print(indices, [tf.shape(indices)],\n",
        "        #                                  message=\"OPT:indices.shape\")\n",
        "        #        print_indices1 = tf.Print(print_indices0, [tf.shape(sample_logits)],\n",
        "        #                                  message=\"OPT:sample_logits.shape\")\n",
        "        #        print_indices2 = tf.Print(print_indices1, [tf.shape(sample_replies)],\n",
        "        #                                  message=\"OPT:sample_replies.shape\")\n",
        "\n",
        "        sample_log_prob = tf.gather_nd(sample_logits, indices)\n",
        "        sample_log_prob0 = tf.Print(sample_log_prob,\n",
        "                                    [tf.shape(sample_log_prob)],\n",
        "                                    message=\"OPT:sample_log_prob\")\n",
        "        return dec_inputs, dec_tgt_lengths, logits, sample_logits, sample_replies, sample_log_prob0, infer_logits, replies, beam_replies, beam_log_probs\n",
        "\n",
        "    @staticmethod\n",
        "    def _build_greedy_inference(hparams, embedding_encoder, enc_state,\n",
        "                                encoder_inputs_lengths, encoder_outputs,\n",
        "                                dec_cell, projection_layer, scope):\n",
        "        # Greedy decoder\n",
        "        dynamic_batch_size = tf.shape(encoder_inputs_lengths)[0]\n",
        "        inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n",
        "            embedding_encoder,\n",
        "            tf.fill([dynamic_batch_size], hparams.sos_id), hparams.eos_id)\n",
        "\n",
        "        infer_logits, replies = ChatbotModel._dynamic_decode(dec_cell,\n",
        "                                                             dynamic_batch_size,\n",
        "                                                             encoder_inputs_lengths,\n",
        "                                                             encoder_outputs,\n",
        "                                                             enc_state,\n",
        "                                                             hparams,\n",
        "                                                             inference_helper,\n",
        "                                                             projection_layer,\n",
        "                                                             scope)\n",
        "        return infer_logits, replies\n",
        "\n",
        "    @staticmethod\n",
        "    def _build_beam_search_inference(hparams, encoder_inputs_lengths,\n",
        "                                     embedding_encoder, enc_state,\n",
        "                                     encoder_outputs, dec_cell,\n",
        "                                     projection_layer, scope):\n",
        "\n",
        "        assert (hparams.beam_width != 0)\n",
        "\n",
        "        dynamic_batch_size = tf.shape(encoder_inputs_lengths)[0]\n",
        "        # https://github.com/tensorflow/tensorflow/issues/11904\n",
        "        if hparams.use_attention:\n",
        "            with tf.variable_scope(scope, reuse=True):\n",
        "                # Attention\n",
        "                # encoder_outputs is time major, so transopse it to batch major.\n",
        "                # attention_encoder_outputs: [batch_size, encoder_length, num_units]\n",
        "                attention_encoder_outputs = tf.transpose(encoder_outputs,\n",
        "                                                         [1, 0, 2])\n",
        "\n",
        "                tiled_encoder_outputs = tf.contrib.seq2seq.tile_batch(\n",
        "                    attention_encoder_outputs, multiplier=hparams.beam_width)\n",
        "                tiled_encoder_final_state = tf.contrib.seq2seq.tile_batch(\n",
        "                    enc_state, multiplier=hparams.beam_width)\n",
        "                tiled_encoder_inputs_lengths = tf.contrib.seq2seq.tile_batch(\n",
        "                    encoder_inputs_lengths, multiplier=hparams.beam_width)\n",
        "\n",
        "                # Create an attention mechanism\n",
        "                attention_mechanism = tf.contrib.seq2seq.LuongAttention(\n",
        "                    hparams.num_units, tiled_encoder_outputs,\n",
        "                    memory_sequence_length=tiled_encoder_inputs_lengths)\n",
        "\n",
        "                wrapped_de_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
        "                    dec_cell, attention_mechanism,\n",
        "                    attention_layer_size=hparams.num_units)\n",
        "\n",
        "                dec_initial_state = wrapped_de_cell.zero_state(\n",
        "                    dtype=tf.float32,\n",
        "                    batch_size=dynamic_batch_size * hparams.beam_width)\n",
        "                dec_initial_state = dec_initial_state.clone(\n",
        "                    cell_state=tiled_encoder_final_state)\n",
        "        else:\n",
        "            with tf.variable_scope(scope, reuse=True):\n",
        "                wrapped_de_cell = dec_cell\n",
        "                dec_initial_state = tf.contrib.seq2seq.tile_batch(\n",
        "                    enc_state,\n",
        "                    multiplier=hparams.beam_width)\n",
        "\n",
        "        # len(inferred_reply) is lte encoder_length,\n",
        "        # because we are targeting tweet (140 for each tweet)\n",
        "        # Also by doing this,\n",
        "        # we can pass the reply to other seq2seq w/o shorten it.\n",
        "        maximum_iterations = hparams.encoder_length\n",
        "\n",
        "        inference_decoder = tf.contrib.seq2seq.BeamSearchDecoder(\n",
        "            cell=wrapped_de_cell,\n",
        "            embedding=embedding_encoder,\n",
        "            start_tokens=tf.fill([dynamic_batch_size], hparams.sos_id),\n",
        "            end_token=hparams.eos_id,\n",
        "            initial_state=dec_initial_state,\n",
        "            beam_width=hparams.beam_width,\n",
        "            output_layer=projection_layer,\n",
        "            length_penalty_weight=0.0)\n",
        "\n",
        "        # Dynamic decoding\n",
        "        with tf.variable_scope(scope, reuse=True):\n",
        "            beam_outputs, final_state, _ = tf.contrib.seq2seq.dynamic_decode(\n",
        "                inference_decoder, maximum_iterations=maximum_iterations)\n",
        "        beam_replies = beam_outputs.predicted_ids\n",
        "        return beam_replies\n",
        "\n",
        "    @staticmethod\n",
        "    def _build_sample_inference(hparams, embedding_encoder, enc_state,\n",
        "                                enc_inputs_lengths, enc_outputs,\n",
        "                                dec_cell, projection_layer, scope):\n",
        "        # Sample decoder\n",
        "        dynamic_batch_size = tf.shape(enc_inputs_lengths)[0]\n",
        "        inference_helper = tf.contrib.seq2seq.SampleEmbeddingHelper(\n",
        "            embedding_encoder,\n",
        "            tf.fill([dynamic_batch_size], hparams.sos_id), hparams.eos_id,\n",
        "            softmax_temperature=1.5)\n",
        "\n",
        "        infer_logits, replies = ChatbotModel._dynamic_decode(dec_cell,\n",
        "                                                             dynamic_batch_size,\n",
        "                                                             enc_inputs_lengths,\n",
        "                                                             enc_outputs,\n",
        "                                                             enc_state,\n",
        "                                                             hparams,\n",
        "                                                             inference_helper,\n",
        "                                                             projection_layer,\n",
        "                                                             scope)\n",
        "        return infer_logits, replies\n",
        "\n",
        "    @staticmethod\n",
        "    def _dynamic_decode(dec_cell, dynamic_batch_size,\n",
        "                        enc_inputs_lengths, enc_outputs, enc_state,\n",
        "                        hparams, dec_helper, projection_layer, scope):\n",
        "        initial_state, wrapped_dec_cell = ChatbotModel._attention_wrapper(\n",
        "            dec_cell, dynamic_batch_size, enc_inputs_lengths, enc_outputs,\n",
        "            enc_state, hparams, scope)\n",
        "        with tf.variable_scope(scope):\n",
        "            with tf.variable_scope(\"infer\"):\n",
        "                inference_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
        "                    wrapped_dec_cell, dec_helper, initial_state,\n",
        "                    output_layer=projection_layer)\n",
        "        # len(inferred_reply) is lte encoder_length,\n",
        "        # because we are targeting tweet (140 for each tweet)\n",
        "        # Also by doing this,\n",
        "        # we can pass the reply to other seq2seq w/o shorten it.\n",
        "        maximum_iterations = hparams.encoder_length\n",
        "        # Dynamic decoding\n",
        "        # Here we reuse Attention Wrapper\n",
        "        with tf.variable_scope(scope, reuse=True):\n",
        "            outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
        "                inference_decoder, maximum_iterations=maximum_iterations)\n",
        "        replies = outputs.sample_id\n",
        "        # We use infer_logits instead of _logits when calculating log_prob,\n",
        "        # because infer_logits doesn't require decoder_target_lengths input.\n",
        "        infer_logits = outputs.rnn_output\n",
        "        return infer_logits, replies\n",
        "\n",
        "    @staticmethod\n",
        "    def _attention_wrapper(dec_cell, dynamic_batch_size, enc_inputs_lengths,\n",
        "                           enc_outputs, enc_state, hparams, scope, reuse=True):\n",
        "        # See https://github.com/tensorflow/tensorflow/issues/11904\n",
        "        if hparams.use_attention:\n",
        "            with tf.variable_scope(scope, reuse=reuse):\n",
        "                # Attention\n",
        "                # encoder_outputs is time major, so transopse it to batch major.\n",
        "                # attention_encoder_outputs: [batch_size, encoder_length, num_units]\n",
        "                attention_encoder_outputs = tf.transpose(enc_outputs,\n",
        "                                                         [1, 0, 2])\n",
        "\n",
        "                # Create an attention mechanism\n",
        "                attention_mechanism = tf.contrib.seq2seq.LuongAttention(\n",
        "                    hparams.num_units,\n",
        "                    attention_encoder_outputs,\n",
        "                    memory_sequence_length=enc_inputs_lengths)\n",
        "\n",
        "                wrapped_dec_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
        "                    dec_cell, attention_mechanism,\n",
        "                    attention_layer_size=hparams.num_units)\n",
        "\n",
        "                initial_state = wrapped_dec_cell.zero_state(\n",
        "                    dynamic_batch_size,\n",
        "                    tf.float32).clone(\n",
        "                    cell_state=enc_state)\n",
        "        else:\n",
        "            with tf.variable_scope(scope, reuse=reuse):\n",
        "                wrapped_dec_cell = dec_cell\n",
        "                initial_state = enc_state\n",
        "        return initial_state, wrapped_dec_cell\n",
        "\n",
        "    # convert sampled_indices to indices for tf.gather_nd.\n",
        "    @staticmethod\n",
        "    def _convert_indices(sampled_indices):\n",
        "        print_sampled_indices = tf.Print(sampled_indices,\n",
        "                                         [tf.shape(sampled_indices)],\n",
        "                                         message=\"sampled_indices\")\n",
        "        batch_size = tf.shape(print_sampled_indices)[0]\n",
        "        dec_length = tf.shape(print_sampled_indices)[1]\n",
        "        print_batch_size = tf.Print(batch_size, [batch_size, dec_length],\n",
        "                                    message=\"(batch_size, dec_length)\")\n",
        "        first_indices = tf.tile(\n",
        "            tf.expand_dims(tf.range(print_batch_size), dim=1),\n",
        "            [1, dec_length])\n",
        "        second_indices = tf.reshape(\n",
        "            tf.tile(tf.range(dec_length), [print_batch_size]),\n",
        "            [print_batch_size, dec_length])\n",
        "        print_first_indices = tf.Print(first_indices, [tf.shape(first_indices),\n",
        "                                                       tf.shape(\n",
        "                                                           second_indices)],\n",
        "                                       message=\"(first_indices, second_indices)\")\n",
        "        return tf.stack([print_first_indices, second_indices, sampled_indices],\n",
        "                        axis=2)\n",
        "\n",
        "    @staticmethod\n",
        "    def _log_probs_beam(logits, predicted_ids):\n",
        "        \"\"\" Return log_probs for predicted_ids by beam search.\n",
        "\n",
        "        Args:\n",
        "            logits: [batch_size, dec_length, vocab_size]\n",
        "            predicted_ids: [batch_size, beam_width, dec_length]\n",
        "\n",
        "        Returns:\n",
        "            Return log_prob:[batch_size, beam_width, dec_length]\n",
        "      \"\"\"\n",
        "\n",
        "        # Sum over vocab_size axis\n",
        "        log_probs = tf.nn.log_softmax(logits)\n",
        "        def one_log_probs(beam_index):\n",
        "            return tf.gather_nd(log_probs, ChatbotModel._convert_indices(\n",
        "                predicted_ids[:, beam_index]))\n",
        "\n",
        "        beam_width = tf.shape(predicted_ids)[1]\n",
        "        i = tf.constant(0)\n",
        "        outputs = tf.TensorArray(tf.float32, size=1, dynamic_size=True)\n",
        "        cond = lambda i, o: tf.less(i, beam_width)\n",
        "        body = lambda i, o: [tf.add(i, 1), o.write(i, one_log_probs(i))]\n",
        "\n",
        "        _, outputs = tf.while_loop(cond, body, [i, outputs])\n",
        "        result = tf.transpose(outputs.stack(), [1, 0, 2])\n",
        "        return result\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DFEYKvBwL3Nm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class TrainDataSource:\n",
        "    def __init__(self, source_path, hparams, vocab_path=None):\n",
        "        Shell.download_file_if_necessary(source_path)\n",
        "        generator = TrainDataGenerator(source_path=source_path,\n",
        "                                       hparams=hparams)\n",
        "        # generator.remove_generated()\n",
        "        train_dataset, vocab, rev_vocab = generator.generate(vocab_path)\n",
        "        # We don't use shuffle here, because we want to align two data source here.\n",
        "        self.train_dataset = train_dataset.repeat()\n",
        "        self.vocab_path = generator.vocab_path\n",
        "        # todo(higepon): Use actual validation dataset.\n",
        "        self.valid_dataset = train_dataset.repeat()\n",
        "        self.vocab = vocab\n",
        "        self.rev_vocab = rev_vocab\n",
        "\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self):\n",
        "        self.loss_step = []\n",
        "        self.val_losses = []\n",
        "        self.reward_step = []\n",
        "        self.reward_average = []\n",
        "        self.last_saved_time = datetime.datetime.now()\n",
        "        self.last_stats_time = datetime.datetime.now()\n",
        "        self.num_stats_per = 20\n",
        "        self.reward_history = []\n",
        "        self._valid_tweets = [\"\", \"\", \"\"]\n",
        "\n",
        "    def train_seq2seq_rl(self, seq2seq_hparams, hparams, source_path, resume):\n",
        "        self._resume_if_necessary(resume, source_path, hparams, seq2seq_hparams)\n",
        "        with tf.device(self._available_device()):\n",
        "            seq2seq_model = self.create_model(seq2seq_hparams)\n",
        "            restored = seq2seq_model.restore()\n",
        "            assert (restored)\n",
        "        self.train_rl(hparams, source_path, self._valid_tweets, seq2seq_model)\n",
        "\n",
        "    def train_seq2seq_beam_rl(self, seq2seq_hparams, hparams, source_path,\n",
        "                              resume):\n",
        "        self._resume_if_necessary(resume, source_path, hparams, seq2seq_hparams)\n",
        "        with tf.device(self._available_device()):\n",
        "            seq2seq_model = self.create_model(seq2seq_hparams)\n",
        "            restored = seq2seq_model.restore()\n",
        "            assert (restored)\n",
        "        self.train_beam_rl(hparams, source_path, self._valid_tweets, seq2seq_model)\n",
        "\n",
        "    def _resume_if_necessary(self, resume, source_path, hparams, seq2seq_hparams):\n",
        "        if resume:\n",
        "            return\n",
        "        self.train_seq2seq(seq2seq_hparams, source_path, self._valid_tweets,\n",
        "                           should_clean_saved_model=False)\n",
        "        Shell.copy_all_files(seq2seq_hparams.model_path, hparams.model_path)\n",
        "\n",
        "    def train_rl(self, hparams, source_path, tweets=[], seq2seq_model=None):\n",
        "        print(\"===== Train RL {} ====\".format(source_path))\n",
        "        now = datetime.datetime.today().strftime(\"%Y%m%d%H%M%S\")\n",
        "        print(\"{}_rl_test\".format(now))\n",
        "        print(\"hparams\")\n",
        "        print_hparams(hparams)\n",
        "\n",
        "        data_source = TrainDataSource(source_path, hparams)\n",
        "        easy_tf_log.set_dir(hparams.model_path)\n",
        "        Shell.download_model_data_if_necessary(hparams.model_path)\n",
        "        device = self._available_device()\n",
        "        with tf.device(device):\n",
        "            model = self.create_model(hparams)\n",
        "\n",
        "        vocab = data_source.vocab\n",
        "        rev_vocab = data_source.rev_vocab\n",
        "        infer_helper = InferenceHelper(model, vocab, rev_vocab)\n",
        "\n",
        "        graph = model.sess.graph\n",
        "        writer = tf.summary.FileWriter(hparams.model_path, graph)\n",
        "        last_saved_time = datetime.datetime.now()\n",
        "        with graph.as_default():\n",
        "            train_data_next = data_source.train_dataset.make_one_shot_iterator().get_next()\n",
        "\n",
        "            data = model.sess.run(train_data_next)\n",
        "            model.train(data[0], data[1], data[2], data[3], data[4])\n",
        "\n",
        "            avg_good_value = 0\n",
        "            for step in range(hparams.num_train_steps):\n",
        "                train_data = model.sess.run(train_data_next)\n",
        "                sampled_replies, _ = model.sample(train_data[0], train_data[1])\n",
        "                for i in range(hparams.batch_size):\n",
        "                    print(\"{}->{}\\n\".format(\n",
        "                        infer_helper.ids_to_words(train_data[0][:, i]),\n",
        "                        infer_helper.ids_to_words(sampled_replies[i])))\n",
        "\n",
        "                if True:\n",
        "                    dull_responses_ids = self._dull_response_ids(infer_helper)\n",
        "                    enc_inputs, enc_inputs_lengths = self.format_enc_inputs(hparams, model, sampled_replies)\n",
        "                    max_len = len(sampled_replies[0])\n",
        "                    # We adjust sampled_replies => enc_inputs, because we need fixed length for seq2seq.\n",
        "                    # But for reward and logits we want to need actual max_len.\n",
        "                    reward = seq2seq_model.reward_ease_of_answering(\n",
        "                        max_len,\n",
        "                        enc_inputs,\n",
        "                        enc_inputs_lengths, dull_responses_ids)\n",
        "                    print(\"reward\", reward)\n",
        "                    good_value_key = \"reward\"\n",
        "                    good_value = np.mean(reward)\n",
        "                else:\n",
        "                    good_value_key = \"good_count\"\n",
        "                    good_value, reward = self._reward_for_test(model,\n",
        "                                                               sampled_replies)\n",
        "\n",
        "                avg_good_value += good_value\n",
        "                if step != 0 and step % 20 == 0:\n",
        "                    print(\"{}:{}\".format(good_value_key, good_value))\n",
        "                if step != 0 and step % 60 == 0:\n",
        "                    self._print_log(good_value_key, avg_good_value / 60)\n",
        "                    avg_good_value = 0\n",
        "\n",
        "                global_step = model.train_with_reward(train_data[0],\n",
        "                                                      train_data[1],\n",
        "                                                      sampled_replies, reward)\n",
        "                if step != 0 and step % 100 == 0:\n",
        "                    print(\"save and restore\")\n",
        "                    model.save()\n",
        "                    is_restored = model.restore()\n",
        "                    assert (is_restored)\n",
        "                    is_restored = model.restore()\n",
        "                    assert (is_restored)\n",
        "                    self._print_inferences(step, tweets, infer_helper)\n",
        "                    now = datetime.datetime.now()\n",
        "                    print(\"delta:\", (now - last_saved_time).total_seconds())\n",
        "                    last_saved_time = now\n",
        "                    assert is_restored\n",
        "                    print(\"step={}, global_step={}\".format(step, global_step))\n",
        "\n",
        "    def train_beam_rl(self, rl_hparams, seq2seq_hparams, backward_hparams,\n",
        "                      seq2seq_source_path, rl_source_path, tweets=[]):\n",
        "        print(\"===== Train Beam RL {} ====\".format(seq2seq_source_path))\n",
        "        now = datetime.datetime.today().strftime(\"%Y%m%d%H%M%S\")\n",
        "        print(\"{}_beam_rl_test\".format(now))\n",
        "        print(\"rl_hparams\")\n",
        "        print_hparams(rl_hparams)\n",
        "\n",
        "        seq2seq_data_source = TrainDataSource(seq2seq_source_path, rl_hparams)\n",
        "        rl_data_source = TrainDataSource(rl_source_path, rl_hparams)\n",
        "        easy_tf_log.set_dir(rl_hparams.model_path)\n",
        "        Shell.download_model_data_if_necessary(rl_hparams.model_path)\n",
        "        device = self._available_device()\n",
        "        with tf.device(device):\n",
        "            model = self.create_model(rl_hparams)\n",
        "            seq2seq_model = self.create_model(seq2seq_hparams)\n",
        "            backward_model = self.create_model(backward_hparams)\n",
        "\n",
        "        vocab = seq2seq_data_source.vocab\n",
        "        rev_vocab = seq2seq_data_source.rev_vocab\n",
        "        infer_helper = InferenceHelper(model, vocab, rev_vocab)\n",
        "\n",
        "        graph = model.sess.graph\n",
        "        writer = tf.summary.FileWriter(rl_hparams.model_path, graph)\n",
        "        last_saved_time = datetime.datetime.now()\n",
        "        with graph.as_default():\n",
        "            seq2seq_train_data_next = seq2seq_data_source.train_dataset.make_one_shot_iterator().get_next()\n",
        "            rl_train_data_next = rl_data_source.train_dataset.make_one_shot_iterator().get_next()\n",
        "\n",
        "            data = model.sess.run(seq2seq_train_data_next)\n",
        "            model.train(data[0], data[1], data[2], data[3], data[4])\n",
        "\n",
        "            avg_good_value = 0\n",
        "            for step in range(rl_hparams.num_train_steps):\n",
        "                seq2seq_train_data = model.sess.run(seq2seq_train_data_next)\n",
        "                rl_train_data = model.sess.run(rl_train_data_next)\n",
        "                beam_predicted_ids, _ = model.infer_beam_search(seq2seq_train_data[0],\n",
        "\n",
        "                                                                seq2seq_train_data[1])\n",
        "\n",
        "                max_len = len(beam_predicted_ids[0, :, 0])\n",
        "\n",
        "                # log_probs_beam: [batch_size, dec_length, beam_width]\n",
        "                log_probs_beam = seq2seq_model.log_probs_beam(seq2seq_train_data[0],\n",
        "                                                              seq2seq_train_data[1],\n",
        "                                                              beam_predicted_ids)\n",
        "                print(\"xxxxxx, log_prob_beams,\", log_probs_beam.shape)\n",
        "\n",
        "                # Calc 1/N_a * logP_seq2seq(a|p_i, q_i) for each beam_predicted\n",
        "                reward_s = np.zeros(\n",
        "                    (rl_hparams.batch_size, max_len, rl_hparams.beam_width))\n",
        "                for batch in range(rl_hparams.batch_size):\n",
        "                    for beam in range(rl_hparams.beam_width):\n",
        "                        print(\"log_probs_beam[batch, :, beam]\", log_probs_beam[batch, :, beam].shape)\n",
        "                        # ayashii\n",
        "                        tweet = beam_predicted_ids[batch][:, beam]\n",
        "                        print(\"tweet\", tweet)\n",
        "                        tweet_len = 0\n",
        "                        p = 0\n",
        "                        for i in range(len(tweet)):\n",
        "                            p += log_probs_beam[batch][i][beam]\n",
        "                            tweet_len = tweet_len + 1\n",
        "                            if tweet[i] == rl_hparams.eos_id:\n",
        "                                break\n",
        "\n",
        "                        assert (tweet_len != 0)\n",
        "                        p /= tweet_len\n",
        "                        print(\"p\", p)\n",
        "                        for i in range(max_len):\n",
        "                            reward_s[batch][i][beam] = p\n",
        "                print(\"reward_s\", reward_s)\n",
        "\n",
        "                # TODO\n",
        "                #   Change log_prob_beam to return actual log_prob using log softmax.\n",
        "                # .    Which axis?\n",
        "                #   Change and rename model.logits to return log_prob using log softmax.\n",
        "\n",
        "\n",
        "                # Calc 1/N_qi * logP_backward(qi|a)\n",
        "                # todo vectorized implmentation here\n",
        "                #####  log_probs  softmax   logits?\n",
        "                #       probs = tf.nn.softmax(logits)\n",
        "                # log_probs = tf.nn.log_softmax(logits)\n",
        "                reward_qi = np.zeros(\n",
        "                    (rl_hparams.batch_size, max_len, rl_hparams.beam_width))\n",
        "\n",
        "                # target label with eos.\n",
        "                # [batch_size, dec_length]\n",
        "                qi = rl_train_data[2]\n",
        "                for beam in range(rl_hparams.beam_width):\n",
        "                    replies = beam_predicted_ids[:, :, beam]\n",
        "                    a_enc_inputs, a_enc_inputs_lengths = self.format_enc_inputs(rl_hparams, model, replies)\n",
        "\n",
        "                    print(\"a_enc_inputs.shape\", a_enc_inputs.shape)\n",
        "\n",
        "                    print(\"a_enc_inputs_lengths\", a_enc_inputs_lengths)\n",
        "\n",
        "                    # [batch_size, dec_len, vocab_size]\n",
        "                    log_probs = backward_model.log_probs(a_enc_inputs,\n",
        "                                                         a_enc_inputs_lengths)\n",
        "                    print(\"log_probs\", log_probs.shape)\n",
        "                    for batch in range(rl_hparams.batch_size):\n",
        "                        tweet = qi[batch]\n",
        "                        print(\"tweet\", tweet)\n",
        "                        tweet_len = 0\n",
        "                        p = 0\n",
        "                        # ayashi\n",
        "                        # do we multply probabiity here, ???? we should probably sum up? because we're using log prob.\n",
        "                        for i, id in enumerate(tweet):\n",
        "                            # log_probs shape is supposed to be [batch_size, dec_length, vocab_size],\n",
        "                            # but it sometimes becomes [batch_size, smaller_value, vocab_size].\n",
        "                            # This is because we're using GreedyDecoder, dynamic_decode finishes the decoder process when it sees eos_id.\n",
        "                            # If all enc_inputs ends up shorter dec_output, we can have smaller_value here.\n",
        "                            if i < len(log_probs[batch]):\n",
        "                                p += log_probs[batch][i][id]\n",
        "                            tweet_len = tweet_len + 1\n",
        "                            if id == rl_hparams.eos_id:\n",
        "                                break\n",
        "\n",
        "                        assert (tweet_len != 0)\n",
        "                        p /= tweet_len\n",
        "                        print(\"p2\", p)\n",
        "                        for i in range(max_len):\n",
        "                            reward_qi[batch][i][beam] = p\n",
        "\n",
        "\n",
        "                for i in range(rl_hparams.batch_size):\n",
        "                    print(\"[{}]:{}\".format(i, infer_helper.ids_to_words(seq2seq_train_data[0][:, i])))\n",
        "                    for j in range(rl_hparams.beam_width):\n",
        "                        print(\"    beam_predicted[{}]:{} prob={:06.2f} backward_prob={:06.2f}\".format(j, infer_helper.ids_to_words(\n",
        "                            beam_predicted_ids[i, :, j]), reward_s[i][0][j].item(), reward_qi[i][0][j].item()))\n",
        "\n",
        "\n",
        "                print(\"reward_qi\", reward_qi)\n",
        "                # todo later\n",
        "                #                for i in range(rl_hparams.batch_size):\n",
        "                #                  print(\"{}->{}\\n\".format(infer_helper.ids_to_words(seq2seq_train_data[0][:, i]), infer_helper.ids_to_words(sampled_replies[i])))\n",
        "\n",
        "                good_value = 1\n",
        "                good_value_key = \"beam\"\n",
        "                rl_hparams = model.hparams\n",
        "\n",
        "                print(\"beam max_len\", max_len)\n",
        "                reward = reward_s + reward_qi  # np.ones(\n",
        "                #                    (rl_hparams.batch_size, max_len, rl_hparams.beam_width))\n",
        "\n",
        "                avg_good_value += good_value\n",
        "                if step != 0 and step % 20 == 0:\n",
        "                    print(\"{}:{}\".format(good_value_key, good_value))\n",
        "                if step != 0 and step % 60 == 0:\n",
        "                    self._print_log(good_value_key, avg_good_value / 60)\n",
        "                    avg_good_value = 0\n",
        "\n",
        "                global_step = model.train_beam_with_reward(seq2seq_train_data[0],\n",
        "                                                           seq2seq_train_data[1],\n",
        "                                                           beam_predicted_ids,\n",
        "                                                           reward)\n",
        "                if step != 0 and step % 100 == 0:\n",
        "                    print(\"save and restore\")\n",
        "                    model.save()\n",
        "                    is_restored = model.restore()\n",
        "                    assert (is_restored)\n",
        "                    is_restored = model.restore()\n",
        "                    assert (is_restored)\n",
        "                    self._print_inferences(step, tweets, infer_helper)\n",
        "                    now = datetime.datetime.now()\n",
        "                    print(\"delta:\", (now - last_saved_time).total_seconds())\n",
        "                    last_saved_time = now\n",
        "                    assert is_restored\n",
        "                    print(\"step={}, global_step={}\".format(step, global_step))\n",
        "\n",
        "    def format_enc_inputs(self, hparams, model, replies):\n",
        "        enc_inputs = []\n",
        "        enc_inputs_lengths = []\n",
        "\n",
        "        # replies: [batch_size, dec_length]\n",
        "        for reply in replies:\n",
        "            reply_len = model.seq_len(reply.tolist())\n",
        "            # Safe guard: sampled reply has sometimes 0 len.\n",
        "            #            adjusted_len = hparams.encoder_length if reply_len == 0 else reply_len\n",
        "            enc_inputs_lengths.append(reply_len)\n",
        "            if reply_len <= hparams.encoder_length:\n",
        "                padded_reply = np.append(reply, ([hparams.pad_id] * (hparams.encoder_length - len(reply))))\n",
        "                enc_inputs.append(padded_reply)\n",
        "            else:\n",
        "                raise Exception(\n",
        "                    \"Inferred\"\n",
        "                    \" reply shouldn't be longer than encoder_input\")\n",
        "\n",
        "        # Expected enc_inputs param is time major.\n",
        "        enc_inputs = np.transpose(np.array(enc_inputs))\n",
        "        return enc_inputs, enc_inputs_lengths\n",
        "\n",
        "    @staticmethod\n",
        "    def _reward_for_test(model, sampled_replies):\n",
        "        max_len = len(sampled_replies[0])\n",
        "        # default negative reward\n",
        "        reward = np.ones((model.hparams.batch_size, max_len)) * -1.0\n",
        "        good_value = 0\n",
        "        for i, reply in enumerate(sampled_replies):\n",
        "            reply_len = model.input_length(reply.tolist())\n",
        "            if reply_len == 8 or reply_len == 0 or reply_len == 1:\n",
        "                for r in range(max_len):\n",
        "                    reward[i][r] = -1.0\n",
        "            else:\n",
        "                good_value += 1\n",
        "                for r in range(max_len):\n",
        "                    reward[i][r] = 1.0\n",
        "        return good_value, reward\n",
        "\n",
        "    def _dull_response_ids(self, infer_helper):\n",
        "        dull_responses = ['', \"\", \"\", \"\", \"www\",\n",
        "                          \"(-)\",\n",
        "                          \"\", \"\", \"\"]\n",
        "        dull_responses_ids = [self.tokenize(infer_helper, text) for text in\n",
        "                              dull_responses]\n",
        "        return dull_responses_ids\n",
        "\n",
        "    def train_seq2seq_swapped(self, hparams, tweets_path, validation_tweets,\n",
        "                              should_clean_saved_model=True, vocab_path=None):\n",
        "        Shell.download_file_if_necessary(tweets_path)\n",
        "        swapped_path = TrainDataGenerator.generate_source_target_swapped(\n",
        "            tweets_path)\n",
        "        return self.train_seq2seq(hparams, swapped_path, validation_tweets,\n",
        "                                  should_clean_saved_model, vocab_path)\n",
        "\n",
        "    def train_seq2seq(self, hparams, tweets_path, val_tweets,\n",
        "                      should_clean_saved_model=True, vocab_path=None):\n",
        "        print(\"===== Train Seq2Seq {} ====\".format(tweets_path))\n",
        "        print_hparams(hparams)\n",
        "\n",
        "        if should_clean_saved_model:\n",
        "            clean_model_path(hparams.model_path)\n",
        "        data_source = TrainDataSource(tweets_path, hparams, vocab_path)\n",
        "        return self._train_loop(data_source, hparams, val_tweets)\n",
        "\n",
        "    def _print_inferences(self, global_step, tweets, helper, ):\n",
        "        print(\"==== {} ====\".format(global_step))\n",
        "        len_array = []\n",
        "        for tweet in tweets:\n",
        "            len_array.append(len(helper.inferences(tweet)[0]))\n",
        "            helper.print_inferences(tweet)\n",
        "        self._print_log('average reply len', np.mean(len_array))\n",
        "\n",
        "    @staticmethod\n",
        "    def create_model(hparams):\n",
        "\n",
        "        # See https://www.tensorflow.org/tutorials/using_gpu#allowing_gpu_memory_growth\n",
        "        config = tf.ConfigProto(log_device_placement=False)\n",
        "        config.gpu_options.allow_growth = True\n",
        "\n",
        "        train_graph = tf.Graph()\n",
        "        train_sess = tf.Session(graph=train_graph, config=config)\n",
        "        with train_graph.as_default():\n",
        "            with tf.variable_scope('root'):\n",
        "                model = ChatbotModel(train_sess, hparams,\n",
        "                                     model_path=hparams.model_path)\n",
        "                if not model.restore():\n",
        "                    train_sess.run(tf.global_variables_initializer())\n",
        "\n",
        "        return model\n",
        "\n",
        "    def _train_loop(self, data_source,\n",
        "                    hparams, tweets):\n",
        "        Shell.download_model_data_if_necessary(hparams.model_path)\n",
        "\n",
        "        device = self._available_device()\n",
        "        with tf.device(device):\n",
        "            model = self.create_model(hparams)\n",
        "\n",
        "        def my_train(**kwargs):\n",
        "            data = kwargs['train_data']\n",
        "            return model.train(data[0], data[1], data[2], data[3], data[4])\n",
        "\n",
        "        return self._generic_train_loop(data_source, hparams,\n",
        "                                        model,\n",
        "                                        tweets, my_train)\n",
        "\n",
        "    @staticmethod\n",
        "    def _available_device():\n",
        "        device = '/cpu:0'\n",
        "        if has_gpu0():\n",
        "            device = '/gpu:0'\n",
        "            print(\"$$$ GPU ENABLED $$$\")\n",
        "        return device\n",
        "\n",
        "    @staticmethod\n",
        "    def tokenize(infer_helper, text):\n",
        "        tagger = MeCab.Tagger(\"-Owakati\")\n",
        "        words = tagger.parse(text).split()\n",
        "        return infer_helper.words_to_ids(words)\n",
        "\n",
        "    def _generic_train_loop(self, data_source, hparams,\n",
        "                            model,\n",
        "                            tweets, train_func):\n",
        "        try:\n",
        "            return self._raw_train_loop(data_source, hparams, model, train_func,\n",
        "                                        tweets)\n",
        "        except KeyboardInterrupt as ke:\n",
        "            raise (ke)\n",
        "        except Exception as e:\n",
        "            pb.push_note(\"Train error\", str(e))\n",
        "            raise (e)\n",
        "\n",
        "    def _raw_train_loop(self, data_source, hparams,\n",
        "                        model, train_func,\n",
        "                        tweets):\n",
        "        vocab = data_source.vocab\n",
        "        rev_vocab = data_source.rev_vocab\n",
        "        infer_helper = InferenceHelper(model, vocab, rev_vocab)\n",
        "        graph = model.sess.graph\n",
        "        with graph.as_default():\n",
        "            train_data_next = data_source.train_dataset.make_one_shot_iterator().get_next()\n",
        "            val_data_next = data_source.valid_dataset.make_one_shot_iterator().get_next()\n",
        "            easy_tf_log.set_dir(hparams.model_path)\n",
        "            writer = tf.summary.FileWriter(hparams.model_path, graph)\n",
        "            self.last_saved_time = datetime.datetime.now()\n",
        "            for i in range(hparams.num_train_steps):\n",
        "                train_data = model.sess.run(train_data_next)\n",
        "\n",
        "                step, summary = train_func(\n",
        "                    train_data=train_data,\n",
        "                )\n",
        "                writer.add_summary(summary, step)\n",
        "\n",
        "                if i != 0 and i % self.num_stats_per == 0:\n",
        "                    model.save(hparams.model_path)\n",
        "                    is_restored = model.restore()\n",
        "                    assert is_restored\n",
        "                    self._print_inferences(step, tweets, infer_helper)\n",
        "                    self._compute_val_loss(step, model, val_data_next, writer)\n",
        "                    #                    self._print_stats(hparams, learning_rate)\n",
        "                    self._plot_if_necessary()\n",
        "                    self._save_model_in_drive(hparams)\n",
        "                else:\n",
        "                    print('.', end='')\n",
        "        return model, infer_helper\n",
        "\n",
        "    def _plot_if_necessary(self):\n",
        "        if len(self.reward_average) > 0 and len(self.reward_average) % 30 == 0:\n",
        "            self._plot(self.reward_step, self.reward_average,\n",
        "                       y_label='reward average')\n",
        "            self._plot(self.loss_step, self.val_losses,\n",
        "                       y_label='validation_loss')\n",
        "\n",
        "    def _print_stats(self, hparams, learning_rate):\n",
        "        print(\"learning rate\", learning_rate)\n",
        "        delta = (\n",
        "                    datetime.datetime.now() - self.last_stats_time).total_seconds() * 1000\n",
        "        self._print_log(\"msec/data\",\n",
        "                        delta / hparams.batch_size / self.num_stats_per)\n",
        "        self.last_stats_time = datetime.datetime.now()\n",
        "\n",
        "    def _save_model_in_drive(self, hparams):\n",
        "        now = datetime.datetime.now()\n",
        "        delta_in_min = (now - self.last_saved_time).total_seconds() / 60\n",
        "\n",
        "        if delta_in_min >= 60:\n",
        "            self.last_saved_time = datetime.datetime.now()\n",
        "            Shell.save_model_in_drive(hparams.model_path)\n",
        "\n",
        "    @staticmethod\n",
        "    def _print_log(key, value):\n",
        "        tflog(\"{}[{}]\".format(key, current_client_id), value)\n",
        "        print(\"{}={}\".format(key, round(value, 1)))\n",
        "\n",
        "    @staticmethod\n",
        "    def _plot(x, y, x_label=\"step\", y_label='y'):\n",
        "        title = \"{}_{}\".format(current_client_id, y_label)\n",
        "        plt.plot(x, y, label=title)\n",
        "        plt.plot()\n",
        "        plt.ylabel(title)\n",
        "        plt.xlabel(x_label)\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "    def _compute_val_loss(self, global_step, model, val_data_next,\n",
        "                          writer):\n",
        "        val_data = model.sess.run(val_data_next)\n",
        "        val_loss, val_loss_log = model.batch_loss(val_data[0],\n",
        "                                                  val_data[1],\n",
        "                                                  val_data[2],\n",
        "                                                  val_data[3],\n",
        "                                                  val_data[4])\n",
        "        # np.float64 to native float\n",
        "        val_loss = val_loss.item()\n",
        "        writer.add_summary(val_loss_log, global_step)\n",
        "        self._print_log(\"validation loss\", val_loss)\n",
        "        self.loss_step.append(global_step)\n",
        "        self.val_losses.append(val_loss)\n",
        "        return val_loss\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XemvojdXd3ng",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ul5WBjSF3vy9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class InferenceHelper:\n",
        "    def __init__(self, model, vocab, rev_vocab):\n",
        "        self.model = model\n",
        "        self.vocab = vocab\n",
        "        self.rev_vocab = rev_vocab\n",
        "\n",
        "    def inferences(self, tweet):\n",
        "        encoder_inputs, encoder_inputs_lengths = self.create_inference_input(\n",
        "            tweet)\n",
        "        replies = self.model.infer(encoder_inputs, encoder_inputs_lengths)\n",
        "        ids = replies[0].tolist()\n",
        "        all_infer = [self.sanitize_text(self.ids_to_words(ids))]\n",
        "        beam_replies, logits = self.model.infer_beam_search(encoder_inputs,\n",
        "                                                    encoder_inputs_lengths)\n",
        "        beam_infer = [self.sanitize_text(self.ids_to_words(beam_replies[0][:, i])) for i in range(self.model.hparams.beam_width)]\n",
        "        all_infer.extend(beam_infer)\n",
        "        return all_infer\n",
        "        \n",
        "    def sanitize_text(self, line):\n",
        "      line = re.sub(r\"\\[EOS\\]\", \" \", line)\n",
        "      line = re.sub(r\"\\[UNK\\]\", \"\", line)\n",
        "      return line\n",
        "\n",
        "    def print_inferences(self, tweet):\n",
        "        print(tweet)\n",
        "        for i, reply in enumerate(self.inferences(tweet)):\n",
        "            print(\"    [{}]{}\".format(i, reply))\n",
        "\n",
        "    def words_to_ids(self, words):\n",
        "        ids = []\n",
        "        for word in words:\n",
        "            if word in self.vocab:\n",
        "                ids.append(self.vocab[word])\n",
        "            else:\n",
        "                ids.append(self.model.hparams.unk_id)\n",
        "        return ids\n",
        "\n",
        "    def ids_to_words(self, ids):\n",
        "        words = \"\"\n",
        "        for id in ids:\n",
        "            words += self.rev_vocab[id]\n",
        "        return words\n",
        "\n",
        "    def create_inference_input(self, text):\n",
        "        inference_encoder_inputs = np.empty((self.model.hparams.encoder_length, self.model.hparams.batch_size),\n",
        "                                            dtype=np.int)\n",
        "        inference_encoder_inputs_lengths = np.empty(self.model.hparams.batch_size, dtype=np.int)\n",
        "        text = TrainDataGenerator.sanitize_line(text)\n",
        "        tagger = MeCab.Tagger(\"-Owakati\")\n",
        "        words = tagger.parse(text).split()\n",
        "        ids = self.words_to_ids(words)\n",
        "        ids = ids[:self.model.hparams.encoder_length]\n",
        "        len_ids = len(ids)\n",
        "        ids.extend([self.model.hparams.pad_id] * (self.model.hparams.encoder_length - len(ids)))\n",
        "        for i in range(self.model.hparams.batch_size):\n",
        "            inference_encoder_inputs[:, i] = np.array(ids, dtype=np.int)\n",
        "            inference_encoder_inputs_lengths[i] = len_ids\n",
        "        return inference_encoder_inputs, inference_encoder_inputs_lengths\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4U1o8gMTP1mA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ConversationTrainDataGenerator:\n",
        "    def __init__(self):\n",
        "        return\n",
        "\n",
        "    # Generate the following file from conversations_txt file.\n",
        "    # Let p_i:   line 3i     in the txt file, which is original tweet.\n",
        "    #     q_i:   line 3i + 1 in the txt file, which is reply to the tweet.\n",
        "    #     p_i+1: line 3i + 2 in the txt file, which is reply to the reply above.\n",
        "    # (A) conversation_seq2seq.txt for train p_seq2seq and p_seq2seq_backward\n",
        "    #     line 2i: p_i + q_i\n",
        "    #     line 2i+1: p_i+1\n",
        "    #\n",
        "    # (B) conversation_rl.txt for train p_rl.\n",
        "    #     line 2i: p_i + q_i\n",
        "    #     line 2i+1: q_i\n",
        "    #\n",
        "    # (A) and (B) should share the vocabulary.\n",
        "    def generate(self, conversations_txt):\n",
        "        basename, extension = os.path.splitext(conversations_txt)\n",
        "        seq2seq_path = \"{}_seq2seq{}\".format(basename, extension)\n",
        "        rl_path = \"{}_rl{}\".format(basename, extension)\n",
        "        with open(seq2seq_path, \"w\") as s_out, open(rl_path, \"w\") as r_out, gfile.GFile(conversations_txt,\n",
        "                                                                                        mode=\"rb\") as fin:\n",
        "            tweet = None\n",
        "            reply = None\n",
        "            reply2 = None\n",
        "            for i, line in enumerate(fin):\n",
        "                line = line.decode('utf-8')\n",
        "                line = line.rstrip()\n",
        "                if i % 3 == 0:\n",
        "                    tweet = line\n",
        "                elif i % 3 == 1:\n",
        "                    reply = line\n",
        "                else:\n",
        "                    reply2 = line\n",
        "                    self._write(s_out, tweet, reply, reply2)\n",
        "                    self._write(r_out, tweet, reply, reply)\n",
        "\n",
        "\n",
        "    def _write(self, s_out, tweet, reply, reply2):\n",
        "        s_out.write(tweet)\n",
        "        s_out.write(' ')\n",
        "        s_out.write(reply)\n",
        "        s_out.write('\\n')\n",
        "        s_out.write(reply2)\n",
        "        s_out.write('\\n')\n",
        "        \n",
        "class TrainDataGenerator:\n",
        "    def __init__(self, source_path, hparams):\n",
        "        self.source_path = source_path\n",
        "        self.hparams = hparams\n",
        "        basename, extension = os.path.splitext(self.source_path)\n",
        "        self.enc_path = \"{}_enc{}\".format(basename, extension)\n",
        "        self.dec_path = \"{}_dec{}\".format(basename, extension)\n",
        "        self.enc_idx_path = \"{}_enc_idx{}\".format(basename, extension)\n",
        "        self.dec_idx_path = \"{}_dec_idx{}\".format(basename, extension)\n",
        "        self.dec_idx_eos_path = \"{}_dec_idx_eos{}\".format(basename, extension)\n",
        "        self.dec_idx_sos_path = \"{}_dec_idx_sos{}\".format(basename, extension)\n",
        "        self.dec_idx_len_path = \"{}_dec_idx_len{}\".format(basename, extension)\n",
        "\n",
        "        self.enc_idx_padded_path = \"{}_enc_idx_padded{}\".format(basename,\n",
        "                                                                extension)\n",
        "        self.enc_idx_len_path = \"{}_enc_idx_len{}\".format(basename, extension)\n",
        "\n",
        "        self.vocab_path = \"{}_vocab{}\".format(basename, extension)\n",
        "        \n",
        "        self.generated_files = [self.enc_path, self.dec_path, self.enc_idx_path, self.dec_idx_path, self.dec_idx_eos_path, self.dec_idx_sos_path, self.dec_idx_len_path, self.enc_idx_padded_path, self.vocab_path, self.enc_idx_len_path]\n",
        "        self.max_vocab_size = hparams.vocab_size\n",
        "        self.start_vocabs = [hparams.sos_token, hparams.eos_token, hparams.pad_token, hparams.unk_token]\n",
        "        self.tagger = MeCab.Tagger(\"-Owakati\")\n",
        "        \n",
        "    def remove_generated(self):\n",
        "      for f in self.generated_files:\n",
        "        if os.path.exists(f):\n",
        "          os.remove(f)\n",
        "\n",
        "    def generate(self, vocab_path=None):\n",
        "        print(\"generating enc and dec files...\")\n",
        "        self._generate_enc_dec()\n",
        "        print(\"generating vocab file...\")\n",
        "        if vocab_path is None:\n",
        "          self._generate_vocab()\n",
        "        else:\n",
        "          shutil.copyfile(vocab_path, self.vocab_path)\n",
        "        print(\"loading vocab...\")\n",
        "        vocab, _ = self._load_vocab()\n",
        "        print(\"generating id files...\")\n",
        "        self._generate_id_file(self.enc_path, self.enc_idx_path, vocab)\n",
        "        self._generate_id_file(self.dec_path, self.dec_idx_path, vocab)\n",
        "        print(\"generating padded input file...\")\n",
        "        self._generate_enc_idx_padded(self.enc_idx_path,\n",
        "                                      self.enc_idx_padded_path,\n",
        "                                      self.enc_idx_len_path,\n",
        "                                      self.hparams.encoder_length)\n",
        "        print(\"generating dec eos/sos files...\")\n",
        "        self._generate_dec_idx_eos(self.dec_idx_path, self.dec_idx_eos_path,\n",
        "                                   self.hparams.decoder_length)\n",
        "        self._generate_dec_idx_sos(self.dec_idx_path, self.dec_idx_sos_path,\n",
        "                                   self.dec_idx_len_path,\n",
        "                                   self.hparams.decoder_length)\n",
        "        print(\"done\")\n",
        "        return self._create_dataset()\n",
        "\n",
        "    def _generate_id_file(self, source_path, dest_path, vocab):\n",
        "        if gfile.Exists(dest_path):\n",
        "            return\n",
        "        with gfile.GFile(source_path, mode=\"rb\") as f, gfile.GFile(dest_path,\n",
        "                                                                   mode=\"wb\") as of:\n",
        "            for line in f:\n",
        "                line = line.decode('utf-8')\n",
        "                words = self.tagger.parse(line).split()\n",
        "                ids = [vocab.get(w, self.hparams.unk_id) for w in words]\n",
        "                of.write(\" \".join([str(id) for id in ids]) + \"\\n\")\n",
        "\n",
        "    def _load_vocab(self):\n",
        "        rev_vocab = []\n",
        "        with gfile.GFile(self.vocab_path, mode=\"r\") as f:\n",
        "            rev_vocab.extend(f.readlines())\n",
        "            rev_vocab = [line.strip() for line in rev_vocab]\n",
        "            # Dictionary of (word, idx)\n",
        "            vocab = dict([(x, y) for (y, x) in enumerate(rev_vocab)])\n",
        "            return vocab, rev_vocab\n",
        "\n",
        "    def _generate_vocab(self):\n",
        "        if gfile.Exists(self.vocab_path):\n",
        "            return\n",
        "        vocab_dic = self._build_vocab_dic(self.enc_path)\n",
        "        vocab_dic = self._build_vocab_dic(self.dec_path, vocab_dic)\n",
        "        vocab_list = self.start_vocabs + sorted(vocab_dic, key=vocab_dic.get,\n",
        "                                                reverse=True)\n",
        "        if len(vocab_list) > self.max_vocab_size:\n",
        "            vocab_list = vocab_list[:self.max_vocab_size]\n",
        "        with gfile.GFile(self.vocab_path, mode=\"w\") as vocab_file:\n",
        "            for w in vocab_list:\n",
        "                vocab_file.write(w + \"\\n\")\n",
        "\n",
        "    def _generate_enc_dec(self):\n",
        "        if gfile.Exists(self.enc_path) and gfile.Exists(self.dec_path):\n",
        "            return\n",
        "        with gfile.GFile(self.source_path, mode=\"rb\") as f, gfile.GFile(\n",
        "                self.enc_path, mode=\"w+\") as ef, gfile.GFile(self.dec_path,\n",
        "                                                             mode=\"w+\") as df:\n",
        "            tweet = None\n",
        "            reply = None\n",
        "            for i, line in enumerate(f):\n",
        "                line = line.decode('utf-8')\n",
        "                line = self.sanitize_line(line)\n",
        "                if i % 2 == 0:\n",
        "                  tweet = line\n",
        "                else:\n",
        "                  reply = line\n",
        "                  if tweet and reply:\n",
        "                    ef.write(tweet)\n",
        "                    df.write(reply)\n",
        "                  tweet = None\n",
        "                  reply = None\n",
        "\n",
        "    def _generate_enc_idx_padded(self, source_path, dest_path, dest_len_path,\n",
        "                                 max_line_len):\n",
        "        if gfile.Exists(dest_path):\n",
        "            return\n",
        "        with open(source_path) as fin, open(dest_path,\n",
        "                                            \"w\") as fout, open(dest_len_path,\n",
        "                                                               \"w\") as flen:\n",
        "            line = fin.readline()\n",
        "            while line:\n",
        "                ids = [int(x) for x in line.split()]\n",
        "                if len(ids) > max_line_len:\n",
        "#                    ids = ids[:max_line_len]\n",
        "                    ids = ids[-max_line_len:]\n",
        "                flen.write(str(len(ids)))\n",
        "                flen.write(\"\\n\")\n",
        "                if len(ids) < max_line_len:\n",
        "                    ids.extend([self.hparams.pad_id] * (max_line_len - len(ids)))\n",
        "                ids = [str(x) for x in ids]\n",
        "                fout.write(\" \".join(ids))\n",
        "                fout.write(\"\\n\")\n",
        "                line = fin.readline()\n",
        "\n",
        "    # read decoder_idx file and append eos at the end of idx list.\n",
        "    def _generate_dec_idx_eos(self, source_path, dest_path, max_line_len):\n",
        "        if gfile.Exists(dest_path):\n",
        "            return\n",
        "        with open(source_path) as fin, open(dest_path, \"w\") as fout:\n",
        "            line = fin.readline()\n",
        "            while line:\n",
        "                ids = [int(x) for x in line.split()]\n",
        "                if len(ids) > max_line_len - 1:\n",
        "#                    ids = ids[:max_line_len - 1]\n",
        "                  ids = ids[-(max_line_len - 1):]\n",
        "                ids.append(self.hparams.eos_id)\n",
        "                if len(ids) < max_line_len:\n",
        "                    ids.extend([self.hparams.pad_id] * (max_line_len - len(ids)))\n",
        "                ids = [str(x) for x in ids]\n",
        "                fout.write(\" \".join(ids))\n",
        "                fout.write(\"\\n\")\n",
        "                line = fin.readline()\n",
        "\n",
        "    # read decoder_idx file and put sos at the beginning of the idx list.\n",
        "    # also write out length of index list.\n",
        "    def _generate_dec_idx_sos(self, source_path, dest_path, dest_len_path,\n",
        "                              max_line_len):\n",
        "        if gfile.Exists(dest_path):\n",
        "            return\n",
        "        with open(source_path) as fin, open(dest_path, \"w\") as fout, open(\n",
        "                dest_len_path, \"w\") as flen:\n",
        "            line = fin.readline()\n",
        "            while line:\n",
        "                ids = [self.hparams.sos_id]\n",
        "                ids.extend([int(x) for x in line.split()])\n",
        "                if len(ids) > max_line_len:\n",
        "                    ids = ids[:max_line_len]\n",
        "                flen.write(str(len(ids)))\n",
        "                flen.write(\"\\n\")\n",
        "                if len(ids) < max_line_len:\n",
        "                    ids.extend([self.hparams.pad_id] * (max_line_len - len(ids)))\n",
        "                ids = [str(x) for x in ids]\n",
        "                fout.write(\" \".join(ids))\n",
        "                fout.write(\"\\n\")\n",
        "                line = fin.readline()\n",
        "\n",
        "    @staticmethod\n",
        "    def sanitize_line(line):\n",
        "        # replace @username\n",
        "        # replacing @username had bad impace where USERNAME token shows up everywhere.\n",
        "#        line = re.sub(r\"@([A-Za-z0-9_]+)\", \"USERNAME\", line)\n",
        "        line = re.sub(r\"@([A-Za-z0-9_]+)\", \"\", line)\n",
        "        # Remove URL\n",
        "        line = re.sub(r'https?:\\/\\/.*', \"\", line)\n",
        "        line = line.lstrip()\n",
        "        return line\n",
        "\n",
        "    @staticmethod\n",
        "    def generate_source_target_swapped(source_path):\n",
        "        basename, extension = os.path.splitext(source_path)\n",
        "        dest_path = \"{}_swapped{}\".format(basename, extension)\n",
        "        with gfile.GFile(source_path, mode=\"rb\") as fin, gfile.GFile(dest_path,\n",
        "                                                                     mode=\"w+\") as fout:\n",
        "            temp = None\n",
        "            for i, line in enumerate(fin):\n",
        "                if i % 2 == 0:\n",
        "                    temp = line\n",
        "                else:\n",
        "                    fout.write(line)\n",
        "                    fout.write(temp)\n",
        "                    temp = None\n",
        "        return dest_path\n",
        "\n",
        "    def _build_vocab_dic(self, source_path, vocab_dic={}):\n",
        "        with gfile.GFile(source_path, mode=\"r\") as f:\n",
        "            for line in f:\n",
        "                words = self.tagger.parse(line).split()\n",
        "                for word in words:\n",
        "                    if word in vocab_dic:\n",
        "                        vocab_dic[word] += 1\n",
        "                    else:\n",
        "                        vocab_dic[word] = 1\n",
        "            return vocab_dic\n",
        "\n",
        "    @staticmethod\n",
        "    def _read_file(source_path):\n",
        "        f = open(source_path)\n",
        "        data = f.read()\n",
        "        f.close()\n",
        "        return data\n",
        "\n",
        "    def _read_vocab(self, source_path):\n",
        "        rev_vocab = []\n",
        "        rev_vocab.extend(self._read_file(source_path).splitlines())\n",
        "        rev_vocab = [line.strip() for line in rev_vocab]\n",
        "        vocab = dict([(x, y) for (y, x) in enumerate(rev_vocab)])\n",
        "        return vocab, rev_vocab\n",
        "\n",
        "    def text_line_split_dataset(self, filename):\n",
        "        return tf.data.TextLineDataset(filename).map(self.split_to_int_values)\n",
        "\n",
        "    @staticmethod\n",
        "    def split_to_int_values(x):\n",
        "        return tf.string_to_number(tf.string_split([x]).values, tf.int32)\n",
        "\n",
        "    def _create_dataset(self):\n",
        "\n",
        "        tweets_dataset = self.text_line_split_dataset(self.enc_idx_padded_path)\n",
        "        tweets_lengths_dataset = tf.data.TextLineDataset(\n",
        "            self.enc_idx_len_path)\n",
        "\n",
        "        replies_sos_dataset = self.text_line_split_dataset(\n",
        "            self.dec_idx_sos_path)\n",
        "        replies_eos_dataset = self.text_line_split_dataset(\n",
        "            self.dec_idx_eos_path)\n",
        "        replies_sos_lengths_dataset = tf.data.TextLineDataset(\n",
        "            self.dec_idx_len_path)\n",
        "\n",
        "        tweets_transposed = tweets_dataset.apply(\n",
        "            tf.contrib.data.batch_and_drop_remainder(\n",
        "                self.hparams.batch_size)).map(\n",
        "            lambda x: tf.transpose(x))\n",
        "        tweets_lengths = tweets_lengths_dataset.apply(\n",
        "            tf.contrib.data.batch_and_drop_remainder(self.hparams.batch_size))\n",
        "\n",
        "        replies_with_eos_suffix = replies_eos_dataset.apply(\n",
        "            tf.contrib.data.batch_and_drop_remainder(self.hparams.batch_size))\n",
        "        replies_with_sos_prefix = replies_sos_dataset.apply(\n",
        "            tf.contrib.data.batch_and_drop_remainder(\n",
        "                self.hparams.batch_size)).map(\n",
        "            lambda x: tf.transpose(x))\n",
        "        replies_with_sos_suffix_lengths = replies_sos_lengths_dataset.apply(\n",
        "            tf.contrib.data.batch_and_drop_remainder(\n",
        "                self.hparams.batch_size))\n",
        "        vocab, rev_vocab = self._read_vocab(self.vocab_path)\n",
        "        return tf.data.Dataset.zip((tweets_transposed, tweets_lengths,\n",
        "                                    replies_with_eos_suffix,\n",
        "                                    replies_with_sos_prefix,\n",
        "                                    replies_with_sos_suffix_lengths)), vocab, rev_vocab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O5MLcyf9OVQZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DQg8kU-2Dr-q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Helper functions to test\n",
        "def make_test_training_data(hparams):\n",
        "    train_encoder_inputs = np.empty(\n",
        "        (hparams.encoder_length, hparams.batch_size), dtype=np.int)\n",
        "    train_encoder_inputs_lengths = np.empty(hparams.batch_size, dtype=np.int)\n",
        "    training_target_labels = np.empty(\n",
        "        (hparams.batch_size, hparams.decoder_length), dtype=np.int)\n",
        "    training_decoder_inputs = np.empty(\n",
        "        (hparams.decoder_length, hparams.batch_size), dtype=np.int)\n",
        "\n",
        "    # We keep first tweet to validate inference.\n",
        "    first_tweet = None\n",
        "\n",
        "    for i in range(hparams.batch_size):\n",
        "        # Tweet\n",
        "        tweet = np.random.randint(low=0, high=hparams.vocab_size,\n",
        "                                  size=hparams.encoder_length)\n",
        "        train_encoder_inputs[:, i] = tweet\n",
        "        train_encoder_inputs_lengths[i] = len(tweet)\n",
        "        # Reply\n",
        "        #   Note that low = 2, as 0 and 1 are reserved.\n",
        "        reply = np.random.randint(low=2, high=hparams.vocab_size,\n",
        "                                  size=hparams.decoder_length - 1)\n",
        "\n",
        "        training_target_label = np.concatenate(\n",
        "            (reply, np.array([hparams.eos_id])))\n",
        "        training_target_labels[i] = training_target_label\n",
        "\n",
        "        training_decoder_input = np.concatenate(([hparams.sos_id], reply))\n",
        "        training_decoder_inputs[:, i] = training_decoder_input\n",
        "\n",
        "        if i == 0:\n",
        "            first_tweet = tweet\n",
        "            info(\"0th tweet={}\".format(tweet), hparams)\n",
        "            info(\"0th reply_with_eos_suffix={}\".format(training_target_label),\n",
        "                 hparams)\n",
        "            info(\"0th reply_with_sos_prefix={}\".format(training_decoder_input),\n",
        "                 hparams)\n",
        "\n",
        "        info(\"Tweets\", hparams)\n",
        "        info(train_encoder_inputs, hparams)\n",
        "        info(\"Replies\", hparams)\n",
        "        info(training_target_labels, hparams)\n",
        "        info(training_decoder_inputs, hparams)\n",
        "    return first_tweet, train_encoder_inputs, train_encoder_inputs_lengths, training_target_labels, training_decoder_inputs\n",
        "\n",
        "\n",
        "def test_training(test_hparams, model):\n",
        "    if test_hparams.use_attention:\n",
        "        print(\"==== training model[attention] ====\")\n",
        "    else:\n",
        "        print(\"==== training model ====\")\n",
        "    first_tweet, train_encoder_inputs, train_encoder_inputs_lengths, training_target_labels, training_decoder_inputs = make_test_training_data(\n",
        "        test_hparams)\n",
        "    for i in range(test_hparams.num_train_steps):\n",
        "        _ = model.train(train_encoder_inputs,\n",
        "                        train_encoder_inputs_lengths,\n",
        "                        training_target_labels,\n",
        "                        training_decoder_inputs,\n",
        "                        np.ones(test_hparams.batch_size,\n",
        "                                dtype=int) * test_hparams.decoder_length)\n",
        "        if i % 5 == 0 and test_hparams.debug_verbose:\n",
        "            print('.', end='')\n",
        "\n",
        "        if i % 15 == 0:\n",
        "            model.save()\n",
        "\n",
        "    inference_encoder_inputs = np.empty((test_hparams.encoder_length, 1),\n",
        "                                        dtype=np.int)\n",
        "    inference_encoder_inputs_lengths = np.empty(1, dtype=np.int)\n",
        "    for i in range(1):\n",
        "        inference_encoder_inputs[:, i] = first_tweet\n",
        "        inference_encoder_inputs_lengths[i] = len(first_tweet)\n",
        "\n",
        "    # testing \n",
        "    log_prob54 = model.log_prob(inference_encoder_inputs,\n",
        "                                      inference_encoder_inputs_lengths,\n",
        "                                      np.array([5, 4]))\n",
        "    log_prob65 = model.log_prob(inference_encoder_inputs,\n",
        "                                      inference_encoder_inputs_lengths,\n",
        "                                      np.array([6, 5]))\n",
        "    print(\"log_prob for 54\", log_prob54)\n",
        "    print(\"log_prob for 65\", log_prob65)\n",
        "\n",
        "    reward = model.reward_ease_of_answering(test_hparams.encoder_length,\n",
        "                                                  inference_encoder_inputs,\n",
        "                                                  inference_encoder_inputs_lengths,\n",
        "                                                  np.array([[5], [6]]))\n",
        "    print(\"reward=\", reward)\n",
        "\n",
        "    if test_hparams.debug_verbose:\n",
        "        print(inference_encoder_inputs)\n",
        "    replies = model.infer(inference_encoder_inputs,\n",
        "                                inference_encoder_inputs_lengths)\n",
        "    print(\"Infered replies\", replies[0])\n",
        "    print(\"Expected replies\", training_target_labels[0])\n",
        "\n",
        "\n",
        "def test_distributed_pattern(hparams):\n",
        "    for d in [hparams.model_path]:\n",
        "        shutil.rmtree(d, ignore_errors=True)\n",
        "        os.makedirs(d, exist_ok=True)\n",
        "\n",
        "    print('==== test_distributed_pattern[{} {}] ===='.format(\n",
        "        'attention' if hparams.use_attention else '',\n",
        "        'beam' if hparams.beam_width > 0 else ''))\n",
        "\n",
        "    first_tweet, train_encoder_inputs, train_encoder_inputs_lengths, training_target_labels, training_decoder_inputs = make_test_training_data(\n",
        "        hparams)\n",
        "\n",
        "    model = Trainer().create_model(hparams)\n",
        "\n",
        "    for i in range(hparams.num_train_steps):\n",
        "        _ = model.train(train_encoder_inputs,\n",
        "                        train_encoder_inputs_lengths,\n",
        "                        training_target_labels,\n",
        "                        training_decoder_inputs,\n",
        "                        np.ones(hparams.batch_size,\n",
        "                                dtype=int) * hparams.decoder_length)\n",
        "\n",
        "    model.save()\n",
        "\n",
        "    inference_encoder_inputs = np.empty((hparams.encoder_length, hparams.batch_size),\n",
        "                                        dtype=np.int)\n",
        "    inference_encoder_inputs_lengths = np.empty(hparams.batch_size, dtype=np.int)\n",
        "\n",
        "    for i in range(hparams.batch_size):\n",
        "      inference_encoder_inputs[:, i] = first_tweet\n",
        "      inference_encoder_inputs_lengths[i] = len(first_tweet)\n",
        "\n",
        "    model.restore()\n",
        "    replies = model.infer(inference_encoder_inputs,\n",
        "                                    inference_encoder_inputs_lengths)\n",
        "    print(\"Inferred replies\", replies[0])\n",
        "        \n",
        "    beam_replies, logits = model.infer_beam_search(inference_encoder_inputs,\n",
        "                                                 inference_encoder_inputs_lengths)\n",
        "\n",
        "    print(\"logits\", logits[0])    \n",
        "    print(\"Inferred replies candidate0\", beam_replies[0][:, 0])\n",
        "    print(\"Inferred replies candidate1\", beam_replies[0][:, 1])\n",
        "        \n",
        "    inference_encoder_inputs = np.empty((hparams.encoder_length, hparams.batch_size),\n",
        "                                        dtype=np.int)\n",
        "    inference_encoder_inputs_lengths = np.empty(hparams.batch_size, dtype=np.int)\n",
        "\n",
        "    for i in range(hparams.batch_size):\n",
        "      inference_encoder_inputs[:, i] = first_tweet\n",
        "      inference_encoder_inputs_lengths[i] = len(first_tweet)\n",
        "      \n",
        "    replies = model.sample(inference_encoder_inputs,\n",
        "                                                   inference_encoder_inputs_lengths)\n",
        "    print(\"sample replies\", replies[0])        \n",
        "    print(\"Expected replies\", training_target_labels[0])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9oDXz_ZCNM1F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def test_distributed_one(enable_attention):\n",
        "    hparams = copy.deepcopy(test_hparams).override_from_dict({\n",
        "        'model_path': ModelDirectory.test_distributed.value,\n",
        "        'use_attention': enable_attention,\n",
        "        'beam_width': 2,\n",
        "    })\n",
        "    test_distributed_pattern(hparams)\n",
        "\n",
        "\n",
        "if mode == Mode.Test:\n",
        "    test_distributed_one(enable_attention=False)\n",
        "    test_distributed_one(enable_attention=True)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JrvS_DURF5Pq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def clean_model_path(model_path):\n",
        "    shutil.rmtree(model_path)\n",
        "    os.makedirs(model_path)\n",
        "\n",
        "\n",
        "def print_header(text):\n",
        "    print(\"============== {} ==============\".format(text))\n",
        "\n",
        "\n",
        "def test_tweets_small_swapped(hparams):\n",
        "    replies = [\"@higepon \", \"\", \"\"]\n",
        "    trainer = Trainer()\n",
        "    trainer.train_seq2seq_swapped(hparams, \"tweets_small.txt\", replies)\n",
        "\n",
        "\n",
        "# vocab size \n",
        "tweet_small_hparams = copy.deepcopy(base_hparams).override_from_dict(\n",
        "    {\n",
        "        'batch_size': 6,  # of tweets should be dividable by batch_size\n",
        "        'encoder_length': 8,\n",
        "        'decoder_length': 8,\n",
        "        'num_units': 256,\n",
        "        'num_layers': 2,\n",
        "        'vocab_size': 34,\n",
        "        'embedding_size': 40,\n",
        "        'beam_width': 2,  # for faster iteration, this should be 10\n",
        "        'num_train_steps': 200,\n",
        "        'model_path': ModelDirectory.tweet_small.value,\n",
        "        'learning_rate': 0.05,\n",
        "        'use_attention': True,\n",
        "    })\n",
        "\n",
        "tweet_small_swapped_hparams = copy.deepcopy(\n",
        "    tweet_small_hparams).override_from_dict(\n",
        "    {'model_path': ModelDirectory.tweet_small_swapped.value})\n",
        "\n",
        "if mode == Mode.Test:\n",
        "    tweets_path = \"tweets_small.txt\"\n",
        "    TrainDataGenerator(tweets_path, tweet_small_hparams).remove_generated()\n",
        "    trainer = Trainer()\n",
        "    trainer.train_seq2seq(tweet_small_hparams, tweets_path,\n",
        "                          [\"\", \"\", \"\"])\n",
        "    test_tweets_small_swapped(tweet_small_swapped_hparams)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tzh2rhEPguJ9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def test_tweets_large(hparams):\n",
        "    tweets = [\"\", \"\", \"\",\n",
        "              \"\", \"\", \"\", \"\",\n",
        "              \"\", \"\", \"\"]\n",
        "    trainer = Trainer()\n",
        "    trainer.train_seq2seq(hparams, \"tweets_conversation.txt\", tweets,\n",
        "                          should_clean_saved_model=False)\n",
        "    return trainer.model\n",
        "\n",
        "\n",
        "def test_tweets_large_swapped(hparams):\n",
        "    tweets = [\"\", \"\", \"\", \"\",\n",
        "              \"\", \"\", \"\"]\n",
        "    trainer = Trainer()\n",
        "    trainer.train_seq2seq_swapped(hparams, \"tweets_large.txt\", tweets,\n",
        "                                  should_clean_saved_model=False)\n",
        "    return trainer.model\n",
        "\n",
        "\n",
        "tweet_large_hparams = copy.deepcopy(base_hparams).override_from_dict(\n",
        "    {\n",
        "        # In typical seq2seq chatbot\n",
        "        # num_layers=3, learning_rate=0.5, batch_size=64, vocab=20000-100000, learning_rate decay is 0.99, which is taken care as default parameter in AdamOptimizer.\n",
        "        'batch_size': 64,  # of tweets should be dividable by batch_size\n",
        "        'encoder_length': 28,\n",
        "        'decoder_length': 28,\n",
        "        'num_units': 1024,\n",
        "        'num_layers': 3,\n",
        "        'vocab_size': 60000,\n",
        "    # conversations.txt actually has about 70K uniq words.\n",
        "        'embedding_size': 1024,\n",
        "        'beam_width': 2,  # for faster iteration, this should be 10\n",
        "        'num_train_steps': 1000000,\n",
        "        'model_path': ModelDirectory.tweet_large.value,\n",
        "        'learning_rate': 0.5,\n",
        "    # For vocab_size 50000, num_layers 3, num_units 1024, tweet_large, starting learning_rate 0.05 works well, change it t0 0.01 at perplexity 800, changed it to 0.005 at 200.\n",
        "        'learning_rate_decay': 0.99,\n",
        "        'use_attention': True,\n",
        "        # testing new restore learning rate and no USERNAME TOKEN\n",
        "    })\n",
        "\n",
        "tweet_large_swapped_hparams = copy.deepcopy(\n",
        "    tweet_large_hparams).override_from_dict(\n",
        "    {\n",
        "        'model_path': ModelDirectory.tweet_large_swapped.value\n",
        "    })\n",
        "\n",
        "#Shell.save_model_in_drive(tweet_large_hparams.model_path)\n",
        "\n",
        "if mode == Mode.TrainSeq2Seq:\n",
        "    print(\"train seq2seq\")\n",
        "    test_tweets_large(tweet_large_hparams)\n",
        "elif mode == Mode.TrainSeq2SeqSwapped:\n",
        "    print(\"train seq2seq swapped\")\n",
        "    test_tweets_large_swapped(tweet_large_swapped_hparams)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aAv3l15jQl4z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "small_hparams = copy.deepcopy(tweet_small_hparams).override_from_dict({\n",
        "    'learning_rate': 0.1,\n",
        "    'batch_size': 16,\n",
        "    'num_train_steps': 200,\n",
        "})\n",
        "\n",
        "rl_small_hparams = copy.deepcopy(tweet_small_hparams).override_from_dict({\n",
        "    'learning_rate': 0.1,\n",
        "    'batch_size': 16,\n",
        "    'num_train_steps': 2000,\n",
        "    'model_path': ModelDirectory.tweet_small_rl.value\n",
        "})\n",
        "\n",
        "if mode == Mode.TrainRL:\n",
        "  with memory_util.capture_stderr() as stderr:\n",
        "      try:\n",
        "          trainer = Trainer()\n",
        "          trainer.train_seq2seq_rl(small_hparams, rl_small_hparams, \"tweets_small.txt\", resume=False)\n",
        "      except Exception as e:\n",
        "        print(stderr.getvalue())\n",
        "        raise (e)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "364y2iONjzmp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "conversations_small_hparams = copy.deepcopy(base_hparams).override_from_dict(\n",
        "    {\n",
        "        'batch_size': 6,  # of tweets should be dividable by batch_size\n",
        "        'encoder_length': 8,\n",
        "        'decoder_length': 8,\n",
        "        'num_units': 256,\n",
        "        'num_layers': 2,\n",
        "        'vocab_size': 34,\n",
        "        'embedding_size': 40,\n",
        "        'beam_width': 2,  # for faster iteration, this should be 10\n",
        "        'num_train_steps': 200,\n",
        "        'model_path': ModelDirectory.conversations_small.value,\n",
        "        'learning_rate': 0.05,\n",
        "        'use_attention': True,\n",
        "    })\n",
        "\n",
        "conversations_small_backward_hparams = copy.deepcopy(\n",
        "    conversations_small_hparams).override_from_dict(\n",
        "    {\n",
        "        'model_path': ModelDirectory.conversations_small_backward.value,\n",
        "    })\n",
        "\n",
        "conversations_small_rl_hparams = copy.deepcopy(\n",
        "    conversations_small_hparams).override_from_dict(\n",
        "    {\n",
        "        'model_path': ModelDirectory.conversations_small_rl.value,\n",
        "        'num_train_steps': 200,\n",
        "    })\n",
        "\n",
        "conversations_txt = \"conversations_tiny.txt\"\n",
        "Shell.download_file_if_necessary(conversations_txt)\n",
        "ConversationTrainDataGenerator().generate(conversations_txt)\n",
        "#data_source = TrainDataSource(\"conversations_tiny_seq2seq.txt\",\n",
        "#                              conversations_small_hparams)\n",
        "#vocab_path = data_source.vocab_path\n",
        "#data_source2 = TrainDataSource(\"conversations_tiny_rl.txt\",\n",
        "#                              conversations_small_hparams, vocab_path)\n",
        "\n",
        "with memory_util.capture_stderr() as stderr:\n",
        "    try:\n",
        "        trainer = Trainer()\n",
        "        valid_tweets = [\"\"]\n",
        "        trainer.train_seq2seq(conversations_small_hparams,\n",
        "                              \"conversations_tiny_seq2seq.txt\",\n",
        "                              valid_tweets)\n",
        "        trainer.train_seq2seq_swapped(conversations_small_backward_hparams,\n",
        "                                      \"conversations_tiny_seq2seq.txt\",\n",
        "                                      [\"\"], vocab_path=\"conversations_tiny_seq2seq_vocab.txt\")\n",
        "\n",
        "        Trainer().train_beam_rl(conversations_small_rl_hparams,\n",
        "                                conversations_small_hparams,\n",
        "                                conversations_small_backward_hparams,\n",
        "                                \"conversations_tiny_seq2seq.txt\",\n",
        "\n",
        "                                \"conversations_tiny_rl.txt\",\n",
        "                                valid_tweets)\n",
        "    except Exception as e:\n",
        "        print(stderr.getvalue())\n",
        "        raise (e)\n",
        "\n",
        "!ls - lSh\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uvSZqViNGQAZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ls -lSh *vocab*\n",
        "#!cat conversations_tiny_rl_vocab.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UrJyk2rTHYG5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!rm conversations_tiny*\n",
        "!head drive/seq2seq_data/conversations_tiny.txt\n",
        "#!ls conversations_tiny*\n",
        "#!head conversations_tiny_seq2seq.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CCiNJG14XrAv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "small_hparams = copy.deepcopy(tweet_small_hparams).override_from_dict({\n",
        "    'learning_rate': 0.1,\n",
        "    'batch_size': 16,\n",
        "    'num_train_steps': 200,\n",
        "})\n",
        "\n",
        "rl_small_hparams = copy.deepcopy(tweet_small_hparams).override_from_dict({\n",
        "    'learning_rate': 0.1,\n",
        "    'batch_size': 16,\n",
        "    'num_train_steps': 2000,\n",
        "    'model_path': ModelDirectory.tweet_small_rl.value\n",
        "})\n",
        "\n",
        "with memory_util.capture_stderr() as stderr:\n",
        "    try:\n",
        "        trainer = Trainer()\n",
        "        trainer.train_seq2seq_beam_rl(small_hparams, rl_small_hparams, \"tweets_small.txt\", resume=False)\n",
        "    except Exception as e:\n",
        "      print(stderr.getvalue())\n",
        "      raise (e)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4ZaAyc29H9Ww",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if mode == Mode.TrainRL:\n",
        "  Shell.download_logs(rl_small_hparams.model_path)\n",
        "  !rm $small_hparams.model_path/*\n",
        "  !ls -lSh $small_hparams.model_path\n",
        "  !rm $rl_small_hparams.model_path/*\n",
        "  !ls -lSh $rl_small_hparams.model_path"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WiazM73iU1hK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "large_hparams = copy.deepcopy(tweet_large_hparams).override_from_dict({\n",
        "    'num_train_steps': 40000,\n",
        "})\n",
        "\n",
        "rl_large_hparams = copy.deepcopy(tweet_large_hparams).override_from_dict({\n",
        "    'num_train_steps': 20000,\n",
        "    'model_path': ModelDirectory.tweet_large_rl.value\n",
        "})\n",
        "\n",
        "if mode == Mode.TrainRL:\n",
        "  with memory_util.capture_stderr() as stderr:\n",
        "    try:\n",
        "        trainer = Trainer()\n",
        "        trainer.train_seq2seq_rl(large_hparams, rl_large_hparams, \"tweets_large.txt\", resume=False)\n",
        "    except KeyboardInterrupt:       \n",
        "      print(stderr.getvalue())      \n",
        "    except Exception as e:\n",
        "      print(stderr.getvalue())\n",
        "      raise (e)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GHMFjv2Nbze3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!cp model/tweet_small/* drive/\n",
        "#!ls model/tweet_small\n",
        "#!cp $rl_small_hparams.model_path/* drive/\n",
        "Shell.download_logs(rl_large_hparams.model_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2ZDb62hiz7r4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!rm tweets_large.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yimhfKo9BXG5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if mode == Mode.Test:\n",
        "    Shell.download_logs(ModelDirectory.tweet_small_rl.value)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T_q-Ns9hiHMB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# N.B: This would fail if we try to download logs in the previous cell.\n",
        "# My guess is tflog is somehow locking the log file when running the cell.\n",
        "#download_logs()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hxPwNy-70_9X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class StreamListener(tweepy.StreamListener):\n",
        "    def __init__(self, api, helper):\n",
        "        self.api = api\n",
        "        self.helper = helper\n",
        "\n",
        "    def on_status(self, status):\n",
        "        # done handle @reply only\n",
        "        # done print reply\n",
        "        # add model parameter\n",
        "        # direct reply\n",
        "        # unk reply\n",
        "        # shuffle beam search\n",
        "        print(\"{0}: {1}\".format(status.text, status.author.screen_name))\n",
        "\n",
        "        screen_name = status.author.screen_name\n",
        "        # ignore my tweets\n",
        "        if screen_name == self.api.me().screen_name:\n",
        "            print(\"Ignored my tweet\")\n",
        "            return True\n",
        "        elif status.text.startswith(\"@{0}\".format(self.api.me().screen_name)):\n",
        "\n",
        "            replies = self.helper.inferences(status.text)\n",
        "            reply = random.choice(replies)\n",
        "            reply = \"@\" + status.author.screen_name + \" \" + reply\n",
        "            print(reply)\n",
        "            self.api.update_status(status=reply,\n",
        "                                   in_reply_to_status_id=status.id)\n",
        "\n",
        "            return True\n",
        "\n",
        "    @staticmethod\n",
        "    def on_error(status_code):\n",
        "        print(status_code)\n",
        "        return True\n",
        "\n",
        "\n",
        "def listener(hparams):\n",
        "    Shell.download_model_data_if_necessary(hparams.model_path)\n",
        "\n",
        "    rl_train_graph = tf.Graph()\n",
        "    rl_infer_graph = tf.Graph()\n",
        "    rl_train_sess = tf.Session(graph=rl_train_graph)\n",
        "    rl_infer_sess = tf.Session(graph=rl_infer_graph)\n",
        "\n",
        "    _, infer_model = create_train_infer_models_in_graphs(rl_train_graph,\n",
        "                                                         rl_train_sess,\n",
        "                                                         rl_infer_graph,\n",
        "                                                         rl_infer_sess,\n",
        "                                                         hparams)\n",
        "\n",
        "    source_path = \"tweets_large.txt\"\n",
        "    Shell.download_file_if_necessary(source_path)\n",
        "    generator = TrainDataGenerator(source_path=source_path, hparams=hparams)\n",
        "    _, vocab, rev_vocab = generator.generate()\n",
        "    infer_model.restore()\n",
        "    helper = InferenceHelper(infer_model, vocab, rev_vocab)\n",
        "\n",
        "    config_path = 'config.yml'\n",
        "    Shell.download_file_if_necessary(config_path)\n",
        "    f = open(config_path, 'rt')\n",
        "    cfg = yaml.load(f)['twitter']\n",
        "\n",
        "    consumer_key = cfg['consumer_key']\n",
        "    consumer_secret = cfg['consumer_secret']\n",
        "    access_token = cfg['access_token']\n",
        "    access_token_secret = cfg['access_token_secret']\n",
        "\n",
        "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "    auth.set_access_token(access_token, access_token_secret)\n",
        "    api = tweepy.API(auth)\n",
        "\n",
        "    while True:\n",
        "        #    try:\n",
        "        stream = tweepy.Stream(auth=api.auth,\n",
        "                               listener=StreamListener(api, helper))\n",
        "        print(\"listener starting...\")\n",
        "        stream.userstream()\n",
        "#    except Exception as e:\n",
        "#     print(e.__doc__)\n",
        "\n",
        "\n",
        "tweet_hparams = copy.deepcopy(rl_dst_hparams).override_from_dict(\n",
        "    {'beam_width': 50})\n",
        "if mode == Mode.TweetBot:\n",
        "    listener(tweet_hparams)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}