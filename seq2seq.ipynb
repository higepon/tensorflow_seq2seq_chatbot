{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FnS-GXJOJOY2"
   },
   "source": [
    "Tensorflow 1.4.0 is required.\n",
    "This is based on [NMT Tutorial](https://github.com/tensorflow/nmt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "jBSwDx-aaOG9"
   },
   "outputs": [],
   "source": [
    "import platform\n",
    "\n",
    "\n",
    "def colab():\n",
    "    return \"Darwin\" != platform.system()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "caxRbRVkDdhp"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import copy\n",
    "import datetime\n",
    "import os\n",
    "from random import randint\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.layers import core as layers_core\n",
    "from tensorflow.python.platform import gfile\n",
    "\n",
    "if colab():\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "## Note for me. You've summarized Seq2Seq at http://d.hatena.ne.jp/higepon/20171210/1512887715.\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "#@formatter:off\n",
    "!mkdir \"./saved_model2\"\n",
    "!mkdir \"./saved_model\"\n",
    "!ls -la ./saved_model2\n",
    "#@formatter:on\n",
    "\n",
    "# TODO\n",
    "# Use GRU instead of LSTM\n",
    "\n",
    "test_hparams = tf.contrib.training.HParams(\n",
    "    batch_size=3,\n",
    "    encoder_length=5,\n",
    "    decoder_length=5,\n",
    "    num_units=6,\n",
    "    vocab_size=9,\n",
    "    embedding_size=8,\n",
    "    learning_rate=0.01,\n",
    "    max_gradient_norm=5.0,\n",
    "    beam_width=9,\n",
    "    use_attention=False,\n",
    "    num_train_steps=100,\n",
    "    debug_verbose=False\n",
    ")\n",
    "\n",
    "test_attention_hparams = copy.deepcopy(test_hparams)\n",
    "test_attention_hparams.use_attention = True\n",
    "\n",
    "real_hparams = tf.contrib.training.HParams(\n",
    "    batch_size=25,  # of tweets should be devidable by batch_size\n",
    "    encoder_length=20,\n",
    "    decoder_length=20,\n",
    "    num_units=1024,\n",
    "    vocab_size=500,\n",
    "    embedding_size=256,\n",
    "    learning_rate=0.01,\n",
    "    max_gradient_norm=5.0,\n",
    "    beam_width=9,\n",
    "    use_attention=False,\n",
    "    num_train_steps=16,\n",
    "    debug_verbose=False\n",
    ")\n",
    "\n",
    "large_hparams = tf.contrib.training.HParams(\n",
    "    batch_size=50,  # of tweets should be devidable by batch_size\n",
    "    encoder_length=30,\n",
    "    decoder_length=30,\n",
    "    num_units=1024,\n",
    "    vocab_size=50000,\n",
    "    embedding_size=1024,\n",
    "    learning_rate=0.01,\n",
    "    max_gradient_norm=5.0,\n",
    "    beam_width=2,  # for faster iteration, this should be 10\n",
    "    use_attention=False,\n",
    "    num_train_steps=1000000,\n",
    "    debug_verbose=False\n",
    ")\n",
    "\n",
    "# Model path\n",
    "model_path = \"./saved_model/twitter\"\n",
    "\n",
    "# Symbol for start decode process.\n",
    "tgt_sos_id = 0\n",
    "\n",
    "# Symbol for end of decode process.\n",
    "tgt_eos_id = 1\n",
    "\n",
    "pad_id = 2\n",
    "\n",
    "unk_id = 3\n",
    "\n",
    "\n",
    "def info(message, hparams):\n",
    "    if hparams.debug_verbose:\n",
    "        print(message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "DFEYKvBwL3Nm"
   },
   "outputs": [],
   "source": [
    "# For debug purpose.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "class ChatbotModel:\n",
    "    def __init__(self, sess, hparams, model_path, scope='ChatbotModel'):\n",
    "        self.sess = sess\n",
    "        # todo remove\n",
    "        self.hparams = hparams\n",
    "\n",
    "        # todo\n",
    "        self.model_path = model_path\n",
    "        self.name = scope\n",
    "\n",
    "        self.encoder_inputs, self.encoder_inputs_lengths, encoder_outputs, encoder_state, embedding_encoder = self._build_encoder(\n",
    "            hparams, scope)\n",
    "        self.decoder_inputs, self.decoder_target_lengths, logits = self._build_decoder(\n",
    "            hparams, self.encoder_inputs_lengths, embedding_encoder,\n",
    "            encoder_state, encoder_outputs, scope)\n",
    "\n",
    "        self.reward = tf.placeholder(tf.float32, name=\"reward\")\n",
    "        self.target_labels, self.loss, self.global_step, self.train_op = self._build_optimizer(\n",
    "            hparams, logits)\n",
    "\n",
    "        # Initialize saver after model created\n",
    "        self.saver = tf.train.Saver(tf.global_variables(), max_to_keep=1)\n",
    "\n",
    "    def restore(self):\n",
    "        ckpt = tf.train.get_checkpoint_state(self.model_path)\n",
    "        if ckpt:\n",
    "            last_model = ckpt.model_checkpoint_path\n",
    "            self.saver.restore(self.sess, last_model)\n",
    "            return True\n",
    "        else:\n",
    "            print(\"Created fresh model.\")\n",
    "            return False\n",
    "\n",
    "    def train(self, encoder_inputs, encoder_inputs_lengths, target_labels,\n",
    "              decoder_inputs, decoder_target_lengths, reward=1.0):\n",
    "        feed_dict = {\n",
    "            self.encoder_inputs: encoder_inputs,\n",
    "            self.encoder_inputs_lengths: encoder_inputs_lengths,\n",
    "            self.target_labels: target_labels,\n",
    "            self.decoder_inputs: decoder_inputs,\n",
    "            self.decoder_target_lengths: decoder_target_lengths,\n",
    "            # For normal Seq2Seq reward is always 1.\n",
    "            self.reward: reward\n",
    "        }\n",
    "        _, loss_value, global_step = self.sess.run(\n",
    "            [self.train_op, self.loss, self.global_step], feed_dict=feed_dict)\n",
    "        return loss_value, global_step\n",
    "\n",
    "    def batch_loss(self, encoder_inputs, encoder_inputs_lengths, target_labels,\n",
    "                   decoder_inputs, decoder_target_lengths):\n",
    "        feed_dict = {\n",
    "            self.encoder_inputs: encoder_inputs,\n",
    "            self.encoder_inputs_lengths: encoder_inputs_lengths,\n",
    "            self.target_labels: target_labels,\n",
    "            self.decoder_inputs: decoder_inputs,\n",
    "            self.decoder_target_lengths: decoder_target_lengths,\n",
    "            # For normal Seq2Seq reward is always 1.\n",
    "            self.reward: 1\n",
    "        }\n",
    "        return self.sess.run(self.loss, feed_dict=feed_dict)\n",
    "\n",
    "    def train_with_reward(self, infer_model, standard_seq2seq_model,\n",
    "                          encoder_inputs, encoder_inputs_lengths, target_labels,\n",
    "                          decoder_inputs, decoder_target_lengths,\n",
    "                          dull_responses):\n",
    "        infered_replies = infer_model.infer(encoder_inputs,\n",
    "                                            encoder_inputs_lengths)\n",
    "        standard_seq2seq_encoder_inputs = []\n",
    "        standard_seq2seq_encoder_inputs_lengths = []\n",
    "        for reply in infered_replies:\n",
    "            standard_seq2seq_encoder_inputs_lengths.append(len(reply))\n",
    "            if len(reply) <= self.hparams.encoder_length:\n",
    "                standard_seq2seq_encoder_inputs.append(np.append(reply, (\n",
    "                    [pad_id] * (self.hparams.encoder_length - len(reply)))))\n",
    "            else:\n",
    "                raise Exception(\n",
    "                    \"Infered reply is not suppose to be longer than encoder_input\")\n",
    "        standard_seq2seq_encoder_inputs = np.transpose(\n",
    "            np.array(standard_seq2seq_encoder_inputs))\n",
    "        reward1 = standard_seq2seq_model.reward_ease_of_answering(\n",
    "            standard_seq2seq_encoder_inputs,\n",
    "            standard_seq2seq_encoder_inputs_lengths, dull_responses)\n",
    "        reward2 = 0  # todo\n",
    "        reward3 = 0  # todo\n",
    "        reward = 0.25 * reward1 + 0.25 * reward2 + 0.5 * reward3\n",
    "        return self.train(encoder_inputs, encoder_inputs_lengths, target_labels,\n",
    "                          decoder_inputs, decoder_target_lengths, reward)\n",
    "\n",
    "    def save(self, model_path=None):\n",
    "        if model_path is None:\n",
    "            model_path = self.model_path\n",
    "        model_dir = \"{}/{}\".format(model_path, self.name)\n",
    "        self.saver.save(self.sess, model_dir, global_step=self.global_step)\n",
    "\n",
    "    @staticmethod\n",
    "    def _softmax(x):\n",
    "        return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "    def _build_optimizer(self, hparams, logits):\n",
    "        # Target labels\n",
    "        #   As described in doc for sparse_softmax_cross_entropy_with_logits,\n",
    "        #   labels should be [batch_size, decoder_target_lengths] instead of [batch_size, decoder_target_lengths, vocab_size].\n",
    "        #   So labels should have indices instead of vocab_size classes.\n",
    "        target_labels = tf.placeholder(tf.int32, shape=(\n",
    "            hparams.batch_size, hparams.decoder_length), name=\"target_labels\")\n",
    "\n",
    "        # Loss\n",
    "        crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            labels=target_labels, logits=logits)\n",
    "\n",
    "        loss = tf.reduce_sum(crossent / tf.to_float(hparams.batch_size))\n",
    "        # Adjust loss with reward.\n",
    "        loss = tf.multiply(loss, self.reward)\n",
    "\n",
    "        # Train\n",
    "        global_step = tf.get_variable(name=\"global_step\", shape=[],\n",
    "                                      dtype=tf.int32,\n",
    "                                      initializer=tf.constant_initializer(0),\n",
    "                                      trainable=False)\n",
    "\n",
    "        # Calculate and clip gradients\n",
    "        params = tf.trainable_variables()\n",
    "        gradients = tf.gradients(loss, params)\n",
    "        clipped_gradients, _ = tf.clip_by_global_norm(\n",
    "            gradients, hparams.max_gradient_norm)\n",
    "\n",
    "        # Optimization\n",
    "        optimizer = tf.train.AdamOptimizer(hparams.learning_rate)\n",
    "        train_op = optimizer.apply_gradients(\n",
    "            zip(clipped_gradients, params), global_step=global_step)\n",
    "        return target_labels, loss, global_step, train_op\n",
    "\n",
    "    def _build_encoder(self, hparams, scope):\n",
    "        # Encoder\n",
    "        #   encoder_inputs: [encoder_length, batch_size]\n",
    "        #   This is time major where encoder_length comes first instead of batch_size.\n",
    "        #   encoder_inputs_lengths: [batch_size]\n",
    "        encoder_inputs = tf.placeholder(tf.int32, shape=(\n",
    "            hparams.encoder_length, hparams.batch_size), name=\"encoder_inputs\")\n",
    "        encoder_inputs_lengths = tf.placeholder(tf.int32,\n",
    "                                                shape=(hparams.batch_size),\n",
    "                                                name=\"encoder_inputs_lengtsh\")\n",
    "\n",
    "        # Embedding\n",
    "        #   We originally didn't share embbedding between encoder and decoder.\n",
    "        #   But now we share it. It makes much easier to calculate rewards.\n",
    "        #   Matrix for embedding: [vocab_size, embedding_size]\n",
    "        #   Should be shared between training and inference.\n",
    "        with tf.variable_scope(scope):\n",
    "            embedding_encoder = tf.get_variable(\"embedding_encoder\",\n",
    "                                                [hparams.vocab_size,\n",
    "                                                 hparams.embedding_size])\n",
    "\n",
    "        # Look up embedding:\n",
    "        #   encoder_inputs: [encoder_length, batch_size]\n",
    "        #   encoder_emb_inputs: [encoder_length, batch_size, embedding_size]\n",
    "        encoder_emb_inputs = tf.nn.embedding_lookup(embedding_encoder,\n",
    "                                                    encoder_inputs)\n",
    "\n",
    "        # LSTM cell.\n",
    "        with tf.variable_scope(scope):\n",
    "            # Should be shared between training and inference.\n",
    "            encoder_cell = tf.nn.rnn_cell.BasicLSTMCell(hparams.num_units)\n",
    "\n",
    "        # Run Dynamic RNN\n",
    "        #   encoder_outputs: [encoder_length, batch_size, num_units]\n",
    "        #   encoder_state: [batch_size, num_units], this is final state of the cell for each batch.\n",
    "        with tf.variable_scope(scope):\n",
    "            encoder_outputs, encoder_state = tf.nn.dynamic_rnn(encoder_cell,\n",
    "                                                               encoder_emb_inputs,\n",
    "                                                               time_major=True,\n",
    "                                                               dtype=tf.float32,\n",
    "                                                               sequence_length=encoder_inputs_lengths)\n",
    "\n",
    "        return encoder_inputs, encoder_inputs_lengths, encoder_outputs, encoder_state, embedding_encoder\n",
    "\n",
    "\n",
    "    def _build_training_decoder(self, hparams, encoder_inputs_lengths,\n",
    "                                encoder_state, encoder_outputs, decoder_cell,\n",
    "                                decoder_emb_inputs, decoder_target_lengths,\n",
    "                                projection_layer):\n",
    "        # Decoder with helper:\n",
    "        #   decoder_emb_inputs: [decoder_length, batch_size, embedding_size]\n",
    "        #   decoder_target_lengths: [batch_size] vector, which represents each target sequence length.\n",
    "        training_helper = tf.contrib.seq2seq.TrainingHelper(decoder_emb_inputs,\n",
    "                                                            decoder_target_lengths,\n",
    "                                                            time_major=True)\n",
    "\n",
    "        # See https://github.com/tensorflow/tensorflow/issues/11904\n",
    "        if hparams.use_attention:\n",
    "            # Attention\n",
    "            # encoder_outputs is time major, so transopse it to batch major.\n",
    "            # attention_encoder_outputs: [batch_size, encoder_length, num_units]\n",
    "            attention_encoder_outputs = tf.transpose(encoder_outputs, [1, 0, 2])\n",
    "\n",
    "            # Create an attention mechanism\n",
    "            attention_mechanism = tf.contrib.seq2seq.LuongAttention(\n",
    "                hparams.num_units,\n",
    "                attention_encoder_outputs,\n",
    "                memory_sequence_length=encoder_inputs_lengths)\n",
    "\n",
    "            wrapped_decoder_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
    "                decoder_cell, attention_mechanism,\n",
    "                attention_layer_size=hparams.num_units)\n",
    "\n",
    "            initial_state = wrapped_decoder_cell.zero_state(hparams.batch_size,\n",
    "                                                            tf.float32).clone(\n",
    "                cell_state=encoder_state)\n",
    "        else:\n",
    "            wrapped_decoder_cell = decoder_cell\n",
    "            initial_state = encoder_state\n",
    "\n",
    "            # Decoder and decode\n",
    "        training_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "            wrapped_decoder_cell, training_helper, initial_state,\n",
    "            output_layer=projection_layer)\n",
    "\n",
    "        # Dynamic decoding\n",
    "        #   final_outputs.rnn_output: [batch_size, decoder_length, vocab_size], list of RNN state.\n",
    "        #   final_outputs.sample_id: [batch_size, decoder_length], list of argmax of rnn_output.\n",
    "        #   final_state: [batch_size, num_units], list of final state of RNN on decode process.\n",
    "        #   final_sequence_lengths: [batch_size], list of each decoded sequence. \n",
    "        final_outputs, _final_state, _final_sequence_lengths = tf.contrib.seq2seq.dynamic_decode(\n",
    "            training_decoder)\n",
    "\n",
    "        if hparams.debug_verbose:\n",
    "            print(\"rnn_output.shape=\", final_outputs.rnn_output.shape)\n",
    "            print(\"sample_id.shape=\", final_outputs.sample_id.shape)\n",
    "            print(\"final_state=\", _final_state)\n",
    "            print(\"final_sequence_lengths.shape=\",\n",
    "                  _final_sequence_lengths.shape)\n",
    "\n",
    "        logits = final_outputs.rnn_output\n",
    "        return logits, wrapped_decoder_cell, initial_state\n",
    "\n",
    "    def _build_decoder(self, hparams, encoder_inputs_lengths, embedding_encoder,\n",
    "                       encoder_state, encoder_outputs, scope):\n",
    "        # Decoder input\n",
    "        #   decoder_inputs: [decoder_length, batch_size]\n",
    "        #   decoder_target_lengths: [batch_size]\n",
    "        #   This is grand truth target inputs for training.\n",
    "        decoder_inputs = tf.placeholder(tf.int32, shape=(\n",
    "            hparams.decoder_length, hparams.batch_size), name=\"decoder_inputs\")\n",
    "        decoder_target_lengths = tf.placeholder(tf.int32,\n",
    "                                                shape=(hparams.batch_size),\n",
    "                                                name=\"decoder_target_lengths\")\n",
    "\n",
    "        # Look up embedding:\n",
    "        #   decoder_inputs: [decoder_length, batch_size]\n",
    "        #   decoder_emb_inp: [decoder_length, batch_size, embedding_size]\n",
    "        decoder_emb_inputs = tf.nn.embedding_lookup(embedding_encoder,\n",
    "                                                    decoder_inputs)\n",
    "\n",
    "        # https://stackoverflow.com/questions/39573188/output-projection-in-seq2seq-model-tensorflow\n",
    "        # Internally, a neural network operates on dense vectors of some size,\n",
    "        # often 256, 512 or 1024 floats (let's say 512 for here). \n",
    "        # But at the end it needs to predict a word from the vocabulary which is often much larger,\n",
    "        # e.g., 40000 words. Output projection is the final linear layer that converts (projects) from the internal representation to the larger one.\n",
    "        # So, for example, it can consist of a 512 x 40000 parameter matrix and a 40000 parameter for the bias vector.\n",
    "        projection_layer = layers_core.Dense(hparams.vocab_size, use_bias=False)\n",
    "\n",
    "        # We share this between training and inference.\n",
    "        decoder_cell = tf.nn.rnn_cell.BasicLSTMCell(hparams.num_units)\n",
    "\n",
    "        # Training graph\n",
    "        logits, wrapped_decoder_cell, initial_state = self._build_training_decoder(\n",
    "            hparams, encoder_inputs_lengths, encoder_state, encoder_outputs,\n",
    "            decoder_cell, decoder_emb_inputs, decoder_target_lengths,\n",
    "            projection_layer)\n",
    "\n",
    "        return decoder_inputs, decoder_target_lengths, logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "JzDknaQZV-iU"
   },
   "outputs": [],
   "source": [
    "class ChatbotInferenceModel:\n",
    "    def __init__(self, sess, hparams, model_path, scope='ChatbotModel'):\n",
    "        self.sess = sess\n",
    "        # todo remove\n",
    "        self.hparams = hparams\n",
    "\n",
    "        # todo\n",
    "        self.model_path = model_path\n",
    "        self.name = scope\n",
    "\n",
    "        self.encoder_inputs, self.encoder_inputs_lengths, encoder_outputs, encoder_state, embedding_encoder = self._build_encoder(\n",
    "            hparams, scope)\n",
    "        self.decoder_inputs, self.decoder_target_lengths, self.replies, self.beam_replies, self.infer_logits = self._build_decoder(\n",
    "            hparams, self.encoder_inputs_lengths, embedding_encoder,\n",
    "            encoder_state, encoder_outputs, scope)\n",
    "\n",
    "        self.reward = tf.placeholder(tf.float32, name=\"reward\")\n",
    "\n",
    "        # we can't use variable length here,  because tiled_batch requires constant length.\n",
    "        self.batch_size = 1\n",
    "\n",
    "        # Initialize saver after model created\n",
    "        self.saver = tf.train.Saver(tf.global_variables(), max_to_keep=1)\n",
    "\n",
    "    def restore(self):\n",
    "        ckpt = tf.train.get_checkpoint_state(self.model_path)\n",
    "        if ckpt:\n",
    "            last_model = ckpt.model_checkpoint_path\n",
    "            self.saver.restore(self.sess, last_model)\n",
    "            return True\n",
    "        else:\n",
    "            print(\"Created fresh model.\")\n",
    "            return False\n",
    "\n",
    "    def infer(self, encoder_inputs, encoder_inputs_lengths):\n",
    "        inference_feed_dict = {\n",
    "            self.encoder_inputs: encoder_inputs,\n",
    "            self.encoder_inputs_lengths: encoder_inputs_lengths,\n",
    "        }\n",
    "        replies = self.sess.run(self.replies, feed_dict=inference_feed_dict)\n",
    "        return replies\n",
    "\n",
    "    def infer_beam_search(self, encoder_inputs, encoder_inputs_lengths):\n",
    "        inference_feed_dict = {\n",
    "            self.encoder_inputs: encoder_inputs,\n",
    "            self.encoder_inputs_lengths: encoder_inputs_lengths,\n",
    "        }\n",
    "        replies = self.sess.run(self.beam_replies,\n",
    "                                feed_dict=inference_feed_dict)\n",
    "        return replies\n",
    "\n",
    "\n",
    "    def log_prob(self, encoder_inputs, encoder_inputs_lengths, expected_output):\n",
    "        \"\"\"Return sum of log probability of given one specific expected_output for sencoder_inputs.\n",
    "    \n",
    "        Args:\n",
    "            encoder_inputs: [encoder_length, batch_size], eg) tweets\n",
    "            expected_output: [1, decoder_length or less than decoder_length], eg) One reply.\n",
    "    \n",
    "        Returns:\n",
    "            Return log probablity of expected output for given encoder inputs.\n",
    "            eg) sum of log probability of reply \"Good\" when given [\"How are you?\", \"What's up?\"]\n",
    "        \"\"\"\n",
    "        inference_feed_dict = {\n",
    "            self.encoder_inputs: encoder_inputs,\n",
    "            self.encoder_inputs_lengths: encoder_inputs_lengths\n",
    "        }\n",
    "\n",
    "        # Logits\n",
    "        #   logits_value: [batch_size, actual_decoder_length, vocab_size]\n",
    "        logits_batch_value = self.sess.run(self.infer_logits,\n",
    "                                           feed_dict=inference_feed_dict)\n",
    "\n",
    "        sum_p = []\n",
    "        # For each batch: [actual_decoder_length, vocab_size]\n",
    "        for logits in logits_batch_value:\n",
    "            p = 1\n",
    "            # Note that expected_output and logits don't always have same length, but zip takes care of the case.\n",
    "            for word_id, logit in zip(expected_output, logits):\n",
    "                # Apply softmax first, see definition of softmax.\n",
    "                norm = (self._softmax(logit))[word_id]\n",
    "                p *= norm\n",
    "            p = np.log(p)\n",
    "            sum_p.append(p)\n",
    "        ret = np.sum(sum_p) / len(sum_p)\n",
    "        return ret\n",
    "\n",
    "    def reward_ease_of_answering(self, encoder_inputs, encoder_inputs_lengths,\n",
    "                                 expected_outputs):\n",
    "        \"\"\" Return reward for ease of answering. See Deep Reinforcement Learning for Dialogue Generation for more details.\n",
    "    \n",
    "        Args:\n",
    "            encoder_inputs: [encoder_length, batch_size], eg) tweets\n",
    "            expected_outputs: [number of pre-defined dull responses, decoder_length or less than decoder_length]. eg) [[\"I'm\", \"Good\"], [\"fine\"]]\n",
    "    \n",
    "        Returns:\n",
    "            Return reward for ease of answering.\n",
    "            Note that this can be calcualated by calling log_prob function for each dull response,\n",
    "            but this function is more efficient because this calculated the reward at once.\n",
    "        \"\"\"\n",
    "        inference_feed_dict = {\n",
    "            self.encoder_inputs: encoder_inputs,\n",
    "            self.encoder_inputs_lengths: encoder_inputs_lengths\n",
    "        }\n",
    "\n",
    "        # Logits\n",
    "        #   logits_value: [batch_size, actual_decoder_length, vocab_size]\n",
    "        logits_batch_value = self.sess.run(self.infer_logits,\n",
    "                                           feed_dict=inference_feed_dict)\n",
    "\n",
    "        batch_sum_p = []\n",
    "        # For each batch: [actual_decoder_length, vocab_size]\n",
    "        for logits in logits_batch_value:\n",
    "            sum_p = []\n",
    "            for expected_output in expected_outputs:\n",
    "                p = 1\n",
    "                # Note that expected_output and logits don't always have same length, but zip takes care of the case.\n",
    "                for word_id, logit in zip(expected_output, logits):\n",
    "                    # Apply softmax first, see definition of softmax.\n",
    "                    norm = (self._softmax(logit))[word_id]\n",
    "                    p *= norm\n",
    "                p = np.log(p) / len(expected_output)\n",
    "                sum_p.append(p)\n",
    "            one_batch_p = np.sum(sum_p)\n",
    "            batch_sum_p.append(one_batch_p)\n",
    "        ret = np.sum(batch_sum_p) / len(batch_sum_p)\n",
    "        return -ret\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _softmax(x):\n",
    "        return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "    def _build_encoder(self, hparams, scope):\n",
    "        # Encoder\n",
    "        #   encoder_inputs: [encoder_length, batch_size]\n",
    "        #   This is time major where encoder_length comes first instead of batch_size.\n",
    "        #   encoder_inputs_lengths: [batch_size]\n",
    "        encoder_inputs = tf.placeholder(tf.int32,\n",
    "                                        shape=[hparams.encoder_length, None],\n",
    "                                        name=\"encoder_inputs\")\n",
    "        encoder_inputs_lengths = tf.placeholder(tf.int32, shape=[None],\n",
    "                                                name=\"encoder_inputs_lengtsh\")\n",
    "\n",
    "        # Embedding\n",
    "        #   We originally didn't share embbedding between encoder and decoder.\n",
    "        #   But now we share it. It makes much easier to calculate rewards.\n",
    "        #   Matrix for embedding: [vocab_size, embedding_size]\n",
    "        #   Should be shared between training and inference.\n",
    "        with tf.variable_scope(scope):\n",
    "            embedding_encoder = tf.get_variable(\"embedding_encoder\",\n",
    "                                                [hparams.vocab_size,\n",
    "                                                 hparams.embedding_size])\n",
    "\n",
    "        # Look up embedding:\n",
    "        #   encoder_inputs: [encoder_length, batch_size]\n",
    "        #   encoder_emb_inputs: [encoder_length, batch_size, embedding_size]\n",
    "        encoder_emb_inputs = tf.nn.embedding_lookup(embedding_encoder,\n",
    "                                                    encoder_inputs)\n",
    "\n",
    "        # LSTM cell.\n",
    "        with tf.variable_scope(scope):\n",
    "            # Should be shared between training and inference.\n",
    "            encoder_cell = tf.nn.rnn_cell.BasicLSTMCell(hparams.num_units)\n",
    "\n",
    "        # Run Dynamic RNN\n",
    "        #   encoder_outputs: [encoder_length, batch_size, num_units]\n",
    "        #   encoder_state: [batch_size, num_units], this is final state of the cell for each batch.\n",
    "        with tf.variable_scope(scope):\n",
    "            encoder_outputs, encoder_state = tf.nn.dynamic_rnn(encoder_cell,\n",
    "                                                               encoder_emb_inputs,\n",
    "                                                               time_major=True,\n",
    "                                                               dtype=tf.float32,\n",
    "                                                               sequence_length=encoder_inputs_lengths)\n",
    "\n",
    "        return encoder_inputs, encoder_inputs_lengths, encoder_outputs, encoder_state, embedding_encoder\n",
    "\n",
    "    def _build_greedy_inference(self, hparams, embedding_encoder, encoder_state,\n",
    "                                encoder_inputs_lengths, encoder_outputs,\n",
    "                                decoder_cell, projection_layer):\n",
    "        # Greedy decoder\n",
    "        dynamic_batch_size = tf.shape(encoder_inputs_lengths)[0]\n",
    "        inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n",
    "            embedding_encoder,\n",
    "            tf.fill([dynamic_batch_size], tgt_sos_id), tgt_eos_id)\n",
    "\n",
    "        # See https://github.com/tensorflow/tensorflow/issues/11904\n",
    "        if hparams.use_attention:\n",
    "            # Attention\n",
    "            # encoder_outputs is time major, so transopse it to batch major.\n",
    "            # attention_encoder_outputs: [batch_size, encoder_length, num_units]\n",
    "            attention_encoder_outputs = tf.transpose(encoder_outputs, [1, 0, 2])\n",
    "\n",
    "            # Create an attention mechanism\n",
    "            attention_mechanism = tf.contrib.seq2seq.LuongAttention(\n",
    "                hparams.num_units,\n",
    "                attention_encoder_outputs,\n",
    "                memory_sequence_length=encoder_inputs_lengths)\n",
    "\n",
    "            wrapped_decoder_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
    "                decoder_cell, attention_mechanism,\n",
    "                attention_layer_size=hparams.num_units)\n",
    "\n",
    "            initial_state = wrapped_decoder_cell.zero_state(dynamic_batch_size,\n",
    "                                                            tf.float32).clone(\n",
    "                cell_state=encoder_state)\n",
    "        else:\n",
    "            wrapped_decoder_cell = decoder_cell\n",
    "            initial_state = encoder_state\n",
    "\n",
    "        inference_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "            wrapped_decoder_cell, inference_helper, initial_state,\n",
    "            output_layer=projection_layer)\n",
    "\n",
    "        # len(infered_reply) is lte encoder_length, because we are targetting tweeet (140 for each tweet)\n",
    "        # Also by doing this, we can pass the reply to other seq2seq w/o shorten it.\n",
    "        maximum_iterations = hparams.encoder_length\n",
    "\n",
    "        # Dynamic decoding\n",
    "        outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "            inference_decoder, maximum_iterations=maximum_iterations)\n",
    "        replies = outputs.sample_id\n",
    "\n",
    "        # We use infer_logits instead of logits when calculating log_prob, because infer_logits doesn't require decoder_target_lengths input.\n",
    "        infer_logits = outputs.rnn_output\n",
    "        return infer_logits, replies\n",
    "\n",
    "\n",
    "    def _build_beam_search_inference(self, hparams, encoder_inputs_lengths,\n",
    "                                     embedding_encoder, encoder_state,\n",
    "                                     encoder_outputs, decoder_cell,\n",
    "                                     projection_layer):\n",
    "        dynamic_batch_size = tf.shape(encoder_inputs_lengths)[0]\n",
    "        # https://github.com/tensorflow/tensorflow/issues/11904\n",
    "        if hparams.use_attention:\n",
    "            # Attention\n",
    "            # encoder_outputs is time major, so transopse it to batch major.\n",
    "            # attention_encoder_outputs: [batch_size, encoder_length, num_units]\n",
    "            attention_encoder_outputs = tf.transpose(encoder_outputs, [1, 0, 2])\n",
    "\n",
    "            tiled_encoder_outputs = tf.contrib.seq2seq.tile_batch(\n",
    "                attention_encoder_outputs, multiplier=hparams.beam_width)\n",
    "            tiled_encoder_final_state = tf.contrib.seq2seq.tile_batch(\n",
    "                encoder_state, multiplier=hparams.beam_width)\n",
    "            tiled_encoder_inputs_lengths = tf.contrib.seq2seq.tile_batch(\n",
    "                encoder_inputs_lengths, multiplier=hparams.beam_width)\n",
    "\n",
    "            # Create an attention mechanism\n",
    "            attention_mechanism = tf.contrib.seq2seq.LuongAttention(\n",
    "                hparams.num_units, tiled_encoder_outputs,\n",
    "                memory_sequence_length=tiled_encoder_inputs_lengths)\n",
    "\n",
    "            wrapped_decoder_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
    "                decoder_cell, attention_mechanism,\n",
    "                attention_layer_size=hparams.num_units)\n",
    "\n",
    "            decoder_initial_state = wrapped_decoder_cell.zero_state(\n",
    "                dtype=tf.float32,\n",
    "                batch_size=dynamic_batch_size * hparams.beam_width)\n",
    "            decoder_initial_state = decoder_initial_state.clone(\n",
    "                cell_state=tiled_encoder_final_state)\n",
    "\n",
    "            # todo\n",
    "            #    X_seq_len = tf.contrib.seq2seq.tile_batch(X_seq_len, multiplier=BEAM_WIDTH)\n",
    "\n",
    "        else:\n",
    "            wrapped_decoder_cell = decoder_cell\n",
    "            decoder_initial_state = tf.contrib.seq2seq.tile_batch(encoder_state,\n",
    "                                                                  multiplier=hparams.beam_width)\n",
    "\n",
    "        # len(infered_reply) is lte encoder_length, because we are targetting tweeet (140 for each tweet)\n",
    "        # Also by doing this, we can pass the reply to other seq2seq w/o shorten it.\n",
    "        maximum_iterations = hparams.encoder_length\n",
    "\n",
    "        inference_decoder = tf.contrib.seq2seq.BeamSearchDecoder(\n",
    "            cell=wrapped_decoder_cell,\n",
    "            embedding=embedding_encoder,\n",
    "            start_tokens=tf.fill([dynamic_batch_size], tgt_sos_id),\n",
    "            end_token=tgt_eos_id,\n",
    "            initial_state=decoder_initial_state,\n",
    "            beam_width=hparams.beam_width,\n",
    "            output_layer=projection_layer,\n",
    "            length_penalty_weight=0.0)\n",
    "\n",
    "        # Dynamic decoding\n",
    "        beam_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "            inference_decoder, maximum_iterations=maximum_iterations)\n",
    "        beam_replies = beam_outputs.predicted_ids\n",
    "        return beam_replies\n",
    "\n",
    "    def _build_decoder(self, hparams, encoder_inputs_lengths, embedding_encoder,\n",
    "                       encoder_state, encoder_outputs, scope):\n",
    "        # Decoder input\n",
    "        #   decoder_inputs: [decoder_length, batch_size]\n",
    "        #   decoder_target_lengths: [batch_size]\n",
    "        #   This is grand truth target inputs for training.\n",
    "        decoder_inputs = tf.placeholder(tf.int32,\n",
    "                                        shape=[hparams.decoder_length, None],\n",
    "                                        name=\"decoder_inputs\")\n",
    "        decoder_target_lengths = tf.placeholder(tf.int32, shape=[None],\n",
    "                                                name=\"decoder_target_lengths\")\n",
    "\n",
    "        # Look up embedding:\n",
    "        #   decoder_inputs: [decoder_length, batch_size]\n",
    "        #   decoder_emb_inp: [decoder_length, batch_size, embedding_size]\n",
    "        decoder_emb_inputs = tf.nn.embedding_lookup(embedding_encoder,\n",
    "                                                    decoder_inputs)\n",
    "\n",
    "        # https://stackoverflow.com/questions/39573188/output-projection-in-seq2seq-model-tensorflow\n",
    "        # Internally, a neural network operates on dense vectors of some size,\n",
    "        # often 256, 512 or 1024 floats (let's say 512 for here). \n",
    "        # But at the end it needs to predict a word from the vocabulary which is often much larger,\n",
    "        # e.g., 40000 words. Output projection is the final linear layer that converts (projects) from the internal representation to the larger one.\n",
    "        # So, for example, it can consist of a 512 x 40000 parameter matrix and a 40000 parameter for the bias vector.\n",
    "        projection_layer = layers_core.Dense(hparams.vocab_size, use_bias=False)\n",
    "\n",
    "        # We share this between training and inference.\n",
    "        decoder_cell = tf.nn.rnn_cell.BasicLSTMCell(hparams.num_units)\n",
    "\n",
    "        # Greedy Inference graph\n",
    "        infer_logits, replies = self._build_greedy_inference(hparams,\n",
    "                                                             embedding_encoder,\n",
    "                                                             encoder_state,\n",
    "                                                             encoder_inputs_lengths,\n",
    "                                                             encoder_outputs,\n",
    "                                                             decoder_cell,\n",
    "                                                             projection_layer)\n",
    "\n",
    "        # Beam Search Inference graph\n",
    "        beam_replies = self._build_beam_search_inference(hparams,\n",
    "                                                         encoder_inputs_lengths,\n",
    "                                                         embedding_encoder,\n",
    "                                                         encoder_state,\n",
    "                                                         encoder_outputs,\n",
    "                                                         decoder_cell,\n",
    "                                                         projection_layer)\n",
    "\n",
    "        return decoder_inputs, decoder_target_lengths, replies, beam_replies, infer_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "kK5zkPkRkUqa"
   },
   "outputs": [],
   "source": [
    "#@formater:off\n",
    "def clear_saved_model():\n",
    "    ! rm -rf ./saved_model \n",
    "    ! mkdir ./saved_model \n",
    "    ! rm -rf ./saved_model2 \n",
    "    ! mkdir ./saved_model2 \n",
    "    ! rm -rf ./saved_model3 \n",
    "    ! mkdir ./saved_model3\n",
    "#@formatter:on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "DQg8kU-2Dr-q"
   },
   "outputs": [],
   "source": [
    "# Helper functions to test\n",
    "def make_test_training_data(hparams):\n",
    "    train_encoder_inputs = np.empty(\n",
    "        (hparams.encoder_length, hparams.batch_size), dtype=np.int)\n",
    "    train_encoder_inputs_lengths = np.empty((hparams.batch_size), dtype=np.int)\n",
    "    training_target_labels = np.empty(\n",
    "        (hparams.batch_size, hparams.decoder_length), dtype=np.int)\n",
    "    training_decoder_inputs = np.empty(\n",
    "        (hparams.decoder_length, hparams.batch_size), dtype=np.int)\n",
    "\n",
    "    # We keep first tweet to validate inference.\n",
    "    first_tweet = None\n",
    "\n",
    "    for i in range(hparams.batch_size):\n",
    "        # Tweet\n",
    "        tweet = np.random.randint(low=0, high=hparams.vocab_size,\n",
    "                                  size=hparams.encoder_length)\n",
    "        train_encoder_inputs[:, i] = tweet\n",
    "        train_encoder_inputs_lengths[i] = len(tweet)\n",
    "        # Reply\n",
    "        #   Note that low = 2, as 0 and 1 are reserved.\n",
    "        reply = np.random.randint(low=2, high=hparams.vocab_size,\n",
    "                                  size=hparams.decoder_length - 1)\n",
    "\n",
    "        training_target_label = np.concatenate((reply, np.array([tgt_eos_id])))\n",
    "        training_target_labels[i] = training_target_label\n",
    "\n",
    "        training_decoder_input = np.concatenate(([tgt_sos_id], reply))\n",
    "        training_decoder_inputs[:, i] = training_decoder_input\n",
    "\n",
    "        if i == 0:\n",
    "            first_tweet = tweet\n",
    "            info(\"0th tweet={}\".format(tweet), hparams)\n",
    "            info(\"0th reply_with_eos_suffix={}\".format(training_target_label),\n",
    "                 hparams)\n",
    "            info(\"0th reply_with_sos_prefix={}\".format(training_decoder_input),\n",
    "                 hparams)\n",
    "\n",
    "        info(\"Tweets\", hparams)\n",
    "        info(train_encoder_inputs, hparams)\n",
    "        info(\"Replies\", hparams)\n",
    "        info(training_target_labels, hparams)\n",
    "        info(training_decoder_inputs, hparams)\n",
    "    return first_tweet, train_encoder_inputs, train_encoder_inputs_lengths, training_target_labels, training_decoder_inputs\n",
    "\n",
    "def test_training(test_hparams, model, infer_model):\n",
    "    if test_hparams.use_attention:\n",
    "        print(\"==== training model[attention] ====\")\n",
    "    else:\n",
    "        print(\"==== training model ====\")\n",
    "    first_tweet, train_encoder_inputs, train_encoder_inputs_lengths, training_target_labels, training_decoder_inputs = make_test_training_data(\n",
    "        test_hparams)\n",
    "    # Train\n",
    "    x = []\n",
    "    y = []\n",
    "    for i in range(test_hparams.num_train_steps):\n",
    "        loss_value, global_step = model.train(train_encoder_inputs,\n",
    "                                              train_encoder_inputs_lengths,\n",
    "                                              training_target_labels,\n",
    "                                              training_decoder_inputs,\n",
    "                                              np.ones((test_hparams.batch_size),\n",
    "                                                      dtype=int) * test_hparams.decoder_length)\n",
    "        if i % 5 == 0 and test_hparams.debug_verbose:\n",
    "            print('.', end='')\n",
    "\n",
    "        if i % 15 == 0:\n",
    "            model.save()\n",
    "            x.append(global_step)\n",
    "            y.append(loss_value)\n",
    "            if test_hparams.debug_verbose:\n",
    "                print(\"loss={} step={}\".format(loss_value, global_step))\n",
    "\n",
    "    inference_encoder_inputs = np.empty((test_hparams.encoder_length, 1),\n",
    "                                        dtype=np.int)\n",
    "    inference_encoder_inputs_lengths = np.empty((1), dtype=np.int)\n",
    "    for i in range(1):\n",
    "        inference_encoder_inputs[:, i] = first_tweet\n",
    "        inference_encoder_inputs_lengths[i] = len(first_tweet)\n",
    "\n",
    "    # testing \n",
    "    log_prob54 = infer_model.log_prob(inference_encoder_inputs,\n",
    "                                      inference_encoder_inputs_lengths,\n",
    "                                      np.array([5, 4]))\n",
    "    log_prob65 = infer_model.log_prob(inference_encoder_inputs,\n",
    "                                      inference_encoder_inputs_lengths,\n",
    "                                      np.array([6, 5]))\n",
    "    print(\"log_prob for 54\", log_prob54)\n",
    "    print(\"log_prob for 65\", log_prob65)\n",
    "\n",
    "    reward = infer_model.reward_ease_of_answering(inference_encoder_inputs,\n",
    "                                                  inference_encoder_inputs_lengths,\n",
    "                                                  np.array([[5], [6]]))\n",
    "    print(\"reward=\", reward)\n",
    "\n",
    "    if test_hparams.debug_verbose:\n",
    "        print(inference_encoder_inputs)\n",
    "    replies = infer_model.infer(inference_encoder_inputs,\n",
    "                                inference_encoder_inputs_lengths)\n",
    "    print(\"Infered replies\", replies[0])\n",
    "    print(\"Expected replies\", training_target_labels[0])\n",
    "\n",
    "    beam_replies = infer_model.infer_beam_search(inference_encoder_inputs,\n",
    "                                                 inference_encoder_inputs_lengths)\n",
    "    print(\"Infered replies candidate0\", beam_replies[0][:, 0])\n",
    "    print(\"Infered replies candidate1\", beam_replies[0][:, 1])\n",
    "\n",
    "    if test_hparams.debug_verbose:\n",
    "        plt.plot(x, y, label=\"Loss\")\n",
    "        plt.plot()\n",
    "        plt.xlabel(\"Loss\")\n",
    "        plt.ylabel(\"steps\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "def create_train_infer_models(graph, sess, hparams, model_path,\n",
    "                              force_restore=False):\n",
    "    with graph.as_default():\n",
    "        with tf.variable_scope('root'):\n",
    "            model = ChatbotModel(sess, hparams, model_path=model_path)\n",
    "\n",
    "        with tf.variable_scope('root', reuse=True):\n",
    "            infer_model = ChatbotInferenceModel(sess, hparams,\n",
    "                                                model_path=model_path)\n",
    "            restored = model.restore()\n",
    "            if not restored:\n",
    "                if force_restore:\n",
    "                    raise (\"Oops, couldn't restore\")\n",
    "                else:\n",
    "                    sess.run(tf.global_variables_initializer())\n",
    "        return model, infer_model\n",
    "\n",
    "def create_train_infer_models_in_graphs(train_graph, train_sess, infer_graph,\n",
    "                                        infer_sess, hparams, model_path):\n",
    "    with train_graph.as_default():\n",
    "        with tf.variable_scope('root'):\n",
    "            model = ChatbotModel(train_sess, hparams, model_path=model_path)\n",
    "            if not model.restore():\n",
    "                train_sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # note that infer_model is not sharing variable with traning model.\n",
    "    with infer_graph.as_default():\n",
    "        with tf.variable_scope('root'):\n",
    "            infer_model = ChatbotInferenceModel(infer_sess, hparams,\n",
    "                                                model_path=model_path)\n",
    "\n",
    "    return model, infer_model\n",
    "\n",
    "def test_multiple_models_training():\n",
    "    first_tweet, train_encoder_inputs, train_encoder_inputs_length, training_target_labels, training_decoder_inputs = make_test_training_data(\n",
    "        test_hparams)\n",
    "\n",
    "    graph1 = tf.Graph()\n",
    "    sess1 = tf.Session(graph=graph1)\n",
    "    model, infer_model = create_train_infer_models(graph1, sess1, test_hparams,\n",
    "                                                   \"./saved_model/hige\")\n",
    "    test_training(test_hparams, model, infer_model)\n",
    "\n",
    "    graph2 = tf.Graph()\n",
    "    sess2 = tf.Session(graph=graph2)\n",
    "    model2, infer_model2 = create_train_infer_models(graph2, sess2,\n",
    "                                                     test_hparams,\n",
    "                                                     \"./saved_model2/hige\")\n",
    "\n",
    "    test_training(test_hparams, model2, infer_model2)\n",
    "    dull_responses = [[4, 6, 6], [5, 5]]\n",
    "    model2.train_with_reward(infer_model2, infer_model, train_encoder_inputs,\n",
    "                             train_encoder_inputs_length,\n",
    "                             training_target_labels, training_decoder_inputs,\n",
    "                             np.ones((test_hparams.batch_size),\n",
    "                                     dtype=int) * test_hparams.decoder_length,\n",
    "                             dull_responses)\n",
    "\n",
    "    # comment out until https://github.com/tensorflow/tensorflow/issues/10731 is fixed\n",
    "    graph3 = tf.Graph()\n",
    "    sess3 = tf.Session(graph=graph3)\n",
    "#  model3, infer_model3 = create_train_infer_models(graph3, sess3, test_attention_hparams, \"./saved_model3/hige\")    \n",
    "#  test_training(test_attention_hparams, model3, infer_model3)        \n",
    "\n",
    "\n",
    "def test_save_restore_multiple_models_training():\n",
    "    clear_saved_model()\n",
    "\n",
    "    # Fresh model\n",
    "    test_multiple_models_training()\n",
    "\n",
    "    # Saved model\n",
    "    test_multiple_models_training()\n",
    "\n",
    "\n",
    "# todo support multiple models and attention.\n",
    "# todo fix save and restore functions\n",
    "# todo have that pattern for large.\n",
    "# This is a test based on \"Building Training, Eval, and Inference Graphs\" in tensorflow/nmt.\n",
    "def test_distributed_pattern(hparams):\n",
    "    clear_saved_model()\n",
    "\n",
    "    if hparams.use_attention:\n",
    "        print(\"==== test_distributed_pattern[attention] ====\")\n",
    "    else:\n",
    "        print(\"==== test_distributed_pattern ====\")\n",
    "    first_tweet, train_encoder_inputs, train_encoder_inputs_lengths, training_target_labels, training_decoder_inputs = make_test_training_data(\n",
    "        hparams)\n",
    "\n",
    "    train_graph = tf.Graph()\n",
    "    infer_graph = tf.Graph()\n",
    "    train_sess = tf.Session(graph=train_graph)\n",
    "    infer_sess = tf.Session(graph=infer_graph)\n",
    "\n",
    "    model, infer_model = create_train_infer_models_in_graphs(train_graph,\n",
    "                                                             train_sess,\n",
    "                                                             infer_graph,\n",
    "                                                             infer_sess,\n",
    "                                                             hparams,\n",
    "                                                             \"./saved_model/hige\")\n",
    "\n",
    "    for i in range(hparams.num_train_steps):\n",
    "        loss_value, global_step = model.train(train_encoder_inputs,\n",
    "                                              train_encoder_inputs_lengths,\n",
    "                                              training_target_labels,\n",
    "                                              training_decoder_inputs,\n",
    "                                              np.ones((hparams.batch_size),\n",
    "                                                      dtype=int) * hparams.decoder_length)\n",
    "\n",
    "    model.save()\n",
    "\n",
    "    inference_encoder_inputs = np.empty((hparams.encoder_length, 1),\n",
    "                                        dtype=np.int)\n",
    "    inference_encoder_inputs_lengths = np.empty((1), dtype=np.int)\n",
    "\n",
    "    inference_encoder_inputs[:, 0] = first_tweet\n",
    "    inference_encoder_inputs_lengths[0] = len(first_tweet)\n",
    "\n",
    "    infer_model.restore()\n",
    "    replies = infer_model.infer(inference_encoder_inputs,\n",
    "                                inference_encoder_inputs_lengths)\n",
    "    print(\"Infered replies\", replies[0])\n",
    "    print(\"Expected replies\", training_target_labels[0])\n",
    "\n",
    "    beam_replies = infer_model.infer_beam_search(inference_encoder_inputs,\n",
    "                                                 inference_encoder_inputs_lengths)\n",
    "    print(\"Infered replies candidate0\", beam_replies[0][:, 0])\n",
    "    print(\"Infered replies candidate1\", beam_replies[0][:, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "w9BMogyzs_BV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "W_ciBCflZq5o"
   },
   "outputs": [],
   "source": [
    "test_save_restore_multiple_models_training()\n",
    "\n",
    "test_distributed_pattern(test_hparams)\n",
    "\n",
    "# todo\n",
    "# test_distributed_pattern(test_attention_hparams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "kxeWpXO5FThm"
   },
   "outputs": [],
   "source": [
    "def download_file_if_necessary(file_name):\n",
    "    path = \"./{}\".format(file_name)\n",
    "    if os.path.exists(path):\n",
    "        return\n",
    "    print(\"downloading {}...\".format(file_name))\n",
    "    str = read_file_from_drive(file_name)\n",
    "    f = open(path, 'w')\n",
    "    f.write(str)\n",
    "    f.close()\n",
    "    print(\"downloaded\")\n",
    "\n",
    "\n",
    "def read_file_from_drive(file_name):\n",
    "    seq2seq_data_dir_id = \"146ZLldWXLDH0l9WbSUNFKi3nVK_HV0Sz\"\n",
    "    file_list = drive.ListFile({'q': \"'{}' in parents and trashed=false\".format(\n",
    "        seq2seq_data_dir_id)}).GetList()\n",
    "    found = [file for file in file_list if file['title'] == file_name]\n",
    "    if found != []:\n",
    "        downloaded = drive.CreateFile({'id': found[0]['id']})\n",
    "        return downloaded.GetContentString()\n",
    "    else:\n",
    "        raise ValueError(\"file {} not found.\".format(file_name))\n",
    "\n",
    "\n",
    "def read_vocabulary(vocabulary_path):\n",
    "    download_file_if_necessary(vocabulary_path)\n",
    "    rev_vocab = []\n",
    "    rev_vocab.extend(read_file(vocabulary_path).splitlines())\n",
    "    rev_vocab = [line.strip() for line in rev_vocab]\n",
    "    vocab = dict([(x, y) for (y, x) in enumerate(rev_vocab)])\n",
    "    return vocab, rev_vocab\n",
    "\n",
    "def read_file(file_name):\n",
    "    f = open(\"./{}\".format(file_name))\n",
    "    data = f.read()\n",
    "    f.close()\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "W3nUhj80H6BE"
   },
   "outputs": [],
   "source": [
    "def download_model_data_if_necessary(drive, model_path):\n",
    "    if drive is None:\n",
    "        return\n",
    "    model_folder_in_drive = \"18lYBgKvX3AG1zhwJqP1tRYJU688U1N95\"\n",
    "    file_list = drive.ListFile({'q': \"'{}' in parents and trashed=false\".format(\n",
    "        model_folder_in_drive)}).GetList()\n",
    "    if not os.path.exists(model_path):\n",
    "        os.makedirs(model_path)\n",
    "    for file in file_list:\n",
    "        print(\"Downloading \", file['title'], \"...\", end='')\n",
    "        target_file = \"{}/{}\".format(model_path, file['title'])\n",
    "        if not os.path.exists(target_file):\n",
    "            file.GetContentFile(\"{}/{}\".format(model_path, file['title']))\n",
    "        print(\"done\")\n",
    "\n",
    "def create_inference_input(hparams, vocab):\n",
    "    inference_encoder_inputs = np.empty((hparams.encoder_length, 1),\n",
    "                                        dtype=np.int)\n",
    "    inference_encoder_inputs_lengths = np.empty((1), dtype=np.int)\n",
    "    tweet = [\"\", \"\", \"\", \"\", \"www\"]\n",
    "    #  tweet = [\"\"]\n",
    "    tweet_ids = words_to_ids(tweet, vocab)\n",
    "    len_tweet = len(tweet_ids)\n",
    "    tweet_ids.extend([pad_id] * (hparams.encoder_length - len(tweet_ids)))\n",
    "    for i in range(1):\n",
    "        inference_encoder_inputs[:, i] = np.array(tweet_ids, dtype=np.int)\n",
    "        inference_encoder_inputs_lengths[i] = len_tweet\n",
    "    return inference_encoder_inputs, inference_encoder_inputs_lengths\n",
    "\n",
    "def infer(infer_model, inference_encoder_inputs,\n",
    "          inference_encoder_inputs_lengths, global_step, rev_vocab):\n",
    "    replies = infer_model.infer(inference_encoder_inputs,\n",
    "                                inference_encoder_inputs_lengths)\n",
    "    reply = replies[0].tolist()\n",
    "    print(\"  [step-{}] Infered reply\".format(global_step),\n",
    "          ids_to_words(reply, rev_vocab))\n",
    "\n",
    "    beam_replies = infer_model.infer_beam_search(inference_encoder_inputs,\n",
    "                                                 inference_encoder_inputs_lengths)\n",
    "    print(\"  [step-{}] Infered replies candidate0\".format(global_step),\n",
    "          ids_to_words(beam_replies[0][:, 0], rev_vocab))\n",
    "    print(\"  [step-{}] Infered replies candidate1\".format(global_step),\n",
    "          ids_to_words(beam_replies[0][:, 1], rev_vocab))\n",
    "\n",
    "def generic_train_loop(train_feed_data, val_feed_data, vocab, rev_vocab,\n",
    "                       model_path, hparams, generate_models_func,\n",
    "                       inference_hook_func, drive=None, short_loop=False):\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    download_model_data_if_necessary(drive, model_path)\n",
    "\n",
    "    inference_encoder_inputs, inference_encoder_inputs_lengths = create_inference_input(\n",
    "        hparams, vocab)\n",
    "\n",
    "    graph, sess, model, infer_model = generate_models_func(hparams, model_path)\n",
    "\n",
    "    with graph.as_default():\n",
    "        train_data_iterator = train_feed_data.make_one_shot_iterator()\n",
    "        val_data_iterator = None\n",
    "        if val_feed_data is not None:\n",
    "            val_data_iterator = val_feed_data.make_one_shot_iterator()\n",
    "\n",
    "        last_saved_time = datetime.datetime.now()\n",
    "        for i in range(hparams.num_train_steps):\n",
    "            train_data = sess.run(train_data_iterator.get_next())\n",
    "            loss_value, global_step = model.train(train_data[0], train_data[1],\n",
    "                                                  train_data[2], train_data[3],\n",
    "                                                  train_data[4])\n",
    "\n",
    "            if short_loop and i == 2:\n",
    "                x.append(global_step)\n",
    "                y.append(loss_value)\n",
    "                print(\"loss={:.2f}\".format(loss_value))\n",
    "                model.save()\n",
    "                inference_hook_func(infer_model)\n",
    "                infer(infer_model, inference_encoder_inputs,\n",
    "                      inference_encoder_inputs_lengths, global_step, rev_vocab)\n",
    "                if val_data_iterator is not None:\n",
    "                    val_data = sess.run(val_data_iterator.get_next())\n",
    "                    val_loss = model.batch_loss(val_data[0], val_data[1],\n",
    "                                                val_data[2], val_data[3],\n",
    "                                                val_data[4])\n",
    "                    print(\"validation loss\", val_loss)\n",
    "\n",
    "                break\n",
    "            elif i != 0 and i % 15 == 0:\n",
    "                print(\"loss={:.2f}\".format(loss_value))\n",
    "                model.save()\n",
    "                inference_hook_func(infer_model)\n",
    "                infer(infer_model, inference_encoder_inputs,\n",
    "                      inference_encoder_inputs_lengths, global_step, rev_vocab)\n",
    "                if val_data_iterator is not None:\n",
    "                    val_data = sess.run(val_data_iterator.get_next())\n",
    "                    val_loss = model.batch_loss(val_data[0], val_data[1],\n",
    "                                                val_data[2], val_data[3],\n",
    "                                                val_data[4])\n",
    "                    print(\"validation loss\", val_loss)\n",
    "                    x.append(global_step)\n",
    "                    y.append(val_loss)\n",
    "\n",
    "            else:\n",
    "                print('.', end='')\n",
    "            now = datetime.datetime.now()\n",
    "            if (\n",
    "                        now - last_saved_time).total_seconds() > 3600 and drive is not None:\n",
    "                drive = make_drive()\n",
    "                last_saved_time = datetime.datetime.now()\n",
    "                save_model_in_drive(drive, model_path)\n",
    "\n",
    "            if i != 0 and i % 100 == 0:\n",
    "                plt.plot(x, y, label=\"Validation Loss\")\n",
    "                plt.plot()\n",
    "                plt.ylabel(\"Validation Loss\")\n",
    "                plt.xlabel(\"steps\")\n",
    "                plt.legend()\n",
    "                plt.show()\n",
    "\n",
    "def train_loop(train_feed_data, val_feed_data, vocab, rev_vocab, model_path,\n",
    "               hparams, drive=None, short_loop=False):\n",
    "    def inference_hook(infer_model):\n",
    "        None\n",
    "    def generate_models(hparams, model_path):\n",
    "        graph = tf.Graph()\n",
    "        sess = tf.Session(graph=graph)\n",
    "        model, infer_model = create_train_infer_models(graph, sess, hparams,\n",
    "                                                       model_path)\n",
    "        return graph, sess, model, infer_model\n",
    "\n",
    "    generic_train_loop(train_feed_data, val_feed_data, vocab, rev_vocab,\n",
    "                       model_path, hparams, generate_models, inference_hook,\n",
    "                       drive, short_loop)\n",
    "\n",
    "def train_loop_distributed_pattern(train_feed_data, val_feed_data, vocab,\n",
    "                                   rev_vocab, model_path, hparams, drive=None,\n",
    "                                   short_loop=False):\n",
    "    def inference_hook(infer_model):\n",
    "        # always restore from file, because it's in different graph.\n",
    "        restored = infer_model.restore()\n",
    "        assert (restored)\n",
    "    def generate_models(hparams, model_path):\n",
    "        train_graph = tf.Graph()\n",
    "        infer_graph = tf.Graph()\n",
    "        train_sess = tf.Session(graph=train_graph)\n",
    "        infer_sess = tf.Session(graph=infer_graph)\n",
    "        model, infer_model = create_train_infer_models_in_graphs(train_graph,\n",
    "                                                                 train_sess,\n",
    "                                                                 infer_graph,\n",
    "                                                                 infer_sess,\n",
    "                                                                 hparams,\n",
    "                                                                 model_path)\n",
    "        return train_graph, train_sess, model, infer_model\n",
    "\n",
    "    generic_train_loop(train_feed_data, val_feed_data, vocab, rev_vocab,\n",
    "                       model_path, hparams, generate_models, inference_hook,\n",
    "                       drive, short_loop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "1ouPJTAVZEM6"
   },
   "outputs": [],
   "source": [
    "def train_rl_loop_distributed_pattern(train_feed_data, vocab, rev_vocab,\n",
    "                                      src_model_path, dst_model_path, hparams,\n",
    "                                      drive=None, short_loop=False):\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    dull_responses = map(lambda x: words_to_ids(x, vocab), [[\"\"], [\"\"]])\n",
    "    print(dull_responses)\n",
    "\n",
    "    download_model_data_if_necessary(drive, src_model_path)\n",
    "\n",
    "    inference_encoder_inputs, inference_encoder_inputs_lengths = create_inference_input(\n",
    "        hparams, vocab)\n",
    "\n",
    "    seq2seq_graph = tf.Graph()\n",
    "    rl_graph = tf.Graph()\n",
    "\n",
    "    seq2seq_sess = tf.Session(graph=seq2seq_graph)\n",
    "    rl_sess = tf.Session(graph=rl_graph)\n",
    "\n",
    "    with seq2seq_graph.as_default():\n",
    "        with tf.variable_scope('root'):\n",
    "            seq2seq_infer_model = ChatbotInferenceModel(seq2seq_sess, hparams,\n",
    "                                                        model_path=src_model_path)\n",
    "            restored = seq2seq_infer_model.restore()\n",
    "            assert (restored)\n",
    "\n",
    "    model, infer_model = create_train_infer_models(rl_graph, rl_sess, hparams,\n",
    "                                                   src_model_path,\n",
    "                                                   force_restore=True)\n",
    "    with rl_graph.as_default():\n",
    "        train_data_iterator = train_feed_data.make_one_shot_iterator()\n",
    "\n",
    "    last_saved_time = datetime.datetime.now()\n",
    "    for i in range(hparams.num_train_steps):\n",
    "        train_data = rl_sess.run(train_data_iterator.get_next())\n",
    "        loss_value, global_step = model.train_with_reward(infer_model,\n",
    "                                                          seq2seq_infer_model,\n",
    "                                                          train_data[0],\n",
    "                                                          train_data[1],\n",
    "                                                          train_data[2],\n",
    "                                                          train_data[3],\n",
    "                                                          train_data[4],\n",
    "                                                          dull_responses)\n",
    "        print('.', end='')\n",
    "\n",
    "        if short_loop and i == 2:\n",
    "            x.append(global_step)\n",
    "            y.append(loss_value)\n",
    "            print(\"loss={:.2f}\".format(loss_value))\n",
    "            model.save()\n",
    "            infer(infer_model, inference_encoder_inputs,\n",
    "                  inference_encoder_inputs_lengths, global_step, rev_vocab)\n",
    "            break\n",
    "        elif i != 0 and i % 15 == 0:\n",
    "            x.append(global_step)\n",
    "            y.append(loss_value)\n",
    "            print(\"loss={:.2f}\".format(loss_value))\n",
    "            model.save(dst_model_path)\n",
    "            infer(infer_model, inference_encoder_inputs,\n",
    "                  inference_encoder_inputs_lengths, global_step, rev_vocab)\n",
    "\n",
    "            now = datetime.datetime.now()\n",
    "            if (\n",
    "                now - last_saved_time).total_seconds() > 7200 and drive is not None:\n",
    "                save_model_in_drive(drive, dst_model_path, is_rl=True)\n",
    "                last_saved_time = datetime.datetime.now()\n",
    "\n",
    "            if i != 0 and i % 100 == 0:\n",
    "                plt.plot(x, y, label=\"Loss\")\n",
    "                plt.plot()\n",
    "                plt.ylabel(\"Loss\")\n",
    "                plt.xlabel(\"steps\")\n",
    "                plt.legend()\n",
    "                plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "k1R4Q230eV-D"
   },
   "outputs": [],
   "source": [
    "def create_encoder_idx_padded(src_file, dst_file, dst_length_file,\n",
    "                              max_line_len):\n",
    "    with open(src_file) as fin, open(dst_file, \"w\") as fout, open(\n",
    "            dst_length_file, \"w\") as flen:\n",
    "        line = fin.readline()\n",
    "        while line:\n",
    "            ids = [int(x) for x in line.split()]\n",
    "            if len(ids) > max_line_len:\n",
    "                ids = ids[:max_line_len]\n",
    "            flen.write(str(len(ids)))\n",
    "            flen.write(\"\\n\")\n",
    "            if len(ids) < max_line_len:\n",
    "                ids.extend([pad_id] * (max_line_len - len(ids)))\n",
    "            ids = [str(x) for x in ids]\n",
    "            fout.write(\" \".join(ids))\n",
    "            fout.write(\"\\n\")\n",
    "            line = fin.readline()\n",
    "\n",
    "# read decoder_idx file and append eos at the end of idx list.\n",
    "def create_decoder_idx_eos(src_file, dst_file, max_line_len):\n",
    "    with open(src_file) as fin, open(dst_file, \"w\") as fout:\n",
    "        line = fin.readline()\n",
    "        while line:\n",
    "            ids = [int(x) for x in line.split()]\n",
    "            if len(ids) > max_line_len - 1:\n",
    "                ids = ids[:max_line_len - 1]\n",
    "            ids.append(tgt_eos_id)\n",
    "            if len(ids) < max_line_len:\n",
    "                ids.extend([pad_id] * (max_line_len - len(ids)))\n",
    "            ids = [str(x) for x in ids]\n",
    "            fout.write(\" \".join(ids))\n",
    "            fout.write(\"\\n\")\n",
    "            line = fin.readline()\n",
    "\n",
    "# read decoder_idx file and put sos at the begining of the idx list.\n",
    "# also wrte out length of index list.\n",
    "def create_decoder_idx_sos(src_file, dst_file, dst_length_file, max_line_len):\n",
    "    with open(src_file) as fin, open(dst_file, \"w\") as fout, open(\n",
    "            dst_length_file, \"w\") as flen:\n",
    "        line = fin.readline()\n",
    "        while line:\n",
    "            ids = [tgt_sos_id]\n",
    "            ids.extend([int(x) for x in line.split()])\n",
    "            if len(ids) > max_line_len:\n",
    "                ids = ids[:max_line_len]\n",
    "            flen.write(str(len(ids)))\n",
    "            flen.write(\"\\n\")\n",
    "            if len(ids) < max_line_len:\n",
    "                ids.extend([pad_id] * (max_line_len - len(ids)))\n",
    "            ids = [str(x) for x in ids]\n",
    "            fout.write(\" \".join(ids))\n",
    "            fout.write(\"\\n\")\n",
    "            line = fin.readline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "I0-o2wg0gzvA"
   },
   "outputs": [],
   "source": [
    "def split_to_int_values(x):\n",
    "    return tf.string_to_number(tf.string_split([x]).values, tf.int32)\n",
    "\n",
    "def textLineSplitDataset(filename):\n",
    "    return tf.data.TextLineDataset(\"./{}\".format(filename)).map(\n",
    "        split_to_int_values)\n",
    "\n",
    "def words_to_ids(words, vocab):\n",
    "    ids = []\n",
    "    for word in words:\n",
    "        if word in vocab:\n",
    "            ids.append(vocab[word])\n",
    "        else:\n",
    "            ids.append(unk_id)\n",
    "    return ids\n",
    "\n",
    "def ids_to_words(ids, rev_vocab):\n",
    "    words = \"\"\n",
    "    for id in ids:\n",
    "        words += rev_vocab[id]\n",
    "    return words\n",
    "\n",
    "def make_train_dataset(tweets_enc_idx_file, tweets_dec_idx_file, vocab_file,\n",
    "                       hparams):\n",
    "    sess = tf.Session()\n",
    "    # todo: skip if already exists\n",
    "    tweets_enc_idx_padded_file = \"{}.padded\".format(tweets_enc_idx_file)\n",
    "    tweets_enc_idx_len_file = \"{}.len\".format(tweets_enc_idx_file)\n",
    "\n",
    "    tweets_dec_idx_eos_file = \"{}.eos\".format(tweets_dec_idx_file)\n",
    "    tweets_dec_idx_sos_file = \"{}.sos\".format(tweets_dec_idx_file)\n",
    "    tweets_dec_idx_len_file = \"{}.len\".format(tweets_dec_idx_file)\n",
    "\n",
    "    download_file_if_necessary(tweets_enc_idx_file)\n",
    "    create_encoder_idx_padded(tweets_enc_idx_file, tweets_enc_idx_padded_file,\n",
    "                              tweets_enc_idx_len_file, hparams.encoder_length)\n",
    "    print(tweets_enc_idx_padded_file, \" created\")\n",
    "\n",
    "    download_file_if_necessary(tweets_dec_idx_file)\n",
    "    create_decoder_idx_eos(tweets_dec_idx_file, tweets_dec_idx_eos_file,\n",
    "                           hparams.decoder_length)\n",
    "    print(tweets_dec_idx_eos_file, \" created\")\n",
    "\n",
    "    create_decoder_idx_sos(tweets_dec_idx_file, tweets_dec_idx_sos_file,\n",
    "                           tweets_dec_idx_len_file, hparams.decoder_length)\n",
    "    print(tweets_dec_idx_sos_file, \" created\")\n",
    "\n",
    "    tweets_dataset = textLineSplitDataset(tweets_enc_idx_padded_file)\n",
    "    tweets_lengths_dataset = tf.data.TextLineDataset(tweets_enc_idx_len_file)\n",
    "\n",
    "    replies_sos_dataset = textLineSplitDataset(tweets_dec_idx_sos_file)\n",
    "    replies_eos_dataset = textLineSplitDataset(tweets_dec_idx_eos_file)\n",
    "    replies_sos_lengths_dataset = tf.data.TextLineDataset(\n",
    "        tweets_dec_idx_len_file)\n",
    "\n",
    "    tweets_transposed = tweets_dataset.batch(hparams.batch_size).map(\n",
    "        lambda x: tf.transpose(x))\n",
    "    tweets_lengths = tweets_lengths_dataset.batch(hparams.batch_size)\n",
    "\n",
    "    replies_with_eos_suffix = replies_eos_dataset.batch(hparams.batch_size)\n",
    "    replies_with_sos_prefix = replies_sos_dataset.batch(hparams.batch_size).map(\n",
    "        lambda x: tf.transpose(x))\n",
    "    replies_with_sos_suffix_lengths = replies_sos_lengths_dataset.batch(\n",
    "        hparams.batch_size)\n",
    "\n",
    "    info(\"tweets_example: {}\".format(\n",
    "        sess.run(tweets_transposed.make_one_shot_iterator().get_next())),\n",
    "         hparams)\n",
    "    info(\"tweets_lengths_example:{}\".format(\n",
    "        sess.run(tweets_lengths.make_one_shot_iterator().get_next())), hparams)\n",
    "    info(\"reply_with_eos_suffix_example:{}\".format(\n",
    "        sess.run(replies_with_eos_suffix.make_one_shot_iterator().get_next())),\n",
    "         hparams)\n",
    "    info(\"reply_with_sos_prefix_example:{}\".format(\n",
    "        sess.run(replies_with_sos_prefix.make_one_shot_iterator().get_next())),\n",
    "         hparams)\n",
    "    info(\"reply_with_sos_lengths_prefix_example:{}\".format(sess.run(\n",
    "        replies_with_sos_suffix_lengths.make_one_shot_iterator().get_next())),\n",
    "         hparams)\n",
    "\n",
    "    # Merge all using zip\n",
    "    train_feed_data = tf.data.Dataset.zip((tweets_transposed, tweets_lengths,\n",
    "                                           replies_with_eos_suffix,\n",
    "                                           replies_with_sos_prefix,\n",
    "                                           replies_with_sos_suffix_lengths))\n",
    "    train_feed_data_value = sess.run(\n",
    "        train_feed_data.make_one_shot_iterator().get_next())\n",
    "    info(\"train_feed_data={}\".format(train_feed_data_value[0]), hparams)\n",
    "    info(\"train_feed_data={}\".format(train_feed_data_value[1]), hparams)\n",
    "    info(\"train_feed_data={}\".format(train_feed_data_value[2]), hparams)\n",
    "    info(\"train_feed_data={}\".format(train_feed_data_value[3]), hparams)\n",
    "\n",
    "    print(\"Dataset created\")\n",
    "\n",
    "    vocab, rev_vocab = read_vocabulary(vocab_file)\n",
    "    return train_feed_data, vocab, rev_vocab\n",
    "\n",
    "def save_model_in_drive(drive, model_path, is_rl=False):\n",
    "    normal_model_folder_in_drive = \"18lYBgKvX3AG1zhwJqP1tRYJU688U1N95\"\n",
    "    rl_model_folder_in_drive = \"1pHnOuT_7JjD1TS8VQ4KN9oUiblBIABXJ\"\n",
    "    model_folder_in_drive = normal_model_folder_in_drive\n",
    "    if is_rl:\n",
    "        model_folder_in_drive = rl_model_folder_in_drive\n",
    "    file_list = drive.ListFile({'q': \"'{}' in parents and trashed=false\".format(\n",
    "        model_folder_in_drive)}).GetList()\n",
    "    for model_file in os.listdir(model_path):\n",
    "        file = drive.CreateFile({'title': model_file, \"parents\": [\n",
    "            {\"kind\": \"drive#fileLink\", \"id\": model_folder_in_drive}]})\n",
    "        file.SetContentFile(\"{}/{}\".format(model_path, model_file))\n",
    "        print(\"Uploading \", model_file, \"...\", end=\"\")\n",
    "        file.Upload()\n",
    "        print(\"done\")\n",
    "    for file in file_list:\n",
    "        f = drive.CreateFile({'id': file['id']})\n",
    "        f.Delete()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "H0v2MWg4Ugcj"
   },
   "outputs": [],
   "source": [
    "if colab():\n",
    "    !pip install pydrive\n",
    "    from pydrive.auth import GoogleAuth\n",
    "    from pydrive.drive import GoogleDrive\n",
    "    from google.colab import auth\n",
    "    from oauth2client.client import GoogleCredentials\n",
    "\n",
    "    def make_drive():\n",
    "        # 1. Authenticate and create the PyDrive client.\n",
    "        auth.authenticate_user()\n",
    "        gauth = GoogleAuth()\n",
    "        gauth.credentials = GoogleCredentials.get_application_default()\n",
    "        drive = GoogleDrive(gauth)\n",
    "        return drive\n",
    "    drive = make_drive()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "hwKcEFIjU9uI"
   },
   "outputs": [],
   "source": [
    "download_file_if_necessary(\"tweets_train_dec_idx.txt\")\n",
    "download_file_if_necessary(\"tweets_train_enc_idx.txt\")\n",
    "\n",
    "create_decoder_idx_eos(\"./tweets_train_dec_idx.txt\",\n",
    "                       \"./tweets_train_dec_eos_idx.txt\",\n",
    "                       real_hparams.decoder_length)\n",
    "create_decoder_idx_sos(\"./tweets_train_dec_idx.txt\",\n",
    "                       \"./tweets_train_dec_sos_idx.txt\",\n",
    "                       \"./tweets_train_dec_sos_idx_len.txt\",\n",
    "                       real_hparams.decoder_length)\n",
    "create_encoder_idx_padded(\"./tweets_train_enc_idx.txt\",\n",
    "                          \"./tweets_train_enc_idx_padded.txt\",\n",
    "                          \"./tweets_train_enc_idx_len.txt\",\n",
    "                          real_hparams.encoder_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "lS-jn6FN0mFx"
   },
   "outputs": [],
   "source": [
    "def print_header(name):\n",
    "    print(\"==========   {}   ========\".format(name))\n",
    "\n",
    "def test_small_train_loops():\n",
    "    print_header(\"make train dataset\")\n",
    "    train_feed_data, vocab, rev_vocab = make_train_dataset(\n",
    "        \"tweets_train_enc_idx.txt\", \"tweets_train_dec_idx.txt\", \"vocab.txt\",\n",
    "        real_hparams)\n",
    "\n",
    "    print_header(\"train_loop\")\n",
    "    train_loop(train_feed_data.repeat(10), None, vocab, rev_vocab,\n",
    "               \"./saved_model/real\", real_hparams, short_loop=True)\n",
    "\n",
    "    print_header(\"train_loop_distributed_pattern\")\n",
    "    train_loop_distributed_pattern(train_feed_data.repeat(10), None, vocab,\n",
    "                                   rev_vocab, \"./saved_model/real\",\n",
    "                                   real_hparams, short_loop=True)\n",
    "  \n",
    "    print_header(\"train_rl_loop_distributed_pattern\")\n",
    "    train_rl_loop_distributed_pattern(train_feed_data.repeat(10), vocab,\n",
    "                                      rev_vocab, \"./saved_model/real\",\n",
    "                                      \"./saved_model/real_rl\", real_hparams,\n",
    "                                      short_loop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "qHz9nfbXaUwZ"
   },
   "outputs": [],
   "source": [
    "test_small_train_loops()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "5DNvNFIOUzfU"
   },
   "outputs": [],
   "source": [
    "should_run_large_train = False\n",
    "should_run_large_train_distributed = False\n",
    "should_run_large_train_rl = False\n",
    "\n",
    "if should_run_large_train or should_run_large_train_distributed or should_run_large_train_rl:\n",
    "    train_feed_data, vocab, rev_vocab = make_train_dataset(\n",
    "        \"tweets_train_enc_idx_large.txt\", \"tweets_train_dec_idx_large.txt\",\n",
    "        \"vocab_large.txt\", large_hparams)\n",
    "    val_feed_data, vocab, rev_vocab = make_train_dataset(\n",
    "        \"tweets_val_enc_idx_large.txt\", \"tweets_val_dec_idx_large.txt\",\n",
    "        \"vocab_large.txt\", large_hparams)\n",
    "    val_feed_data = val_feed_data.shuffle(4096).repeat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "tPFK8YzV1s_P"
   },
   "outputs": [],
   "source": [
    "if should_run_large_train:\n",
    "    train_loop(train_feed_data, vocab, rev_vocab, \"./saved_model/large\",\n",
    "               large_hparams, drive=drive)\n",
    "if should_run_large_train_distributed:\n",
    "    train_loop_distributed_pattern(train_feed_data, val_feed_data, vocab,\n",
    "                                   rev_vocab, \"./saved_model/large\",\n",
    "                                   large_hparams, drive=drive)\n",
    "if should_run_large_train_rl:\n",
    "    train_rl_loop_distributed_pattern(train_feed_data, vocab, rev_vocab,\n",
    "                                      \"./saved_model/large\",\n",
    "                                      \"./saved_model/large_rl\", large_hparams,\n",
    "                                      drive=drive)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "1b6tvWa-rUDT"
   },
   "outputs": [],
   "source": [
    "#drive=make_drive()\n",
    "#save_model_in_drive(drive, \"./saved_model/large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "FBQFWP5U_Dz7"
   },
   "outputs": [],
   "source": [
    "!ls saved_model/large\n",
    "#!rm -rf  saved_model/large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "S86EmWR8J3UI"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "seq2seq.ipynb",
   "provenance": [
    {
     "file_id": "1Os9oWOWM-thM7tlXMNYfNlAC_L-l7arY",
     "timestamp": 1515554387158
    }
   ],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
