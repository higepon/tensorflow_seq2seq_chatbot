{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seq2seq_backwrd_prep.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "HWOxK9T5I8sb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Chatbot based on Seq2Seq Beam Search + Attention + Reinforcment Learning(Experimental)\n",
        "- Tensorflow 1.4.0+ is required.\n",
        "- This is based on [NMT Tutorial](https://github.com/tensorflow/nmt).\n",
        "- Experiment [notes](https://github.com/higepon/tensorflow_seq2seq_chatbot/wiki).\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "kK1r053SI2f9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Special commands should be located here.\n",
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "!apt-get -qq install -y mecab libmecab-dev mecab-ipadic mecab-ipadic-utf8\n",
        "\n",
        "!pip -q install git+https://github.com/mrahtz/easy-tf-log#egg=easy-tf-log[tf]\n",
        "!pip install pushbullet.py\n",
        "!pip install tweepy pyyaml\n",
        "!pip install mecab-python3\n",
        "\n",
        "def auth_google_drive():\n",
        "  # Generate creds for the Drive FUSE library.\n",
        "  if not os.path.exists('drive'):\n",
        "    from oauth2client.client import GoogleCredentials\n",
        "    creds = GoogleCredentials.get_application_default()\n",
        "    import getpass\n",
        "    !google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "    vcode = getpass.getpass()\n",
        "    !echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}  \n",
        "\n",
        "def mount_google_drive():\n",
        "  if not os.path.exists('drive'):\n",
        "    os.makedirs('drive', exist_ok=True)\n",
        "    !google-drive-ocamlfuse drive \n",
        "    \n",
        "def kill_docker():\n",
        "  !kill -9 -1  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "90XCqkUfbnUZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "response = urllib.request.urlopen(\"https://raw.githubusercontent.com/yaroslavvb/memory_util/master/memory_util.py\")\n",
        "open(\"memory_util.py\", \"wb\").write(response.read())\n",
        "import memory_util"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WE9v1UerJMRo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import copy as copy\n",
        "import datetime\n",
        "import hashlib\n",
        "import json\n",
        "import os\n",
        "import os.path\n",
        "import random\n",
        "import re\n",
        "import shutil\n",
        "from enum import Enum, auto\n",
        "\n",
        "import MeCab\n",
        "import easy_tf_log\n",
        "import matplotlib.pyplot as plt\n",
        "import random as random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tweepy\n",
        "import yaml\n",
        "from easy_tf_log import tflog\n",
        "from google.colab import auth\n",
        "from google.colab import files\n",
        "from pushbullet import Pushbullet\n",
        "from tensorflow.python.layers import core as layers_core\n",
        "from tensorflow.python.platform import gfile\n",
        "\n",
        "# Generate auth tokens for Colab\n",
        "auth.authenticate_user()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OoMe73Z51zNk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#kill_docker()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KapXwLNkJtH-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def client_id():\n",
        "    clients = {'dfc1d5b22ba03430800179d23e522f6f': 'client1',\n",
        "               'f8e857a2d792038820ebb2ae8d803f7c': 'client2',\n",
        "               '7628f983785173edabbde501ef8f781d': 'client3'}\n",
        "    with open('/content/datalab/adc.json') as json_data:\n",
        "        d = json.load(json_data)\n",
        "        email = d['id_token']['email'].encode('utf-8')\n",
        "        return clients[hashlib.md5(email).hexdigest()]\n",
        "\n",
        "\n",
        "print(client_id())\n",
        "current_client_id = client_id()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mM1uEwbYJPJK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "auth_google_drive()\n",
        "mount_google_drive()\n",
        "\n",
        "drive_path = 'drive/seq2seq_data'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bPLkjCHPSyGx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Mode(Enum):\n",
        "    Test = auto()\n",
        "    TrainSeq2Seq = auto()\n",
        "    TrainSeq2SeqSwapped = auto()\n",
        "    TrainRL = auto()\n",
        "    TweetBot = auto()\n",
        "    \n",
        "\n",
        "\n",
        "mode = Mode.Test\n",
        "#mode = Mode.TrainSeq2Seq\n",
        "#mode = Mode.TrainSeq2SeqSwapped\n",
        "#mode = Mode.TrainRL\n",
        "#mode = Mode.TweetBot\n",
        "\n",
        "\n",
        "class Shell:\n",
        "    @staticmethod\n",
        "    def download_file_if_necessary(file_name):\n",
        "        if os.path.exists(file_name):\n",
        "            return\n",
        "        print(\"downloading {}...\".format(file_name))\n",
        "        shutil.copy2(os.path.join(drive_path, file_name), file_name)\n",
        "        print(\"downloaded\")\n",
        "\n",
        "    @staticmethod\n",
        "    def download_model_data_if_necessary(model_path):\n",
        "        if not os.path.exists(model_path):\n",
        "            os.makedirs(model_path)\n",
        "        print(\"Downloading model files...\")\n",
        "        src_dir = os.path.join(drive_path, model_path)\n",
        "        Shell.copy_all_files(src_dir, model_path)\n",
        "        print(\"done\")\n",
        "\n",
        "    @staticmethod\n",
        "    def copy_all_files(src_dir, dst_dir):\n",
        "        if os.path.exists(src_dir):\n",
        "            for f in os.listdir(src_dir):\n",
        "                shutil.copy2(os.path.join(src_dir, f), os.path.join(dst_dir, f))\n",
        "\n",
        "    @staticmethod\n",
        "    def remove_all_files(target_dir):\n",
        "        for f in Shell.listdir(target_dir):\n",
        "            os.remove(f)\n",
        "\n",
        "    @staticmethod\n",
        "    def remove_matched_files(target_dir, pattern):\n",
        "        for f in Shell.listdir(target_dir):\n",
        "            if re.match(pattern, f):\n",
        "                os.remove(f)\n",
        "\n",
        "    @staticmethod\n",
        "    def download_logs(path):\n",
        "        for f in Shell.listdir(path):\n",
        "            if re.match('.*events', f):\n",
        "                files.download(f)\n",
        "\n",
        "    @staticmethod\n",
        "    def remove_saved_model(hparams):\n",
        "        os.makedirs(hparams.model_path, exist_ok=True)\n",
        "        Shell.remove_all_files(hparams.model_path)\n",
        "        os.makedirs(os.path.join(drive_path, hparams.model_path), exist_ok=True)\n",
        "        Shell.remove_all_files(os.path.join(drive_path, hparams.model_path))\n",
        "\n",
        "    @staticmethod\n",
        "    def copy_saved_model(src_hparams, dst_hparams):\n",
        "        Shell.copy_all_files(src_hparams.model_path, dst_hparams.model_path)\n",
        "        # rm tf.logs from source so that it wouldn't be mixed in dest tf.logs.\n",
        "        Shell.remove_matched_files(dst_hparams.model_path, \".*events.*\")\n",
        "\n",
        "    @staticmethod\n",
        "    def listdir(target_dir):\n",
        "        for dir_path, _, file_names in os.walk(target_dir):\n",
        "            for f in file_names:\n",
        "                yield os.path.abspath(os.path.join(dir_path, f))\n",
        "\n",
        "    @staticmethod\n",
        "    def list_model_file(path):\n",
        "        f = open('{}/checkpoint'.format(path))\n",
        "        text = f.read()\n",
        "        f.close()\n",
        "        print(text)\n",
        "        m = re.match(r\".*ChatbotModel\\-(\\d+)\", text)\n",
        "        model_name = m.group(1)\n",
        "        files = [\"checkpoint\"]\n",
        "        files.extend([x for x in os.listdir(path) if\n",
        "                      re.search(model_name, x) or re.search('events.out', x)])\n",
        "        return files\n",
        "\n",
        "    @staticmethod\n",
        "    def save_model_in_drive(model_path):\n",
        "        path = os.path.join(drive_path, model_path)\n",
        "        os.makedirs(path, exist_ok=True)\n",
        "        Shell.remove_all_files(os.path.join(drive_path, model_path))\n",
        "        print(\"Saving model in Google Drive...\")\n",
        "        for file in Shell.list_model_file(model_path):\n",
        "            print(\"Saving \", file)\n",
        "            shutil.copy2(os.path.join(model_path, file),\n",
        "                         os.path.join(drive_path, model_path, file))\n",
        "        print(\"done\")\n",
        "\n",
        "\n",
        "config_path = 'config.yml'\n",
        "Shell.download_file_if_necessary(config_path)\n",
        "f = open(config_path, 'rt')\n",
        "push_key = yaml.load(f)['pushbullet']['api_key']\n",
        "\n",
        "pb = Pushbullet(push_key)\n",
        "\n",
        "# Note for myself.\n",
        "# You've summarized Seq2Seq\n",
        "# at http://d.hatena.ne.jp/higepon/20171210/1512887715.\n",
        "\n",
        "# If you see following error, it means your max(len(tweets of training set)) <  decoder_length.\n",
        "# This should be a bug somewhere in build_decoder, but couldn't find one yet.\n",
        "# You can workaround by setting hparams.decoder_length=max len of tweet in training set.\n",
        "# InvalidArgumentError: logits and labels must have the same first dimension, got logits shape [48,50] and labels shape [54]\n",
        "#\t [[Node: root/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits = SparseSoftmaxCrossEntropyWithLogits[T=DT_FLOAT, Tlabels=DT_INT32, \n",
        "\n",
        "print(tf.__version__)\n",
        "\n",
        "def info(message, hparams):\n",
        "    if hparams.debug_verbose:\n",
        "        print(message)\n",
        "\n",
        "\n",
        "def has_gpu0():\n",
        "    return tf.test.gpu_device_name() == \"/device:GPU:0\"\n",
        "\n",
        "class ModelDirectory(Enum):\n",
        "    tweet_large = 'model/tweet_large'\n",
        "    tweet_large_rl = 'model/tweet_large_rl'\n",
        "    tweet_large_swapped = 'model/tweet_large_swapped'\n",
        "    tweet_small = 'model/tweet_small'\n",
        "    tweet_small_swapped = 'model/tweet_small_swapped'\n",
        "    tweet_small_rl = 'model/tweet_small_rl'\n",
        "    test_multiple1 = 'model/test_multiple1'\n",
        "    test_multiple2 = 'model/test_multiple2'\n",
        "    test_multiple3 = 'model/test_multiple3'\n",
        "    test_distributed = 'model/test_distributed'\n",
        "\n",
        "    @staticmethod\n",
        "    def create_all_directories():\n",
        "        for d in ModelDirectory:\n",
        "            os.makedirs(d.value, exist_ok=True)\n",
        "\n",
        "\n",
        "# todo\n",
        "# collect all initializer\n",
        "ModelDirectory.create_all_directories()\n",
        "\n",
        "base_hparams = tf.contrib.training.HParams(\n",
        "    machine=current_client_id,\n",
        "    batch_size=3,\n",
        "    num_units=6,\n",
        "    num_layers=2,\n",
        "    vocab_size=9,\n",
        "    embedding_size=8,\n",
        "    learning_rate=0.01,\n",
        "    learning_rate_decay=0.99,\n",
        "    use_attention=False,\n",
        "    encoder_length=5,\n",
        "    decoder_length=5,\n",
        "    max_gradient_norm=5.0,\n",
        "    beam_width=2,\n",
        "    num_train_steps=100,\n",
        "    debug_verbose=False,\n",
        "    model_path='Please override model_directory',\n",
        "    sos_id=0,\n",
        "    eos_id=1,\n",
        "    pad_id=2,\n",
        "    unk_id=3,\n",
        "    sos_token=\"[SOS]\",\n",
        "    eos_token=\"[EOS]\",\n",
        "    pad_token=\"[PAD]\",\n",
        "    unk_token=\"[UNK]\",\n",
        ")\n",
        "\n",
        "test_hparams = copy.deepcopy(base_hparams).override_from_dict(\n",
        "    {'beam_width': 0, 'num_train_steps': 100, 'learning_rate': 0.5})\n",
        "\n",
        "test_attention_hparams = copy.deepcopy(test_hparams).override_from_dict(\n",
        "    {'use_attention': True})\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fNrRD9yOFXM6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def print_hparams(hparams):\n",
        "    result = {}\n",
        "    for key in ['machine', 'batch_size', 'num_units', 'num_layers',\n",
        "                'vocab_size',\n",
        "                'embedding_size', 'learning_rate', 'learning_rate_decay',\n",
        "                'use_attention', 'encoder_length', 'decoder_length',\n",
        "                'max_gradient_norm', 'beam_width', 'num_train_steps',\n",
        "                'model_path']:\n",
        "        result[key] = hparams.get(key)\n",
        "    print(result)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DFEYKvBwL3Nm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# For debug purpose.\n",
        "tf.reset_default_graph()\n",
        "\n",
        "\n",
        "class ChatbotModel:\n",
        "    def __init__(self, sess, hparams, model_path, scope='ChatbotModel'):\n",
        "        self.sess = sess\n",
        "        # todo remove\n",
        "        self.hparams = hparams\n",
        "\n",
        "        # todo\n",
        "        self.model_path = model_path\n",
        "        self.scope = scope\n",
        "        # Sampled replies in previous session, this is necessary to backprop.\n",
        "        self.sampled = tf.placeholder(tf.int32, name=\"sampled\")\n",
        "        self.encoder_inputs, self.encoder_inputs_lengths, encoder_outputs, encoder_state, embedding_encoder = self._build_encoder(\n",
        "            hparams, scope)\n",
        "\n",
        "        self.decoder_inputs, self.decoder_target_lengths, self.logits, self.sample_logits, self.sample_replies, self.sample_log_prob, self.infer_logits, self.replies, self.beam_replies = self._build_decoder(\n",
        "            hparams, self.encoder_inputs_lengths, embedding_encoder,\n",
        "            encoder_state, encoder_outputs)\n",
        "\n",
        "        self.reward = tf.placeholder(tf.float32, name=\"reward\")\n",
        "        self.target_labels, self.global_step, self.loss, self.train_op = self._build_seq2seq_optimizer(\n",
        "            hparams, self.logits)\n",
        "        self.rl_loss, self.rl_train_op = self._build_rl_optimizer(hparams)\n",
        "\n",
        "        self.train_loss_summary = tf.summary.scalar(\"loss\", self.loss)\n",
        "        self.validation_loss_summary = tf.summary.scalar(\"validation_loss\",\n",
        "                                                         self.loss)\n",
        "        self.merged_summary = tf.summary.merge_all()\n",
        "\n",
        "        # Initialize saver after model created\n",
        "        self.saver = tf.train.Saver(tf.global_variables(), max_to_keep=1)\n",
        "\n",
        "    def restore(self):\n",
        "        ckpt = tf.train.get_checkpoint_state(self.model_path)\n",
        "        if ckpt:\n",
        "            last_model = ckpt.model_checkpoint_path\n",
        "            self.saver.restore(self.sess, last_model)\n",
        "            return True\n",
        "        else:\n",
        "            print(\"Created fresh model.\")\n",
        "            return False\n",
        "\n",
        "    def train(self, encoder_inputs, encoder_inputs_lengths, target_labels,\n",
        "              decoder_inputs, decoder_target_lengths):\n",
        "\n",
        "        feed_dict = {\n",
        "            self.encoder_inputs: encoder_inputs,\n",
        "            self.encoder_inputs_lengths: encoder_inputs_lengths,\n",
        "            self.target_labels: target_labels,\n",
        "            self.decoder_inputs: decoder_inputs,\n",
        "            self.decoder_target_lengths: decoder_target_lengths,\n",
        "        }\n",
        "        _, global_step, summary = self.sess.run(\n",
        "            [self.train_op, self.global_step, self.train_loss_summary],\n",
        "            feed_dict=feed_dict)\n",
        "\n",
        "        return global_step, summary\n",
        "\n",
        "      \n",
        "    def infer(self, encoder_inputs, encoder_inputs_lengths):\n",
        "        inference_feed_dict = {\n",
        "            self.encoder_inputs: encoder_inputs,\n",
        "            self.encoder_inputs_lengths: encoder_inputs_lengths,\n",
        "        }\n",
        "        replies = self.sess.run(self.replies, feed_dict=inference_feed_dict)\n",
        "        return replies\n",
        "\n",
        "    def infer_beam_search(self, encoder_inputs, encoder_inputs_lengths):\n",
        "        inference_feed_dict = {\n",
        "            self.encoder_inputs: encoder_inputs,\n",
        "            self.encoder_inputs_lengths: encoder_inputs_lengths,\n",
        "        }\n",
        "        replies = self.sess.run(self.beam_replies,\n",
        "                                feed_dict=inference_feed_dict)\n",
        "        return replies\n",
        "      \n",
        "    def sample(self, encoder_inputs, encoder_inputs_lengths):\n",
        "        #        print(\"encoder_inputs\", encoder_inputs)\n",
        "        inference_feed_dict = {\n",
        "            self.encoder_inputs: encoder_inputs,\n",
        "            self.encoder_inputs_lengths: encoder_inputs_lengths,\n",
        "        }\n",
        "\n",
        "        # Note: This will add print node to the graph on every sample.\n",
        "        # So this is debug only.\n",
        "        if False:\n",
        "          print_sample_replies = tf.Print(self.sample_replies,\n",
        "                                          [tf.shape(self.sample_replies)],\n",
        "                                          message=\"==== sample_replies in sample\")\n",
        "          \n",
        "        # log_prob: [batch_size, decoder_length]\n",
        "        replies, logits = self.sess.run(\n",
        "            [self.sample_replies, self.sample_logits],\n",
        "            feed_dict=inference_feed_dict)\n",
        "        return replies, logits\n",
        "\n",
        "    def batch_loss(self, encoder_inputs, encoder_inputs_lengths, target_labels,\n",
        "                   decoder_inputs, decoder_target_lengths):\n",
        "        feed_dict = {\n",
        "            self.encoder_inputs: encoder_inputs,\n",
        "            self.encoder_inputs_lengths: encoder_inputs_lengths,\n",
        "            self.target_labels: target_labels,\n",
        "            self.decoder_inputs: decoder_inputs,\n",
        "            self.decoder_target_lengths: decoder_target_lengths,\n",
        "        }\n",
        "        return self.sess.run([self.loss, self.validation_loss_summary],\n",
        "                             feed_dict=feed_dict)\n",
        "\n",
        "    def input_length(self, input):\n",
        "        try:\n",
        "            l = input.index(self.hparams.eos_id)\n",
        "            return l\n",
        "        except ValueError:\n",
        "            return self.hparams.encoder_length\n",
        "\n",
        "    def train_with_reward(self, encoder_inputs, encoder_inputs_lengths, sampled,\n",
        "                          reward):\n",
        "        feed_dict = {\n",
        "            self.encoder_inputs: encoder_inputs,\n",
        "            self.encoder_inputs_lengths: encoder_inputs_lengths,\n",
        "            self.sampled: sampled,\n",
        "            self.reward: reward\n",
        "        }\n",
        "\n",
        "        _, global_step = self.sess.run(\n",
        "            [self.rl_train_op, self.global_step],\n",
        "            feed_dict=feed_dict)\n",
        "        return global_step\n",
        "\n",
        "    def save(self, model_path=None):\n",
        "        if model_path is None:\n",
        "            model_path = self.model_path\n",
        "        model_dir = \"{}/{}\".format(model_path, self.scope)\n",
        "        self.saver.save(self.sess, model_dir, global_step=self.global_step)\n",
        "\n",
        "    @staticmethod\n",
        "    def _softmax(x):\n",
        "        return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
        "\n",
        "    def _build_rl_optimizer(self, hparams):\n",
        "        # RL optimizer\n",
        "        # todo mask the sampling results\n",
        "\n",
        "        sample_log_prob_shape = tf.shape(self.sample_log_prob)\n",
        "        reward_shape = tf.shape(self.reward)\n",
        "        reward_shape_print = tf.Print(reward_shape,\n",
        "                                      [reward_shape],\n",
        "                                      message=\"reward_shape\")\n",
        "        reward_print = tf.Print(self.reward,\n",
        "                                      [self.reward],\n",
        "                                      message=\"reward\")\n",
        "        \n",
        "        asserts = [tf.assert_equal(sample_log_prob_shape[0],\n",
        "                                   reward_shape_print[0],\n",
        "                                   [self.sample_log_prob,\n",
        "                                    self.reward]),\n",
        "                   tf.assert_equal(sample_log_prob_shape[1],\n",
        "                                   reward_shape_print[1],\n",
        "                                   [self.sample_log_prob,\n",
        "                                    self.reward]), reward_print\n",
        "                   ]\n",
        "        with tf.control_dependencies(asserts):\n",
        "            loss = -tf.reduce_sum(\n",
        "                self.sample_log_prob * self.reward) / tf.to_float(\n",
        "                hparams.batch_size)\n",
        "        train_op = self._build_optimizer_with_loss(self.global_step, hparams, loss)\n",
        "        return loss, train_op\n",
        "\n",
        "    def _build_optimizer_with_loss(self, global_step, hparams, loss):\n",
        "        params = tf.trainable_variables()\n",
        "        optimizer = tf.train.GradientDescentOptimizer(hparams.learning_rate)\n",
        "        gradients = tf.gradients(loss, params)\n",
        "        clipped_gradients, _ = tf.clip_by_global_norm(\n",
        "            gradients, hparams.max_gradient_norm)\n",
        "        with tf.device(self.available_device()):\n",
        "            train_op = optimizer.apply_gradients(\n",
        "                zip(clipped_gradients, params), global_step=global_step)\n",
        "        return train_op\n",
        "\n",
        "    def _build_seq2seq_optimizer(self, hparams, logits):\n",
        "        # Target labels\n",
        "        #   As described in doc for sparse_softmax_cross_entropy_with_logits,\n",
        "        #   labels should be [batch_size, decoder_target_lengths]\n",
        "        #   instead of [batch_size, decoder_target_lengths, vocab_size].\n",
        "        #   So labels should have indices instead of vocab_size classes.\n",
        "        target_labels = tf.placeholder(tf.int32, shape=(\n",
        "            hparams.batch_size, hparams.decoder_length), name=\"target_labels\")\n",
        "        # Loss\n",
        "        #   target_labels: [batch_size, decoder_length]\n",
        "        #   logits: [batch_size, decoder_length, vocab_size]\n",
        "        #   crossent: [batch_size, decoder_length]\n",
        "        crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "            labels=target_labels, logits=logits)\n",
        "        target_weights = tf.sequence_mask(self.decoder_target_lengths,\n",
        "                                          hparams.decoder_length,\n",
        "                                          dtype=logits.dtype)\n",
        "        crossent = crossent * target_weights\n",
        "        crossent_by_batch = tf.reduce_sum(crossent, axis=1)\n",
        "        loss = tf.reduce_sum(crossent_by_batch) / tf.to_float(\n",
        "            hparams.batch_size)\n",
        "        # Train\n",
        "        global_step = tf.get_variable(name=\"global_step\", shape=[],\n",
        "                                      dtype=tf.int32,\n",
        "                                      initializer=tf.constant_initializer(0),\n",
        "                                      trainable=False)\n",
        "        train_op = self._build_optimizer_with_loss(global_step, hparams, loss)\n",
        "        return target_labels, global_step, loss, train_op\n",
        "\n",
        "    @staticmethod\n",
        "    def available_device():\n",
        "        device = '/cpu:0'\n",
        "        if has_gpu0():\n",
        "            device = '/gpu:0'\n",
        "            print(\"$$$ GPU ENABLED $$$\")\n",
        "        return device\n",
        "\n",
        "    @staticmethod\n",
        "    def _build_encoder(hparams, scope):\n",
        "        # Encoder\n",
        "        #   encoder_inputs: [encoder_length, batch_size]\n",
        "        #   This is time major where encoder_length comes\n",
        "        #   first instead of batch_size.\n",
        "        #   encoder_inputs_lengths: [batch_size]\n",
        "        encoder_inputs = tf.placeholder(tf.int32, shape=(\n",
        "            hparams.encoder_length, hparams.batch_size), name=\"encoder_inputs\")\n",
        "        encoder_inputs_lengths = tf.placeholder(tf.int32,\n",
        "                                                shape=hparams.batch_size,\n",
        "                                                name=\"encoder_inputs_length\")\n",
        "\n",
        "        # Embedding\n",
        "        #   We originally didn't share embedding between encoder and decoder.\n",
        "        #   But now we share it. It makes much easier to calculate rewards.\n",
        "        #   Matrix for embedding: [vocab_size, embedding_size]\n",
        "        #   Should be shared between training and inference.\n",
        "        with tf.variable_scope(scope):\n",
        "            embedding_encoder = tf.get_variable(\"embedding_encoder\",\n",
        "                                                [hparams.vocab_size,\n",
        "                                                 hparams.embedding_size])\n",
        "\n",
        "        # Look up embedding:\n",
        "        #   encoder_inputs: [encoder_length, batch_size]\n",
        "        #   encoder_emb_inputs: [encoder_length, batch_size, embedding_size]\n",
        "        encoder_emb_inputs = tf.nn.embedding_lookup(embedding_encoder,\n",
        "                                                    encoder_inputs)\n",
        "\n",
        "        # LSTM cell.\n",
        "        with tf.variable_scope(scope):\n",
        "            # Should be shared between training and inference.\n",
        "            cell_list = []\n",
        "            for _ in range(hparams.num_layers):\n",
        "                cell_list.append(\n",
        "                    tf.nn.rnn_cell.BasicLSTMCell(hparams.num_units))\n",
        "            encoder_cell = tf.contrib.rnn.MultiRNNCell(cell_list)\n",
        "\n",
        "        # Run Dynamic RNN\n",
        "        #   encoder_outputs: [encoder_length, batch_size, num_units]\n",
        "        #   encoder_state: [batch_size, num_units],\n",
        "        #   this is final state of the cell for each batch.\n",
        "        with tf.variable_scope(scope):\n",
        "            encoder_outputs, encoder_state = tf.nn.dynamic_rnn(encoder_cell,\n",
        "                                                               encoder_emb_inputs,\n",
        "                                                               time_major=True,\n",
        "                                                               dtype=tf.float32,\n",
        "                                                               sequence_length=encoder_inputs_lengths)\n",
        "\n",
        "        return encoder_inputs, encoder_inputs_lengths, encoder_outputs, encoder_state, embedding_encoder\n",
        "\n",
        "    @staticmethod\n",
        "    def _build_training_decoder(hparams, encoder_inputs_lengths,\n",
        "                                encoder_state, encoder_outputs, decoder_cell,\n",
        "                                decoder_emb_inputs, decoder_target_lengths,\n",
        "                                projection_layer, scope):\n",
        "        # Decoder with helper:\n",
        "        #   decoder_emb_inputs: [decoder_length, batch_size, embedding_size]\n",
        "        #   decoder_target_lengths: [batch_size] vector,\n",
        "        #   which represents each target sequence length.\n",
        "        with tf.variable_scope(scope):\n",
        "          training_helper = tf.contrib.seq2seq.TrainingHelper(decoder_emb_inputs,\n",
        "                                                              decoder_target_lengths,\n",
        "                                                              time_major=True)\n",
        "\n",
        "        # See https://github.com/tensorflow/tensorflow/issues/11904\n",
        "        if hparams.use_attention:\n",
        "          with tf.variable_scope(scope):                 \n",
        "              # Attention\n",
        "              # encoder_outputs is time major, so transopse it to batch major.\n",
        "              # attention_encoder_outputs: [batch_size, encoder_length, num_units]\n",
        "              attention_encoder_outputs = tf.transpose(encoder_outputs, [1, 0, 2])\n",
        "     \n",
        "              # Create an attention mechanism\n",
        "              attention_mechanism = tf.contrib.seq2seq.LuongAttention(\n",
        "                  hparams.num_units,\n",
        "                  attention_encoder_outputs,\n",
        "                  memory_sequence_length=encoder_inputs_lengths)\n",
        "\n",
        "              wrapped_decoder_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
        "                  decoder_cell, attention_mechanism,\n",
        "                  attention_layer_size=hparams.num_units)\n",
        "\n",
        "              initial_state = wrapped_decoder_cell.zero_state(hparams.batch_size,\n",
        "                                                              tf.float32).clone(\n",
        "                  cell_state=encoder_state)\n",
        "        else:\n",
        "           with tf.variable_scope(scope):              \n",
        "              wrapped_decoder_cell = decoder_cell\n",
        "              initial_state = encoder_state\n",
        "\n",
        "            # Decoder and decode\n",
        "        with tf.variable_scope(scope): \n",
        "          with tf.variable_scope(\"training\"):     \n",
        "            training_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
        "                wrapped_decoder_cell, training_helper, initial_state,\n",
        "                output_layer=projection_layer)\n",
        "\n",
        "        # Dynamic decoding\n",
        "        #   final_outputs.rnn_output: [batch_size, decoder_length,\n",
        "        #                             vocab_size], list of RNN state.\n",
        "        #   final_outputs.sample_id: [batch_size, decoder_length],\n",
        "        #                            list of argmax of rnn_output.\n",
        "        #   final_state: [batch_size, num_units],\n",
        "        #                list of final state of RNN on decode process.\n",
        "        #   final_sequence_lengths: [batch_size], list of each decoded sequence.\n",
        "        with tf.variable_scope(scope):         \n",
        "          final_outputs, _final_state, _final_sequence_lengths = tf.contrib.seq2seq.dynamic_decode(\n",
        "              training_decoder)\n",
        "\n",
        "        if hparams.debug_verbose:\n",
        "            print(\"rnn_output.shape=\", final_outputs.rnn_output.shape)\n",
        "            print(\"sample_id.shape=\", final_outputs.sample_id.shape)\n",
        "            print(\"final_state=\", _final_state)\n",
        "            print(\"final_sequence_lengths.shape=\",\n",
        "                  _final_sequence_lengths.shape)\n",
        "\n",
        "        logits = final_outputs.rnn_output\n",
        "        return logits, wrapped_decoder_cell, initial_state\n",
        "\n",
        "    def _build_decoder(self, hparams, encoder_inputs_lengths, embedding_encoder,\n",
        "                       encoder_state, encoder_outputs):\n",
        "        # Decoder input\n",
        "        #   decoder_inputs: [decoder_length, batch_size]\n",
        "        #   decoder_target_lengths: [batch_size]\n",
        "        #   This is grand truth target inputs for training.\n",
        "        decoder_inputs = tf.placeholder(tf.int32, shape=(\n",
        "            hparams.decoder_length, hparams.batch_size), name=\"decoder_inputs\")\n",
        "        decoder_target_lengths = tf.placeholder(tf.int32,\n",
        "                                                shape=hparams.batch_size,\n",
        "                                                name=\"decoder_target_lengths\")\n",
        "\n",
        "        # Look up embedding:\n",
        "        #   decoder_inputs: [decoder_length, batch_size]\n",
        "        #   decoder_emb_inp: [decoder_length, batch_size, embedding_size]\n",
        "        decoder_emb_inputs = tf.nn.embedding_lookup(embedding_encoder,\n",
        "                                                    decoder_inputs)\n",
        "\n",
        "        # https://stackoverflow.com/questions/39573188/output-projection-in-seq2seq-model-tensorflow\n",
        "        # Internally, a neural network operates on dense vectors of some size,\n",
        "        # often 256, 512 or 1024 floats (let's say 512 for here).\n",
        "        # But at the end it needs to predict a word\n",
        "        # from the vocabulary which is often much larger,\n",
        "        # e.g., 40000 words. Output projection is the final linear layer\n",
        "        # that converts (projects) from the internal representation\n",
        "        #  to the larger one.\n",
        "        # So, for example, it can consist of a 512 x 40000 parameter matrix\n",
        "        # and a 40000 parameter for the bias vector.\n",
        "        projection_layer = layers_core.Dense(hparams.vocab_size, use_bias=False)\n",
        "\n",
        "        # We share this between training and inference.\n",
        "        cell_list = []\n",
        "        for _ in range(hparams.num_layers):\n",
        "            cell_list.append(tf.nn.rnn_cell.BasicLSTMCell(hparams.num_units))\n",
        "        decoder_cell = tf.contrib.rnn.MultiRNNCell(cell_list)\n",
        "\n",
        "        #        decoder_cell = tf.nn.rnn_cell.BasicLSTMCell(hparams.num_units)\n",
        "\n",
        "        # Training graph\n",
        "        logits, wrapped_decoder_cell, initial_state = self._build_training_decoder(\n",
        "            hparams, encoder_inputs_lengths, encoder_state, encoder_outputs,\n",
        "            decoder_cell, decoder_emb_inputs, decoder_target_lengths,\n",
        "            projection_layer, self.scope)\n",
        "        \n",
        "        infer_logits, replies = self._build_greedy_inference(hparams,\n",
        "                                                             embedding_encoder,\n",
        "                                                             encoder_state,\n",
        "                                                             encoder_inputs_lengths,\n",
        "                                                             encoder_outputs,\n",
        "                                                             decoder_cell,\n",
        "                                                             projection_layer,\n",
        "                                                             self.scope)        \n",
        "        \n",
        "        # Beam Search Inference graph\n",
        "        beam_replies = self._build_beam_search_inference(hparams,\n",
        "                                                         encoder_inputs_lengths,\n",
        "                                                         embedding_encoder,\n",
        "                                                         encoder_state,\n",
        "                                                         encoder_outputs,\n",
        "                                                         decoder_cell,\n",
        "                                                         projection_layer,\n",
        "                                                         self.scope)\n",
        "        \n",
        "\n",
        "        # Sample Inference graph\n",
        "        sample_logits, sample_replies = self._build_sample_inference(hparams,\n",
        "                                                                     embedding_encoder,\n",
        "                                                                     encoder_state,\n",
        "                                                                     encoder_inputs_lengths,\n",
        "                                                                     encoder_outputs,\n",
        "                                                                     decoder_cell,\n",
        "                                                                     projection_layer, self.scope)\n",
        "        indices = self._convert_indices(self.sampled)\n",
        "#        print_indices0 = tf.Print(indices, [tf.shape(indices)],\n",
        "#                                  message=\"OPT:indices.shape\")\n",
        "#        print_indices1 = tf.Print(print_indices0, [tf.shape(sample_logits)],\n",
        "#                                  message=\"OPT:sample_logits.shape\")\n",
        "#        print_indices2 = tf.Print(print_indices1, [tf.shape(sample_replies)],\n",
        "#                                  message=\"OPT:sample_replies.shape\")\n",
        "\n",
        "        sample_log_prob = tf.gather_nd(sample_logits, indices)\n",
        "        sample_log_prob0 = tf.Print(sample_log_prob,\n",
        "                                    [tf.shape(sample_log_prob)],\n",
        "                                    message=\"OPT:sample_log_prob\")\n",
        "        return decoder_inputs, decoder_target_lengths, logits, sample_logits, sample_replies, sample_log_prob0, infer_logits, replies, beam_replies\n",
        "    \n",
        "    \n",
        "    @staticmethod\n",
        "    def _build_greedy_inference(hparams, embedding_encoder, encoder_state,\n",
        "                                encoder_inputs_lengths, encoder_outputs,\n",
        "                                decoder_cell, projection_layer, scope):\n",
        "        # Greedy decoder\n",
        "        dynamic_batch_size = tf.shape(encoder_inputs_lengths)[0]\n",
        "        inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n",
        "            embedding_encoder,\n",
        "            tf.fill([dynamic_batch_size], hparams.sos_id), hparams.eos_id)\n",
        "\n",
        "        # See https://github.com/tensorflow/tensorflow/issues/11904\n",
        "        if hparams.use_attention:\n",
        "            with tf.variable_scope(scope, reuse=True):\n",
        "                # Attention\n",
        "                # encoder_outputs is time major, so transopse it to batch major.\n",
        "                # attention_encoder_outputs: [batch_size, encoder_length, num_units]\n",
        "                attention_encoder_outputs = tf.transpose(encoder_outputs,\n",
        "                                                         [1, 0, 2])\n",
        "\n",
        "                # Create an attention mechanism\n",
        "                attention_mechanism = tf.contrib.seq2seq.LuongAttention(\n",
        "                    hparams.num_units,\n",
        "                    attention_encoder_outputs,\n",
        "                    memory_sequence_length=encoder_inputs_lengths)\n",
        "\n",
        "                wrapped_decoder_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
        "                    decoder_cell, attention_mechanism,\n",
        "                    attention_layer_size=hparams.num_units)\n",
        "\n",
        "                initial_state = wrapped_decoder_cell.zero_state(\n",
        "                    dynamic_batch_size,\n",
        "                    tf.float32).clone(\n",
        "                    cell_state=encoder_state)\n",
        "        else:\n",
        "            with tf.variable_scope(scope, reuse=True):\n",
        "                wrapped_decoder_cell = decoder_cell\n",
        "                initial_state = encoder_state\n",
        "\n",
        "        with tf.variable_scope(scope):\n",
        "            with tf.variable_scope(\"infer\"):\n",
        "                inference_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
        "                    wrapped_decoder_cell, inference_helper, initial_state,\n",
        "                    output_layer=projection_layer)    \n",
        "                \n",
        "        # len(inferred_reply) is lte encoder_length,\n",
        "        # because we are targeting tweet (140 for each tweet)\n",
        "        # Also by doing this,\n",
        "        # we can pass the reply to other seq2seq w/o shorten it.\n",
        "        maximum_iterations = hparams.encoder_length\n",
        "\n",
        "        # Dynamic decoding\n",
        "        with tf.variable_scope(scope, reuse=True):\n",
        "            outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
        "                inference_decoder, maximum_iterations=maximum_iterations)\n",
        "        replies = outputs.sample_id\n",
        "\n",
        "        # We use infer_logits instead of logits when calculating log_prob,\n",
        "        # because infer_logits doesn't require decoder_target_lengths input.\n",
        "        infer_logits = outputs.rnn_output\n",
        "        return infer_logits, replies                \n",
        "    \n",
        "    @staticmethod\n",
        "    def _build_beam_search_inference(hparams, encoder_inputs_lengths,\n",
        "                                     embedding_encoder, encoder_state,\n",
        "                                     encoder_outputs, decoder_cell,\n",
        "                                     projection_layer, scope):\n",
        "\n",
        "        assert(hparams.beam_width != 0)\n",
        "\n",
        "        dynamic_batch_size = tf.shape(encoder_inputs_lengths)[0]\n",
        "        # https://github.com/tensorflow/tensorflow/issues/11904\n",
        "        if hparams.use_attention:\n",
        "            with tf.variable_scope(scope, reuse=True):\n",
        "                # Attention\n",
        "                # encoder_outputs is time major, so transopse it to batch major.\n",
        "                # attention_encoder_outputs: [batch_size, encoder_length, num_units]\n",
        "                attention_encoder_outputs = tf.transpose(encoder_outputs,\n",
        "                                                         [1, 0, 2])\n",
        "\n",
        "                tiled_encoder_outputs = tf.contrib.seq2seq.tile_batch(\n",
        "                    attention_encoder_outputs, multiplier=hparams.beam_width)\n",
        "                tiled_encoder_final_state = tf.contrib.seq2seq.tile_batch(\n",
        "                    encoder_state, multiplier=hparams.beam_width)\n",
        "                tiled_encoder_inputs_lengths = tf.contrib.seq2seq.tile_batch(\n",
        "                    encoder_inputs_lengths, multiplier=hparams.beam_width)\n",
        "\n",
        "                # Create an attention mechanism\n",
        "                attention_mechanism = tf.contrib.seq2seq.LuongAttention(\n",
        "                    hparams.num_units, tiled_encoder_outputs,\n",
        "                    memory_sequence_length=tiled_encoder_inputs_lengths)\n",
        "\n",
        "                wrapped_decoder_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
        "                    decoder_cell, attention_mechanism,\n",
        "                    attention_layer_size=hparams.num_units)\n",
        "\n",
        "                decoder_initial_state = wrapped_decoder_cell.zero_state(\n",
        "                    dtype=tf.float32,\n",
        "                    batch_size=dynamic_batch_size * hparams.beam_width)\n",
        "                decoder_initial_state = decoder_initial_state.clone(\n",
        "                    cell_state=tiled_encoder_final_state)\n",
        "        else:\n",
        "            with tf.variable_scope(scope, reuse=True):\n",
        "                wrapped_decoder_cell = decoder_cell\n",
        "                decoder_initial_state = tf.contrib.seq2seq.tile_batch(\n",
        "                    encoder_state,\n",
        "                    multiplier=hparams.beam_width)\n",
        "\n",
        "        # len(inferred_reply) is lte encoder_length,\n",
        "        # because we are targeting tweet (140 for each tweet)\n",
        "        # Also by doing this,\n",
        "        # we can pass the reply to other seq2seq w/o shorten it.\n",
        "        maximum_iterations = hparams.encoder_length\n",
        "\n",
        "        inference_decoder = tf.contrib.seq2seq.BeamSearchDecoder(\n",
        "            cell=wrapped_decoder_cell,\n",
        "            embedding=embedding_encoder,\n",
        "            start_tokens=tf.fill([dynamic_batch_size], hparams.sos_id),\n",
        "            end_token=hparams.eos_id,\n",
        "            initial_state=decoder_initial_state,\n",
        "            beam_width=hparams.beam_width,\n",
        "            output_layer=projection_layer,\n",
        "            length_penalty_weight=0.0)\n",
        "\n",
        "        # Dynamic decoding\n",
        "        with tf.variable_scope(scope, reuse=True):\n",
        "            beam_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
        "                inference_decoder, maximum_iterations=maximum_iterations)\n",
        "        beam_replies = beam_outputs.predicted_ids\n",
        "        return beam_replies\n",
        "    \n",
        "\n",
        "    @staticmethod\n",
        "    def _build_sample_inference(hparams, embedding_encoder, encoder_state,\n",
        "                                encoder_inputs_lengths, encoder_outputs,\n",
        "                                decoder_cell, projection_layer, scope):\n",
        "        # Sample decoder\n",
        "        dynamic_batch_size = tf.shape(encoder_inputs_lengths)[0]\n",
        "        inference_helper = tf.contrib.seq2seq.SampleEmbeddingHelper(\n",
        "            embedding_encoder,\n",
        "            tf.fill([dynamic_batch_size], hparams.sos_id), hparams.eos_id,\n",
        "            softmax_temperature=1.5)\n",
        "        \n",
        "#        with tf.variable_scope(scope):\n",
        "#          inference_helper =  tf.contrib.seq2seq.GreedyEmbeddingHelper(\n",
        "#              embedding_encoder,\n",
        "#            tf.fill([dynamic_batch_size], hparams.sos_id), hparams.eos_id)        \n",
        "\n",
        "        # See https://github.com/tensorflow/tensorflow/issues/11904\n",
        "        if hparams.use_attention:\n",
        "          with tf.variable_scope(scope, reuse=True):\n",
        "              # Attention\n",
        "              # encoder_outputs is time major, so transopse it to batch major.\n",
        "              # attention_encoder_outputs: [batch_size, encoder_length, num_units]\n",
        "              attention_encoder_outputs = tf.transpose(encoder_outputs, [1, 0, 2])\n",
        "\n",
        "              # Create an attention mechanism\n",
        "              attention_mechanism = tf.contrib.seq2seq.LuongAttention(\n",
        "                  hparams.num_units,\n",
        "                  attention_encoder_outputs,\n",
        "                  memory_sequence_length=encoder_inputs_lengths)\n",
        "\n",
        "              wrapped_decoder_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
        "                  decoder_cell, attention_mechanism,\n",
        "                  attention_layer_size=hparams.num_units)\n",
        "\n",
        "              initial_state = wrapped_decoder_cell.zero_state(dynamic_batch_size,\n",
        "                                                              tf.float32).clone(\n",
        "                  cell_state=encoder_state)\n",
        "        else:\n",
        "           with tf.variable_scope(scope, reuse=True):          \n",
        "              wrapped_decoder_cell = decoder_cell\n",
        "              initial_state = encoder_state\n",
        "\n",
        "        with tf.variable_scope(scope):  \n",
        "          with tf.variable_scope(\"sampling\"):  \n",
        "            inference_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
        "                wrapped_decoder_cell, inference_helper, initial_state,\n",
        "                output_layer=projection_layer)\n",
        "\n",
        "        # len(inferred_reply) is lte encoder_length,\n",
        "        # because we are targeting tweet (140 for each tweet)\n",
        "        # Also by doing this,\n",
        "        # we can pass the reply to other seq2seq w/o shorten it.\n",
        "        maximum_iterations = hparams.encoder_length\n",
        "\n",
        "        # Dynamic decoding\n",
        "        # Here we reuse Attention Wrapper\n",
        "        with tf.variable_scope(scope, reuse=True): \n",
        "          outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
        "              inference_decoder, maximum_iterations=maximum_iterations)\n",
        "        replies = outputs.sample_id\n",
        "\n",
        "        # We use infer_logits instead of logits when calculating log_prob,\n",
        "        # because infer_logits doesn't require decoder_target_lengths input.\n",
        "        infer_logits = outputs.rnn_output\n",
        "        return infer_logits, replies\n",
        "\n",
        "    # convert sampled_indices to indices for tf.gather_nd.\n",
        "    @staticmethod\n",
        "    def _convert_indices(sampled_indices):\n",
        "        print_sampled_indices = tf.Print(sampled_indices,\n",
        "                                         [tf.shape(sampled_indices)],\n",
        "                                         message=\"sampled_indices\")\n",
        "        batch_size = tf.shape(print_sampled_indices)[0]\n",
        "        decoder_length = tf.shape(print_sampled_indices)[1]\n",
        "        print_batch_size = tf.Print(batch_size, [batch_size, decoder_length],\n",
        "                                    message=\"(batch_size, decoder_length)\")\n",
        "        first_indices = tf.tile(\n",
        "            tf.expand_dims(tf.range(print_batch_size), dim=1),\n",
        "            [1, decoder_length])\n",
        "        second_indices = tf.reshape(\n",
        "            tf.tile(tf.range(decoder_length), [print_batch_size]),\n",
        "            [print_batch_size, decoder_length])\n",
        "        print_first_indices = tf.Print(first_indices, [tf.shape(first_indices),\n",
        "                                                       tf.shape(\n",
        "                                                           second_indices)],\n",
        "                                       message=\"(first_indices, second_indices)\")\n",
        "        return tf.stack([print_first_indices, second_indices, sampled_indices],\n",
        "                        axis=2)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JzDknaQZV-iU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ChatbotInferenceModel:\n",
        "    def __init__(self, sess, hparams, model_path, scope='ChatbotModel'):\n",
        "        self.sess = sess\n",
        "        # todo remove\n",
        "        self.hparams = hparams\n",
        "\n",
        "        # todo\n",
        "        self.model_path = model_path\n",
        "        self.name = scope\n",
        "        self.scope = scope\n",
        "\n",
        "        self.encoder_inputs, self.encoder_inputs_lengths, encoder_outputs, encoder_state, embedding_encoder = self._build_encoder(\n",
        "            hparams, scope)\n",
        "        self.decoder_inputs, self.decoder_target_lengths, self.replies, self.beam_replies, self.infer_logits = self._build_decoder(\n",
        "            hparams, self.encoder_inputs_lengths, embedding_encoder,\n",
        "            encoder_state, encoder_outputs)\n",
        "\n",
        "        self.reward = tf.placeholder(tf.float32, name=\"reward\")\n",
        "\n",
        "        # we can't use variable length here,\n",
        "        # because tiled_batch requires constant length.\n",
        "        self.batch_size = 1\n",
        "\n",
        "        # Initialize saver after model created\n",
        "        self.saver = tf.train.Saver(tf.global_variables(), max_to_keep=1)\n",
        "\n",
        "    def restore(self):\n",
        "        ckpt = tf.train.get_checkpoint_state(self.model_path)\n",
        "        if ckpt:\n",
        "            last_model = ckpt.model_checkpoint_path\n",
        "            self.saver.restore(self.sess, last_model)\n",
        "            return True\n",
        "        else:\n",
        "            print(\"Created fresh model.\")\n",
        "            return False\n",
        "\n",
        "    def infer(self, encoder_inputs, encoder_inputs_lengths):\n",
        "        inference_feed_dict = {\n",
        "            self.encoder_inputs: encoder_inputs,\n",
        "            self.encoder_inputs_lengths: encoder_inputs_lengths,\n",
        "        }\n",
        "        replies = self.sess.run(self.replies, feed_dict=inference_feed_dict)\n",
        "        return replies\n",
        "\n",
        "    def infer_beam_search(self, encoder_inputs, encoder_inputs_lengths):\n",
        "        inference_feed_dict = {\n",
        "            self.encoder_inputs: encoder_inputs,\n",
        "            self.encoder_inputs_lengths: encoder_inputs_lengths,\n",
        "        }\n",
        "        replies = self.sess.run(self.beam_replies,\n",
        "                                feed_dict=inference_feed_dict)\n",
        "        return replies\n",
        "\n",
        "    def infer_mi(self, swapped_model, encoder_inputs, encoder_inputs_lengths):\n",
        "        inference_feed_dict = {\n",
        "            self.encoder_inputs: encoder_inputs,\n",
        "            self.encoder_inputs_lengths: encoder_inputs_lengths,\n",
        "        }\n",
        "        beam_replies = self.sess.run(self.beam_replies,\n",
        "                                     feed_dict=inference_feed_dict)\n",
        "        # beam_replis [batch_size, length, , batch_width]\n",
        "        # for now we assume encoder_inputs is batch_size = 1\n",
        "\n",
        "        swapped_encoder_inputs = beam_replies[0]\n",
        "        # beam_width = batch_size\n",
        "        swapped_batch_size = swapped_encoder_inputs.shape[1]\n",
        "\n",
        "        # beam_replies can be shorten less than decoder_output_legth, so we pad them.\n",
        "        paddings = tf.constant([[0, self.hparams.encoder_length -\n",
        "                                 swapped_encoder_inputs.shape[0], ], [0, 0]])\n",
        "        swapped_encoder_inputs = swapped_model.sess.run(\n",
        "            tf.pad(swapped_encoder_inputs, paddings, \"CONSTANT\",\n",
        "                   constant_values=self.hparams.pad_id))\n",
        "        swapped_encoder_inputs_lengths = np.empty(swapped_batch_size,\n",
        "                                                  dtype=np.int)\n",
        "        for i in range(swapped_batch_size):\n",
        "            swapped_encoder_inputs_lengths[i] = swapped_encoder_inputs.shape[0]\n",
        "\n",
        "        return swapped_model.infer_beam_search(swapped_encoder_inputs,\n",
        "                                               swapped_encoder_inputs_lengths)\n",
        "        # todo make correct length\n",
        "\n",
        "    #        for repy in beam_replies:\n",
        "    # logits from swapped_model for this reply\n",
        "    # cals prob for in original encoder_input\n",
        "\n",
        "    def log_prob(self, encoder_inputs, encoder_inputs_lengths, expected_output):\n",
        "        \"\"\"Return sum of log probability of given\n",
        "           one specific expected_output for encoder_inputs.\n",
        "\n",
        "        Args:\n",
        "            encoder_inputs: [encoder_length, batch_size], eg) tweets\n",
        "            expected_output: [1, decoder_length or less than decoder_length],\n",
        "            eg) One reply.\n",
        "\n",
        "        Returns:\n",
        "            Return log probablity of expected output for given encoder inputs.\n",
        "            eg) sum of log probability of reply \"Good\" when given [\"How are you?\",\n",
        "             \"What's up?\"]\n",
        "        \"\"\"\n",
        "        inference_feed_dict = {\n",
        "            self.encoder_inputs: encoder_inputs,\n",
        "            self.encoder_inputs_lengths: encoder_inputs_lengths\n",
        "        }\n",
        "\n",
        "        # Logits\n",
        "        #   logits_value: [batch_size, actual_decoder_length, vocab_size]\n",
        "        logits_batch_value = self.sess.run(self.infer_logits,\n",
        "                                           feed_dict=inference_feed_dict)\n",
        "\n",
        "        sum_p = []\n",
        "        # For each batch: [actual_decoder_length, vocab_size]\n",
        "        for logits in logits_batch_value:\n",
        "            p = 1\n",
        "            # Note that expected_output and logits don't always have\n",
        "            # same length, but zip takes care of the case.\n",
        "            for word_id, logit in zip(expected_output, logits):\n",
        "                # Apply softmax first, see definition of softmax.\n",
        "                norm = (self._softmax(logit))[word_id]\n",
        "                p *= norm\n",
        "            p = np.log(p)\n",
        "            sum_p.append(p)\n",
        "        ret = np.sum(sum_p) / len(sum_p)\n",
        "        return ret\n",
        "\n",
        "    def reward_ease_of_answering(self, max_len, encoder_inputs,\n",
        "                                 encoder_inputs_lengths,\n",
        "                                 dull_responses):\n",
        "        \"\"\" Return reward for ease of answering.\n",
        "            See Deep Reinforcement Learning for Dialogue Generation\n",
        "            for more details.\n",
        "\n",
        "        Args:\n",
        "            encoder_inputs: [encoder_length, batch_size], eg) tweets\n",
        "            dull_responses: [number of pre-defined dull responses,\n",
        "            decoder_length or less than decoder_length].\n",
        "            eg) [[\"I'm\", \"Good\"], [\"fine\"]]\n",
        "\n",
        "        Returns:\n",
        "            Return reward for ease of answering.\n",
        "        \"\"\"\n",
        "        inference_feed_dict = {\n",
        "            self.encoder_inputs: encoder_inputs,\n",
        "            self.encoder_inputs_lengths: encoder_inputs_lengths\n",
        "        }\n",
        "\n",
        "        # Logits\n",
        "        #   logits_value: [batch_size, actual_decoder_length, vocab_size]\n",
        "        logits_batch_value = self.sess.run(self.infer_logits,\n",
        "                                           feed_dict=inference_feed_dict)\n",
        "\n",
        "        # Note that encoder_inputs here is time major.\n",
        "        reward = np.ones((self.hparams.batch_size, max_len))\n",
        "        print(\n",
        "            \"constructing reward{}{}\".format(self.hparams.batch_size, max_len))\n",
        "        # For each batch: [actual_decoder_length, vocab_size]\n",
        "        for i, logits in enumerate(logits_batch_value):\n",
        "            p_array = []\n",
        "            for dull_response in dull_responses:\n",
        "                p = 1\n",
        "                # Note that dull_response and logits don't\n",
        "                # always have same length, but zip takes care of the case.\n",
        "                for word_id, logit in zip(dull_response, logits):\n",
        "                    # Apply softmax first, see definition of softmax.\n",
        "                    norm = (self._softmax(logit))[word_id]\n",
        "                    p *= norm\n",
        "                # This is P(dull_response|encoder_input)\n",
        "                p = np.log(p) / len(dull_response)\n",
        "                p_array.append(p)\n",
        "            batch_p = np.sum(p_array) / len(dull_responses)\n",
        "            batch_reward = -batch_p\n",
        "            for j in range(max_len):\n",
        "                reward[i][j] = batch_reward\n",
        "        return reward\n",
        "\n",
        "    @staticmethod\n",
        "    def _softmax(x):\n",
        "        return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
        "\n",
        "    @staticmethod\n",
        "    def _build_encoder(hparams, scope):\n",
        "        # Encoder\n",
        "        #   encoder_inputs: [encoder_length, batch_size]\n",
        "        #   This is time major where encoder_length\n",
        "        #   comes first instead of batch_size.\n",
        "        #   encoder_inputs_lengths: [batch_size]\n",
        "        encoder_inputs = tf.placeholder(tf.int32,\n",
        "                                        shape=[hparams.encoder_length, None],\n",
        "                                        name=\"encoder_inputs\")\n",
        "        encoder_inputs_lengths = tf.placeholder(tf.int32, shape=[None],\n",
        "                                                name=\"encoder_inputs_lengths\")\n",
        "\n",
        "        # Embedding\n",
        "        #   We originally didn't share embedding between encoder and decoder.\n",
        "        #   But now we share it. It makes much easier to calculate rewards.\n",
        "        #   Matrix for embedding: [vocab_size, embedding_size]\n",
        "        #   Should be shared between training and inference.\n",
        "        with tf.variable_scope(scope):\n",
        "            embedding_encoder = tf.get_variable(\"embedding_encoder\",\n",
        "                                                [hparams.vocab_size,\n",
        "                                                 hparams.embedding_size])\n",
        "\n",
        "        # Look up embedding:\n",
        "        #   encoder_inputs: [encoder_length, batch_size]\n",
        "        #   encoder_emb_inputs: [encoder_length, batch_size, embedding_size]\n",
        "        encoder_emb_inputs = tf.nn.embedding_lookup(embedding_encoder,\n",
        "                                                    encoder_inputs)\n",
        "\n",
        "        # LSTM cell.\n",
        "        with tf.variable_scope(scope):\n",
        "            # Should be shared between training and inference.\n",
        "            #            encoder_cell = tf.nn.rnn_cell.BasicLSTMCell(hparams.num_units)\n",
        "            cell_list = []\n",
        "            for _ in range(hparams.num_layers):\n",
        "                cell_list.append(\n",
        "                    tf.nn.rnn_cell.BasicLSTMCell(hparams.num_units))\n",
        "            encoder_cell = tf.contrib.rnn.MultiRNNCell(cell_list)\n",
        "\n",
        "        # Run Dynamic RNN\n",
        "        #   encoder_outputs: [encoder_length, batch_size, num_units]\n",
        "        #   encoder_state: [batch_size, num_units],\n",
        "        #   this is final state of the cell for each batch.\n",
        "        with tf.variable_scope(scope):\n",
        "            encoder_outputs, encoder_state = tf.nn.dynamic_rnn(encoder_cell,\n",
        "                                                               encoder_emb_inputs,\n",
        "                                                               time_major=True,\n",
        "                                                               dtype=tf.float32,\n",
        "                                                               sequence_length=encoder_inputs_lengths)\n",
        "\n",
        "        return encoder_inputs, encoder_inputs_lengths, encoder_outputs, encoder_state, embedding_encoder\n",
        "\n",
        "    @staticmethod\n",
        "    def _build_greedy_inference(hparams, embedding_encoder, encoder_state,\n",
        "                                encoder_inputs_lengths, encoder_outputs,\n",
        "                                decoder_cell, projection_layer, scope):\n",
        "        # Greedy decoder\n",
        "        dynamic_batch_size = tf.shape(encoder_inputs_lengths)[0]\n",
        "        inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n",
        "            embedding_encoder,\n",
        "            tf.fill([dynamic_batch_size], hparams.sos_id), hparams.eos_id)\n",
        "\n",
        "        # See https://github.com/tensorflow/tensorflow/issues/11904\n",
        "        if hparams.use_attention:\n",
        "            with tf.variable_scope(scope):\n",
        "                # Attention\n",
        "                # encoder_outputs is time major, so transopse it to batch major.\n",
        "                # attention_encoder_outputs: [batch_size, encoder_length, num_units]\n",
        "                attention_encoder_outputs = tf.transpose(encoder_outputs,\n",
        "                                                         [1, 0, 2])\n",
        "\n",
        "                # Create an attention mechanism\n",
        "                attention_mechanism = tf.contrib.seq2seq.LuongAttention(\n",
        "                    hparams.num_units,\n",
        "                    attention_encoder_outputs,\n",
        "                    memory_sequence_length=encoder_inputs_lengths)\n",
        "\n",
        "                wrapped_decoder_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
        "                    decoder_cell, attention_mechanism,\n",
        "                    attention_layer_size=hparams.num_units)\n",
        "\n",
        "                initial_state = wrapped_decoder_cell.zero_state(\n",
        "                    dynamic_batch_size,\n",
        "                    tf.float32).clone(\n",
        "                    cell_state=encoder_state)\n",
        "        else:\n",
        "            with tf.variable_scope(scope):\n",
        "                wrapped_decoder_cell = decoder_cell\n",
        "                initial_state = encoder_state\n",
        "\n",
        "        with tf.variable_scope(scope):\n",
        "            with tf.variable_scope(\"infer\"):\n",
        "                inference_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
        "                    wrapped_decoder_cell, inference_helper, initial_state,\n",
        "                    output_layer=projection_layer)\n",
        "\n",
        "        # len(inferred_reply) is lte encoder_length,\n",
        "        # because we are targeting tweet (140 for each tweet)\n",
        "        # Also by doing this,\n",
        "        # we can pass the reply to other seq2seq w/o shorten it.\n",
        "        maximum_iterations = hparams.encoder_length\n",
        "\n",
        "        # Dynamic decoding\n",
        "        with tf.variable_scope(scope):\n",
        "            outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
        "                inference_decoder, maximum_iterations=maximum_iterations)\n",
        "        replies = outputs.sample_id\n",
        "\n",
        "        # We use infer_logits instead of logits when calculating log_prob,\n",
        "        # because infer_logits doesn't require decoder_target_lengths input.\n",
        "        infer_logits = outputs.rnn_output\n",
        "        return infer_logits, replies\n",
        "\n",
        "    @staticmethod\n",
        "    def _build_beam_search_inference(hparams, encoder_inputs_lengths,\n",
        "                                     embedding_encoder, encoder_state,\n",
        "                                     encoder_outputs, decoder_cell,\n",
        "                                     projection_layer, scope):\n",
        "\n",
        "        assert(hparams.beam_width != 0)\n",
        "\n",
        "        dynamic_batch_size = tf.shape(encoder_inputs_lengths)[0]\n",
        "        # https://github.com/tensorflow/tensorflow/issues/11904\n",
        "        if hparams.use_attention:\n",
        "            with tf.variable_scope(scope, reuse=True):\n",
        "                # Attention\n",
        "                # encoder_outputs is time major, so transopse it to batch major.\n",
        "                # attention_encoder_outputs: [batch_size, encoder_length, num_units]\n",
        "                attention_encoder_outputs = tf.transpose(encoder_outputs,\n",
        "                                                         [1, 0, 2])\n",
        "\n",
        "                tiled_encoder_outputs = tf.contrib.seq2seq.tile_batch(\n",
        "                    attention_encoder_outputs, multiplier=hparams.beam_width)\n",
        "                tiled_encoder_final_state = tf.contrib.seq2seq.tile_batch(\n",
        "                    encoder_state, multiplier=hparams.beam_width)\n",
        "                tiled_encoder_inputs_lengths = tf.contrib.seq2seq.tile_batch(\n",
        "                    encoder_inputs_lengths, multiplier=hparams.beam_width)\n",
        "\n",
        "                # Create an attention mechanism\n",
        "                attention_mechanism = tf.contrib.seq2seq.LuongAttention(\n",
        "                    hparams.num_units, tiled_encoder_outputs,\n",
        "                    memory_sequence_length=tiled_encoder_inputs_lengths)\n",
        "\n",
        "                wrapped_decoder_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
        "                    decoder_cell, attention_mechanism,\n",
        "                    attention_layer_size=hparams.num_units)\n",
        "\n",
        "                decoder_initial_state = wrapped_decoder_cell.zero_state(\n",
        "                    dtype=tf.float32,\n",
        "                    batch_size=dynamic_batch_size * hparams.beam_width)\n",
        "                decoder_initial_state = decoder_initial_state.clone(\n",
        "                    cell_state=tiled_encoder_final_state)\n",
        "        else:\n",
        "            with tf.variable_scope(scope, reuse=True):\n",
        "                wrapped_decoder_cell = decoder_cell\n",
        "                decoder_initial_state = tf.contrib.seq2seq.tile_batch(\n",
        "                    encoder_state,\n",
        "                    multiplier=hparams.beam_width)\n",
        "\n",
        "        # len(inferred_reply) is lte encoder_length,\n",
        "        # because we are targeting tweet (140 for each tweet)\n",
        "        # Also by doing this,\n",
        "        # we can pass the reply to other seq2seq w/o shorten it.\n",
        "        maximum_iterations = hparams.encoder_length\n",
        "\n",
        "        inference_decoder = tf.contrib.seq2seq.BeamSearchDecoder(\n",
        "            cell=wrapped_decoder_cell,\n",
        "            embedding=embedding_encoder,\n",
        "            start_tokens=tf.fill([dynamic_batch_size], hparams.sos_id),\n",
        "            end_token=hparams.eos_id,\n",
        "            initial_state=decoder_initial_state,\n",
        "            beam_width=hparams.beam_width,\n",
        "            output_layer=projection_layer,\n",
        "            length_penalty_weight=0.0)\n",
        "\n",
        "        # Dynamic decoding\n",
        "        with tf.variable_scope(scope, reuse=True):\n",
        "            beam_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
        "                inference_decoder, maximum_iterations=maximum_iterations)\n",
        "        beam_replies = beam_outputs.predicted_ids\n",
        "        return beam_replies\n",
        "\n",
        "    def _build_decoder(self, hparams, encoder_inputs_lengths, embedding_encoder,\n",
        "                       encoder_state, encoder_outputs):\n",
        "        # Decoder input\n",
        "        #   decoder_inputs: [decoder_length, batch_size]\n",
        "        #   decoder_target_lengths: [batch_size]\n",
        "        #   This is grand truth target inputs for training.\n",
        "        decoder_inputs = tf.placeholder(tf.int32,\n",
        "                                        shape=[hparams.decoder_length, None],\n",
        "                                        name=\"decoder_inputs\")\n",
        "        decoder_target_lengths = tf.placeholder(tf.int32, shape=[None],\n",
        "                                                name=\"decoder_target_lengths\")\n",
        "\n",
        "        # https://stackoverflow.com/questions/39573188/output-projection-in-seq2seq-model-tensorflow\n",
        "        # Internally, a neural network operates on dense vectors of some size,\n",
        "        # often 256, 512 or 1024 floats (let's say 512 for here).\n",
        "        # But at the end it needs to predict a word\n",
        "        # from the vocabulary which is often much larger,\n",
        "        # e.g., 40000 words. Output projection is the final linear\n",
        "        # layer that converts (projects) from the internal\n",
        "        # representation to the larger one.\n",
        "        # So, for example, it can consist of a 512 x 40000 parameter\n",
        "        # matrix and a 40000 parameter for the bias vector.\n",
        "        projection_layer = layers_core.Dense(hparams.vocab_size, use_bias=False)\n",
        "\n",
        "        # We share this between training and inference.\n",
        "        #        decoder_cell = tf.nn.rnn_cell.BasicLSTMCell(hparams.num_units)\n",
        "        cell_list = []\n",
        "        for _ in range(hparams.num_layers):\n",
        "            cell_list.append(tf.nn.rnn_cell.BasicLSTMCell(hparams.num_units))\n",
        "        decoder_cell = tf.contrib.rnn.MultiRNNCell(cell_list)\n",
        "\n",
        "        # Greedy Inference graph\n",
        "        infer_logits, replies = self._build_greedy_inference(hparams,\n",
        "                                                             embedding_encoder,\n",
        "                                                             encoder_state,\n",
        "                                                             encoder_inputs_lengths,\n",
        "                                                             encoder_outputs,\n",
        "                                                             decoder_cell,\n",
        "                                                             projection_layer,\n",
        "                                                             self.scope)\n",
        "\n",
        "        # Beam Search Inference graph\n",
        "        beam_replies = self._build_beam_search_inference(hparams,\n",
        "                                                         encoder_inputs_lengths,\n",
        "                                                         embedding_encoder,\n",
        "                                                         encoder_state,\n",
        "                                                         encoder_outputs,\n",
        "                                                         decoder_cell,\n",
        "                                                         projection_layer,\n",
        "                                                         self.scope)\n",
        "\n",
        "        return decoder_inputs, decoder_target_lengths, replies, beam_replies, infer_logits\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ul5WBjSF3vy9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class InferenceHelper:\n",
        "    def __init__(self, model, vocab, rev_vocab):\n",
        "        self.model = model\n",
        "        self.vocab = vocab\n",
        "        self.rev_vocab = rev_vocab\n",
        "\n",
        "    def inferences(self, tweet):\n",
        "        encoder_inputs, encoder_inputs_lengths = self.create_inference_input(\n",
        "            tweet)\n",
        "        replies = self.model.infer(encoder_inputs, encoder_inputs_lengths)\n",
        "        ids = replies[0].tolist()\n",
        "        all_infer = [self.sanitize_text(self.ids_to_words(ids))]\n",
        "        beam_replies = self.model.infer_beam_search(encoder_inputs,\n",
        "                                                    encoder_inputs_lengths)\n",
        "        beam_infer = [self.sanitize_text(self.ids_to_words(beam_replies[0][:, i])) for i in range(self.model.hparams.beam_width)]\n",
        "        all_infer.extend(beam_infer)\n",
        "        return all_infer\n",
        "        \n",
        "    def sanitize_text(self, line):\n",
        "      line = re.sub(r\"\\[EOS\\]\", \" \", line)\n",
        "      line = re.sub(r\"\\[UNK\\]\", \"\", line)\n",
        "      return line\n",
        "\n",
        "    def print_inferences(self, tweet):\n",
        "        print(tweet)\n",
        "        for i, reply in enumerate(self.inferences(tweet)):\n",
        "            print(\"    [{}]{}\".format(i, reply))\n",
        "\n",
        "    def words_to_ids(self, words):\n",
        "        ids = []\n",
        "        for word in words:\n",
        "            if word in self.vocab:\n",
        "                ids.append(self.vocab[word])\n",
        "            else:\n",
        "                ids.append(self.model.hparams.unk_id)\n",
        "        return ids\n",
        "\n",
        "    def ids_to_words(self, ids):\n",
        "        words = \"\"\n",
        "        for id in ids:\n",
        "            words += self.rev_vocab[id]\n",
        "        return words\n",
        "\n",
        "    def create_inference_input(self, text):\n",
        "        inference_encoder_inputs = np.empty((self.model.hparams.encoder_length, self.model.hparams.batch_size),\n",
        "                                            dtype=np.int)\n",
        "        inference_encoder_inputs_lengths = np.empty(self.model.hparams.batch_size, dtype=np.int)\n",
        "        text = TrainDataGenerator.sanitize_line(text)\n",
        "        tagger = MeCab.Tagger(\"-Owakati\")\n",
        "        words = tagger.parse(text).split()\n",
        "        ids = self.words_to_ids(words)\n",
        "        ids = ids[:self.model.hparams.encoder_length]\n",
        "        len_ids = len(ids)\n",
        "        ids.extend([self.model.hparams.pad_id] * (self.model.hparams.encoder_length - len(ids)))\n",
        "        for i in range(self.model.hparams.batch_size):\n",
        "            inference_encoder_inputs[:, i] = np.array(ids, dtype=np.int)\n",
        "            inference_encoder_inputs_lengths[i] = len_ids\n",
        "        return inference_encoder_inputs, inference_encoder_inputs_lengths\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4U1o8gMTP1mA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class TrainDataGenerator:\n",
        "    def __init__(self, source_path, hparams):\n",
        "        self.source_path = source_path\n",
        "        self.hparams = hparams\n",
        "        basename, extension = os.path.splitext(self.source_path)\n",
        "        self.enc_path = \"{}_enc{}\".format(basename, extension)\n",
        "        self.dec_path = \"{}_dec{}\".format(basename, extension)\n",
        "        self.enc_idx_path = \"{}_enc_idx{}\".format(basename, extension)\n",
        "        self.dec_idx_path = \"{}_dec_idx{}\".format(basename, extension)\n",
        "        self.dec_idx_eos_path = \"{}_dec_idx_eos{}\".format(basename, extension)\n",
        "        self.dec_idx_sos_path = \"{}_dec_idx_sos{}\".format(basename, extension)\n",
        "        self.dec_idx_len_path = \"{}_dec_idx_len{}\".format(basename, extension)\n",
        "\n",
        "        self.enc_idx_padded_path = \"{}_enc_idx_padded{}\".format(basename,\n",
        "                                                                extension)\n",
        "        self.enc_idx_len_path = \"{}_enc_idx_len{}\".format(basename, extension)\n",
        "\n",
        "        self.vocab_path = \"{}_vocab{}\".format(basename, extension)\n",
        "        \n",
        "        self.generated_files = [self.enc_path, self.dec_path, self.enc_idx_path, self.dec_idx_path, self.dec_idx_eos_path, self.dec_idx_sos_path, self.dec_idx_len_path, self.enc_idx_padded_path, self.vocab_path, self.enc_idx_len_path]\n",
        "        self.max_vocab_size = hparams.vocab_size\n",
        "        self.start_vocabs = [hparams.sos_token, hparams.eos_token, hparams.pad_token, hparams.unk_token]\n",
        "        self.tagger = MeCab.Tagger(\"-Owakati\")\n",
        "        \n",
        "    def remove_generated(self):\n",
        "      for f in self.generated_files:\n",
        "        if os.path.exists(f):\n",
        "          os.remove(f)\n",
        "\n",
        "    def generate(self):\n",
        "        print(\"generating enc and dec files...\")\n",
        "        self._generate_enc_dec()\n",
        "        print(\"generating vocab file...\")\n",
        "        self._generate_vocab()\n",
        "        print(\"loading vocab...\")\n",
        "        vocab, _ = self._load_vocab()\n",
        "        print(\"generating id files...\")\n",
        "        self._generate_id_file(self.enc_path, self.enc_idx_path, vocab)\n",
        "        self._generate_id_file(self.dec_path, self.dec_idx_path, vocab)\n",
        "        print(\"generating padded input file...\")\n",
        "        self._generate_enc_idx_padded(self.enc_idx_path,\n",
        "                                      self.enc_idx_padded_path,\n",
        "                                      self.enc_idx_len_path,\n",
        "                                      self.hparams.encoder_length)\n",
        "        print(\"generating dec eos/sos files...\")\n",
        "        self._generate_dec_idx_eos(self.dec_idx_path, self.dec_idx_eos_path,\n",
        "                                   self.hparams.decoder_length)\n",
        "        self._generate_dec_idx_sos(self.dec_idx_path, self.dec_idx_sos_path,\n",
        "                                   self.dec_idx_len_path,\n",
        "                                   self.hparams.decoder_length)\n",
        "        print(\"done\")\n",
        "        return self._create_dataset()\n",
        "\n",
        "    def _generate_id_file(self, source_path, dest_path, vocab):\n",
        "        if gfile.Exists(dest_path):\n",
        "            return\n",
        "        with gfile.GFile(source_path, mode=\"rb\") as f, gfile.GFile(dest_path,\n",
        "                                                                   mode=\"wb\") as of:\n",
        "            for line in f:\n",
        "                line = line.decode('utf-8')\n",
        "                words = self.tagger.parse(line).split()\n",
        "                ids = [vocab.get(w, self.hparams.unk_id) for w in words]\n",
        "                of.write(\" \".join([str(id) for id in ids]) + \"\\n\")\n",
        "\n",
        "    def _load_vocab(self):\n",
        "        rev_vocab = []\n",
        "        with gfile.GFile(self.vocab_path, mode=\"r\") as f:\n",
        "            rev_vocab.extend(f.readlines())\n",
        "            rev_vocab = [line.strip() for line in rev_vocab]\n",
        "            # Dictionary of (word, idx)\n",
        "            vocab = dict([(x, y) for (y, x) in enumerate(rev_vocab)])\n",
        "            return vocab, rev_vocab\n",
        "\n",
        "    def _generate_vocab(self):\n",
        "        if gfile.Exists(self.vocab_path):\n",
        "            return\n",
        "        vocab_dic = self._build_vocab_dic(self.enc_path)\n",
        "        vocab_dic = self._build_vocab_dic(self.dec_path, vocab_dic)\n",
        "        vocab_list = self.start_vocabs + sorted(vocab_dic, key=vocab_dic.get,\n",
        "                                                reverse=True)\n",
        "        if len(vocab_list) > self.max_vocab_size:\n",
        "            vocab_list = vocab_list[:self.max_vocab_size]\n",
        "        with gfile.GFile(self.vocab_path, mode=\"w\") as vocab_file:\n",
        "            for w in vocab_list:\n",
        "                vocab_file.write(w + \"\\n\")\n",
        "\n",
        "    def _generate_enc_dec(self):\n",
        "        if gfile.Exists(self.enc_path) and gfile.Exists(self.dec_path):\n",
        "            return\n",
        "        with gfile.GFile(self.source_path, mode=\"rb\") as f, gfile.GFile(\n",
        "                self.enc_path, mode=\"w+\") as ef, gfile.GFile(self.dec_path,\n",
        "                                                             mode=\"w+\") as df:\n",
        "            tweet = None\n",
        "            reply = None\n",
        "            for i, line in enumerate(f):\n",
        "                line = line.decode('utf-8')\n",
        "                line = self.sanitize_line(line)\n",
        "                if i % 2 == 0:\n",
        "                  tweet = line\n",
        "                else:\n",
        "                  reply = line\n",
        "                  if tweet and reply:\n",
        "                    ef.write(tweet)\n",
        "                    df.write(reply)\n",
        "                  tweet = None\n",
        "                  reply = None\n",
        "\n",
        "    def _generate_enc_idx_padded(self, source_path, dest_path, dest_len_path,\n",
        "                                 max_line_len):\n",
        "        if gfile.Exists(dest_path):\n",
        "            return\n",
        "        with open(source_path) as fin, open(dest_path,\n",
        "                                            \"w\") as fout, open(dest_len_path,\n",
        "                                                               \"w\") as flen:\n",
        "            line = fin.readline()\n",
        "            while line:\n",
        "                ids = [int(x) for x in line.split()]\n",
        "                if len(ids) > max_line_len:\n",
        "#                    ids = ids[:max_line_len]\n",
        "                    ids = ids[-max_line_len:]\n",
        "                flen.write(str(len(ids)))\n",
        "                flen.write(\"\\n\")\n",
        "                if len(ids) < max_line_len:\n",
        "                    ids.extend([self.hparams.pad_id] * (max_line_len - len(ids)))\n",
        "                ids = [str(x) for x in ids]\n",
        "                fout.write(\" \".join(ids))\n",
        "                fout.write(\"\\n\")\n",
        "                line = fin.readline()\n",
        "\n",
        "    # read decoder_idx file and append eos at the end of idx list.\n",
        "    def _generate_dec_idx_eos(self, source_path, dest_path, max_line_len):\n",
        "        if gfile.Exists(dest_path):\n",
        "            return\n",
        "        with open(source_path) as fin, open(dest_path, \"w\") as fout:\n",
        "            line = fin.readline()\n",
        "            while line:\n",
        "                ids = [int(x) for x in line.split()]\n",
        "                if len(ids) > max_line_len - 1:\n",
        "#                    ids = ids[:max_line_len - 1]\n",
        "                  ids = ids[-(max_line_len - 1):]\n",
        "                ids.append(self.hparams.eos_id)\n",
        "                if len(ids) < max_line_len:\n",
        "                    ids.extend([self.hparams.pad_id] * (max_line_len - len(ids)))\n",
        "                ids = [str(x) for x in ids]\n",
        "                fout.write(\" \".join(ids))\n",
        "                fout.write(\"\\n\")\n",
        "                line = fin.readline()\n",
        "\n",
        "    # read decoder_idx file and put sos at the beginning of the idx list.\n",
        "    # also write out length of index list.\n",
        "    def _generate_dec_idx_sos(self, source_path, dest_path, dest_len_path,\n",
        "                              max_line_len):\n",
        "        if gfile.Exists(dest_path):\n",
        "            return\n",
        "        with open(source_path) as fin, open(dest_path, \"w\") as fout, open(\n",
        "                dest_len_path, \"w\") as flen:\n",
        "            line = fin.readline()\n",
        "            while line:\n",
        "                ids = [self.hparams.sos_id]\n",
        "                ids.extend([int(x) for x in line.split()])\n",
        "                if len(ids) > max_line_len:\n",
        "                    ids = ids[:max_line_len]\n",
        "                flen.write(str(len(ids)))\n",
        "                flen.write(\"\\n\")\n",
        "                if len(ids) < max_line_len:\n",
        "                    ids.extend([self.hparams.pad_id] * (max_line_len - len(ids)))\n",
        "                ids = [str(x) for x in ids]\n",
        "                fout.write(\" \".join(ids))\n",
        "                fout.write(\"\\n\")\n",
        "                line = fin.readline()\n",
        "\n",
        "    @staticmethod\n",
        "    def sanitize_line(line):\n",
        "        # replace @username\n",
        "        # replacing @username had bad impace where USERNAME token shows up everywhere.\n",
        "#        line = re.sub(r\"@([A-Za-z0-9_]+)\", \"USERNAME\", line)\n",
        "        line = re.sub(r\"@([A-Za-z0-9_]+)\", \"\", line)\n",
        "        # Remove URL\n",
        "        line = re.sub(r'https?:\\/\\/.*', \"\", line)\n",
        "        line = line.lstrip()\n",
        "        return line\n",
        "\n",
        "    @staticmethod\n",
        "    def generate_source_target_swapped(source_path):\n",
        "        basename, extension = os.path.splitext(source_path)\n",
        "        dest_path = \"{}_swapped{}\".format(basename, extension)\n",
        "        with gfile.GFile(source_path, mode=\"rb\") as fin, gfile.GFile(dest_path,\n",
        "                                                                     mode=\"w+\") as fout:\n",
        "            temp = None\n",
        "            for i, line in enumerate(fin):\n",
        "                if i % 2 == 0:\n",
        "                    temp = line\n",
        "                else:\n",
        "                    fout.write(line)\n",
        "                    fout.write(temp)\n",
        "                    temp = None\n",
        "        return dest_path\n",
        "\n",
        "    def _build_vocab_dic(self, source_path, vocab_dic={}):\n",
        "        with gfile.GFile(source_path, mode=\"r\") as f:\n",
        "            for line in f:\n",
        "                words = self.tagger.parse(line).split()\n",
        "                for word in words:\n",
        "                    if word in vocab_dic:\n",
        "                        vocab_dic[word] += 1\n",
        "                    else:\n",
        "                        vocab_dic[word] = 1\n",
        "            return vocab_dic\n",
        "\n",
        "    @staticmethod\n",
        "    def _read_file(source_path):\n",
        "        f = open(source_path)\n",
        "        data = f.read()\n",
        "        f.close()\n",
        "        return data\n",
        "\n",
        "    def _read_vocab(self, source_path):\n",
        "        rev_vocab = []\n",
        "        rev_vocab.extend(self._read_file(source_path).splitlines())\n",
        "        rev_vocab = [line.strip() for line in rev_vocab]\n",
        "        vocab = dict([(x, y) for (y, x) in enumerate(rev_vocab)])\n",
        "        return vocab, rev_vocab\n",
        "\n",
        "    def text_line_split_dataset(self, filename):\n",
        "        return tf.data.TextLineDataset(filename).map(self.split_to_int_values)\n",
        "\n",
        "    @staticmethod\n",
        "    def split_to_int_values(x):\n",
        "        return tf.string_to_number(tf.string_split([x]).values, tf.int32)\n",
        "\n",
        "    def _create_dataset(self):\n",
        "\n",
        "        tweets_dataset = self.text_line_split_dataset(self.enc_idx_padded_path)\n",
        "        tweets_lengths_dataset = tf.data.TextLineDataset(\n",
        "            self.enc_idx_len_path)\n",
        "\n",
        "        replies_sos_dataset = self.text_line_split_dataset(\n",
        "            self.dec_idx_sos_path)\n",
        "        replies_eos_dataset = self.text_line_split_dataset(\n",
        "            self.dec_idx_eos_path)\n",
        "        replies_sos_lengths_dataset = tf.data.TextLineDataset(\n",
        "            self.dec_idx_len_path)\n",
        "\n",
        "        tweets_transposed = tweets_dataset.apply(\n",
        "            tf.contrib.data.batch_and_drop_remainder(\n",
        "                self.hparams.batch_size)).map(\n",
        "            lambda x: tf.transpose(x))\n",
        "        tweets_lengths = tweets_lengths_dataset.apply(\n",
        "            tf.contrib.data.batch_and_drop_remainder(self.hparams.batch_size))\n",
        "\n",
        "        replies_with_eos_suffix = replies_eos_dataset.apply(\n",
        "            tf.contrib.data.batch_and_drop_remainder(self.hparams.batch_size))\n",
        "        replies_with_sos_prefix = replies_sos_dataset.apply(\n",
        "            tf.contrib.data.batch_and_drop_remainder(\n",
        "                self.hparams.batch_size)).map(\n",
        "            lambda x: tf.transpose(x))\n",
        "        replies_with_sos_suffix_lengths = replies_sos_lengths_dataset.apply(\n",
        "            tf.contrib.data.batch_and_drop_remainder(\n",
        "                self.hparams.batch_size))\n",
        "        vocab, rev_vocab = self._read_vocab(self.vocab_path)\n",
        "        return tf.data.Dataset.zip((tweets_transposed, tweets_lengths,\n",
        "                                    replies_with_eos_suffix,\n",
        "                                    replies_with_sos_prefix,\n",
        "                                    replies_with_sos_suffix_lengths)), vocab, rev_vocab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O5MLcyf9OVQZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class TrainDataSource:\n",
        "    def __init__(self, source_path, hparams):\n",
        "        Shell.download_file_if_necessary(source_path)\n",
        "        generator = TrainDataGenerator(source_path=source_path,\n",
        "                                       hparams=hparams)\n",
        "        # generator.remove_generated()\n",
        "        train_dataset, vocab, rev_vocab = generator.generate()\n",
        "        self.train_dataset = train_dataset.repeat().shuffle(1024)\n",
        "        # todo(higepon): Use actual validation dataset.\n",
        "        self.valid_dataset = train_dataset.repeat().shuffle(1024,\n",
        "                                                            seed=1234)\n",
        "        self.vocab = vocab\n",
        "        self.rev_vocab = rev_vocab\n",
        "\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self):\n",
        "        self.loss_step = []\n",
        "        self.val_losses = []\n",
        "        self.reward_step = []\n",
        "        self.reward_average = []\n",
        "        self.last_saved_time = datetime.datetime.now()\n",
        "        self.last_stats_time = datetime.datetime.now()\n",
        "        self.num_stats_per = 20\n",
        "        self.reward_history = []\n",
        "\n",
        "    def train_seq2seq_rl(self, seq2seq_hparams, hparams, source_path, resume):\n",
        "        tweets = [\"\", \"\", \"\"]\n",
        "        if not resume:\n",
        "          self.train_seq2seq(seq2seq_hparams, source_path, tweets, should_clean_saved_model=False)\n",
        "          Shell.copy_all_files(seq2seq_hparams.model_path, hparams.model_path)\n",
        "        with tf.device(self._available_device()):\n",
        "            _, seq2seq_model = self.create_models(seq2seq_hparams)\n",
        "            restored = seq2seq_model.restore()\n",
        "            assert (restored)\n",
        "        self.train_rl(hparams, source_path, tweets, seq2seq_model)\n",
        "\n",
        "    def train_rl(self, hparams, source_path, tweets=[], seq2seq_model=None):\n",
        "        print(\"===== Train RL {} ====\".format(source_path))\n",
        "        now = datetime.datetime.today().strftime(\"%Y%m%d%H%M%S\")\n",
        "        print(\"{}_rl_test\".format(now))\n",
        "        print(\"hparams\")\n",
        "        print_hparams(hparams)\n",
        "\n",
        "        data_source = TrainDataSource(source_path, hparams)\n",
        "        easy_tf_log.set_dir(hparams.model_path)\n",
        "        Shell.download_model_data_if_necessary(hparams.model_path)\n",
        "        device = self._available_device()\n",
        "        with tf.device(device):\n",
        "            model, infer_model = self.create_models(hparams)\n",
        "\n",
        "        vocab = data_source.vocab\n",
        "        rev_vocab = data_source.rev_vocab\n",
        "        infer_helper = InferenceHelper(model, vocab, rev_vocab)\n",
        "\n",
        "        graph = model.sess.graph\n",
        "        writer = tf.summary.FileWriter(hparams.model_path, graph)\n",
        "        last_saved_time = datetime.datetime.now()\n",
        "        with graph.as_default():\n",
        "            train_data_next = data_source.train_dataset.make_one_shot_iterator().get_next()\n",
        "\n",
        "            data = model.sess.run(train_data_next)\n",
        "            model.train(data[0], data[1], data[2], data[3], data[4])\n",
        "\n",
        "            avg_good_value = 0\n",
        "            for step in range(hparams.num_train_steps):\n",
        "                train_data = model.sess.run(train_data_next)\n",
        "                sampled_replies, _ = model.sample(train_data[0], train_data[1])\n",
        "                for i in range(hparams.batch_size):\n",
        "                  print(\"{}->{}\\n\".format(infer_helper.ids_to_words(train_data[0][:, i]), infer_helper.ids_to_words(sampled_replies[i])))\n",
        "\n",
        "                if True:\n",
        "                    dull_responses_ids = self._dull_response_ids(infer_helper)\n",
        "                    enc_inputs, enc_inputs_lengths = self._sampled_enc_inputs(\n",
        "                        model, sampled_replies)\n",
        "                    max_len = len(sampled_replies[0])\n",
        "                    # We adjust sampled_replies => enc_inputs, because we need fixed length for seq2seq.\n",
        "                    # But for reward and logits we want to need actual max_len.\n",
        "                    reward = seq2seq_model.reward_ease_of_answering(\n",
        "                        max_len,\n",
        "                        enc_inputs,\n",
        "                        enc_inputs_lengths, dull_responses_ids)\n",
        "                    print(\"reward\", reward)\n",
        "                    good_value_key = \"reward\"\n",
        "                    good_value = np.mean(reward)\n",
        "                else:\n",
        "                    good_value_key = \"good_count\"\n",
        "                    good_value, reward = self._reward_for_test(model, sampled_replies)\n",
        "\n",
        "                avg_good_value += good_value\n",
        "                if step != 0 and step % 20 == 0:\n",
        "                    print(\"{}:{}\".format(good_value_key, good_value))\n",
        "                if step != 0 and step % 60 == 0:\n",
        "                    self._print_log(good_value_key, avg_good_value / 60)\n",
        "                    avg_good_value = 0\n",
        "\n",
        "                global_step = model.train_with_reward(train_data[0],\n",
        "                                                      train_data[1],\n",
        "                                                      sampled_replies, reward)\n",
        "                if step != 0 and step % 100 == 0:\n",
        "                    print(\"save and restore\")\n",
        "                    model.save()\n",
        "                    is_restored = model.restore()\n",
        "                    assert (is_restored)\n",
        "                    is_restored = infer_model.restore()\n",
        "                    assert (is_restored)\n",
        "                    self._print_inferences(step, tweets, infer_helper, None)\n",
        "                    now = datetime.datetime.now()\n",
        "                    print(\"delta:\", (now - last_saved_time).total_seconds())\n",
        "                    last_saved_time = now\n",
        "                    assert is_restored\n",
        "                    print(\"step={}, global_step={}\".format(step, global_step))\n",
        "\n",
        "    @staticmethod\n",
        "    def _reward_for_test(model, sampled_replies):\n",
        "        max_len = len(sampled_replies[0])\n",
        "        # default negative reward\n",
        "        reward = np.ones((model.hparams.batch_size, max_len)) * -1.0\n",
        "        good_value = 0\n",
        "        for i, reply in enumerate(sampled_replies):\n",
        "            reply_len = model.input_length(reply.tolist())\n",
        "            if reply_len == 8 or reply_len == 0 or reply_len == 1:\n",
        "                for r in range(max_len):\n",
        "                    reward[i][r] = -1.0\n",
        "            else:\n",
        "                good_value += 1\n",
        "                for r in range(max_len):\n",
        "                    reward[i][r] = 1.0\n",
        "        return good_value, reward\n",
        "\n",
        "    def _dull_response_ids(self, infer_helper):\n",
        "        dull_responses = ['', \"\", \"\", \"\", \"www\",\n",
        "                          \"(-)\",\n",
        "                          \"\", \"\", \"\"]\n",
        "        dull_responses_ids = [self.tokenize(infer_helper, text) for text in\n",
        "                              dull_responses]\n",
        "        return dull_responses_ids\n",
        "\n",
        "    @staticmethod\n",
        "    def _sampled_enc_inputs(model, sampled_replies):\n",
        "        hparams = model.hparams\n",
        "        enc_inputs = []\n",
        "        enc_inputs_lengths = []\n",
        "        for reply in sampled_replies:\n",
        "            reply_len = model.input_length(reply.tolist())\n",
        "            # Safe guard: sampled reply has sometimes 0 len.\n",
        "            adjusted_len = hparams.encoder_length if reply_len == 0 else reply_len\n",
        "            enc_inputs_lengths.append(adjusted_len)\n",
        "            if reply_len <= hparams.encoder_length:\n",
        "                enc_inputs.append(np.append(reply, (\n",
        "                    [hparams.pad_id] * (\n",
        "                    hparams.encoder_length - len(reply)))))\n",
        "            else:\n",
        "                raise Exception(\n",
        "                    \"Inferred\"\n",
        "                    \" reply shouldn't be longer than encoder_input\")\n",
        "        enc_inputs = np.transpose(np.array(enc_inputs))\n",
        "        return enc_inputs, enc_inputs_lengths\n",
        "\n",
        "    def train_seq2seq_swapped(self, hparams, tweets_path, validation_tweets,\n",
        "                              should_clean_saved_model=True):\n",
        "        Shell.download_file_if_necessary(tweets_path)\n",
        "        swapped_path = TrainDataGenerator.generate_source_target_swapped(\n",
        "            tweets_path)\n",
        "        return self.train_seq2seq(hparams, swapped_path, validation_tweets,\n",
        "                           should_clean_saved_model)\n",
        "\n",
        "    def train_seq2seq(self, hparams, tweets_path, val_tweets,\n",
        "                      should_clean_saved_model=True):\n",
        "        print(\"===== Train Seq2Seq {} ====\".format(tweets_path))\n",
        "        print_hparams(hparams)\n",
        "\n",
        "        if should_clean_saved_model:\n",
        "            clean_model_path(hparams.model_path)\n",
        "        data_source = TrainDataSource(tweets_path, hparams)\n",
        "        return self._train_loop(data_source, hparams, val_tweets)\n",
        "\n",
        "    def _print_inferences(self, global_step, tweets, helper,):\n",
        "        print(\"==== {} ====\".format(global_step))\n",
        "        len_array = []\n",
        "        for tweet in tweets:\n",
        "            len_array.append(len(helper.inferences(tweet)[0]))\n",
        "            helper.print_inferences(tweet)\n",
        "        self._print_log('average reply len', np.mean(len_array))\n",
        "\n",
        "    # should this be public/private?\n",
        "    @staticmethod\n",
        "    def create_models(hparams):\n",
        "\n",
        "        # See https://www.tensorflow.org/tutorials/using_gpu#allowing_gpu_memory_growth\n",
        "        config = tf.ConfigProto(log_device_placement=False)\n",
        "        config.gpu_options.allow_growth = True\n",
        "\n",
        "        train_graph = tf.Graph()\n",
        "        train_sess = tf.Session(graph=train_graph, config=config)\n",
        "        with train_graph.as_default():\n",
        "            with tf.variable_scope('root'):\n",
        "                model = ChatbotModel(train_sess, hparams,\n",
        "                                     model_path=hparams.model_path)\n",
        "                if not model.restore():\n",
        "                    train_sess.run(tf.global_variables_initializer())\n",
        "\n",
        "        # note that infer_model is not sharing variable with training model.\n",
        "        infer_graph = tf.Graph()\n",
        "        infer_sess = tf.Session(graph=infer_graph, config=config)\n",
        "        with infer_graph.as_default():\n",
        "            with tf.variable_scope('root'):\n",
        "                infer_model = ChatbotInferenceModel(infer_sess, hparams,\n",
        "                                                    model_path=hparams.model_path)\n",
        "\n",
        "        return model, infer_model\n",
        "\n",
        "    def _train_loop(self, data_source,\n",
        "                    hparams, tweets):\n",
        "        Shell.download_model_data_if_necessary(hparams.model_path)\n",
        "\n",
        "        device = self._available_device()\n",
        "        with tf.device(device):\n",
        "            model, infer_model = self.create_models(hparams)\n",
        "\n",
        "        def my_train(**kwargs):\n",
        "            data = kwargs['train_data']\n",
        "            return model.train(data[0], data[1], data[2], data[3], data[4])\n",
        "\n",
        "        return self._generic_train_loop(data_source, hparams, infer_model,\n",
        "                                 model,\n",
        "                                 tweets, my_train)\n",
        "\n",
        "    @staticmethod\n",
        "    def _available_device():\n",
        "        device = '/cpu:0'\n",
        "        if has_gpu0():\n",
        "            device = '/gpu:0'\n",
        "            print(\"$$$ GPU ENABLED $$$\")\n",
        "        return device\n",
        "\n",
        "    @staticmethod\n",
        "    def tokenize(infer_helper, text):\n",
        "        tagger = MeCab.Tagger(\"-Owakati\")\n",
        "        words = tagger.parse(text).split()\n",
        "        return infer_helper.words_to_ids(words)\n",
        "\n",
        "    def _generic_train_loop(self, data_source, hparams, infer_model,\n",
        "                            model,\n",
        "                            tweets, train_func):\n",
        "        try:\n",
        "            return self._raw_train_loop(data_source, hparams, infer_model, model, train_func, tweets)\n",
        "        except KeyboardInterrupt as ke:\n",
        "            raise (ke)\n",
        "        except Exception as e:\n",
        "            pb.push_note(\"Train error\", str(e))\n",
        "            raise (e)\n",
        "\n",
        "    def _raw_train_loop(self, data_source, hparams,\n",
        "                        infer_model, model, train_func,\n",
        "                        tweets):\n",
        "        vocab = data_source.vocab\n",
        "        rev_vocab = data_source.rev_vocab\n",
        "        infer_helper = InferenceHelper(model, vocab, rev_vocab)\n",
        "        graph = model.sess.graph\n",
        "        with graph.as_default():\n",
        "            train_data_next = data_source.train_dataset.make_one_shot_iterator().get_next()\n",
        "            val_data_next = data_source.valid_dataset.make_one_shot_iterator().get_next()\n",
        "            easy_tf_log.set_dir(hparams.model_path)\n",
        "            writer = tf.summary.FileWriter(hparams.model_path, graph)\n",
        "            self.last_saved_time = datetime.datetime.now()\n",
        "            for i in range(hparams.num_train_steps):\n",
        "                train_data = model.sess.run(train_data_next)\n",
        "\n",
        "                step, summary = train_func(\n",
        "                    train_data=train_data,\n",
        "                )\n",
        "                writer.add_summary(summary, step)\n",
        "\n",
        "                if i != 0 and i % self.num_stats_per == 0:\n",
        "                    model.save(hparams.model_path)\n",
        "                    is_restored = infer_model.restore()\n",
        "                    assert is_restored\n",
        "                    self._print_inferences(step, tweets, infer_helper)\n",
        "                    self._compute_val_loss(step, model, val_data_next, writer)\n",
        "                    #                    self._print_stats(hparams, learning_rate)\n",
        "                    self._plot_if_necessary()\n",
        "                    self._save_model_in_drive(hparams)\n",
        "                else:\n",
        "                    print('.', end='')\n",
        "        return model, infer_model, infer_helper\n",
        "\n",
        "    def _plot_if_necessary(self):\n",
        "        if len(self.reward_average) > 0 and len(self.reward_average) % 30 == 0:\n",
        "            self._plot(self.reward_step, self.reward_average,\n",
        "                       y_label='reward average')\n",
        "            self._plot(self.loss_step, self.val_losses,\n",
        "                       y_label='validation_loss')\n",
        "\n",
        "    def _print_stats(self, hparams, learning_rate):\n",
        "        print(\"learning rate\", learning_rate)\n",
        "        delta = (\n",
        "                    datetime.datetime.now() - self.last_stats_time).total_seconds() * 1000\n",
        "        self._print_log(\"msec/data\",\n",
        "                        delta / hparams.batch_size / self.num_stats_per)\n",
        "        self.last_stats_time = datetime.datetime.now()\n",
        "\n",
        "    def _save_model_in_drive(self, hparams):\n",
        "        now = datetime.datetime.now()\n",
        "        delta_in_min = (now - self.last_saved_time).total_seconds() / 60\n",
        "\n",
        "        if delta_in_min >= 60:\n",
        "            self.last_saved_time = datetime.datetime.now()\n",
        "            Shell.save_model_in_drive(hparams.model_path)\n",
        "\n",
        "    @staticmethod\n",
        "    def _print_log(key, value):\n",
        "        tflog(\"{}[{}]\".format(key, current_client_id), value)\n",
        "        print(\"{}={}\".format(key, round(value, 1)))\n",
        "\n",
        "    @staticmethod\n",
        "    def _plot(x, y, x_label=\"step\", y_label='y'):\n",
        "        title = \"{}_{}\".format(current_client_id, y_label)\n",
        "        plt.plot(x, y, label=title)\n",
        "        plt.plot()\n",
        "        plt.ylabel(title)\n",
        "        plt.xlabel(x_label)\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "    def _compute_val_loss(self, global_step, model, val_data_next,\n",
        "                          writer):\n",
        "        val_data = model.sess.run(val_data_next)\n",
        "        val_loss, val_loss_log = model.batch_loss(val_data[0],\n",
        "                                                  val_data[1],\n",
        "                                                  val_data[2],\n",
        "                                                  val_data[3],\n",
        "                                                  val_data[4])\n",
        "        # np.float64 to native float\n",
        "        val_loss = val_loss.item()\n",
        "        writer.add_summary(val_loss_log, global_step)\n",
        "        self._print_log(\"validation loss\", val_loss)\n",
        "        self.loss_step.append(global_step)\n",
        "        self.val_losses.append(val_loss)\n",
        "        return val_loss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DQg8kU-2Dr-q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Helper functions to test\n",
        "def make_test_training_data(hparams):\n",
        "    train_encoder_inputs = np.empty(\n",
        "        (hparams.encoder_length, hparams.batch_size), dtype=np.int)\n",
        "    train_encoder_inputs_lengths = np.empty(hparams.batch_size, dtype=np.int)\n",
        "    training_target_labels = np.empty(\n",
        "        (hparams.batch_size, hparams.decoder_length), dtype=np.int)\n",
        "    training_decoder_inputs = np.empty(\n",
        "        (hparams.decoder_length, hparams.batch_size), dtype=np.int)\n",
        "\n",
        "    # We keep first tweet to validate inference.\n",
        "    first_tweet = None\n",
        "\n",
        "    for i in range(hparams.batch_size):\n",
        "        # Tweet\n",
        "        tweet = np.random.randint(low=0, high=hparams.vocab_size,\n",
        "                                  size=hparams.encoder_length)\n",
        "        train_encoder_inputs[:, i] = tweet\n",
        "        train_encoder_inputs_lengths[i] = len(tweet)\n",
        "        # Reply\n",
        "        #   Note that low = 2, as 0 and 1 are reserved.\n",
        "        reply = np.random.randint(low=2, high=hparams.vocab_size,\n",
        "                                  size=hparams.decoder_length - 1)\n",
        "\n",
        "        training_target_label = np.concatenate(\n",
        "            (reply, np.array([hparams.eos_id])))\n",
        "        training_target_labels[i] = training_target_label\n",
        "\n",
        "        training_decoder_input = np.concatenate(([hparams.sos_id], reply))\n",
        "        training_decoder_inputs[:, i] = training_decoder_input\n",
        "\n",
        "        if i == 0:\n",
        "            first_tweet = tweet\n",
        "            info(\"0th tweet={}\".format(tweet), hparams)\n",
        "            info(\"0th reply_with_eos_suffix={}\".format(training_target_label),\n",
        "                 hparams)\n",
        "            info(\"0th reply_with_sos_prefix={}\".format(training_decoder_input),\n",
        "                 hparams)\n",
        "\n",
        "        info(\"Tweets\", hparams)\n",
        "        info(train_encoder_inputs, hparams)\n",
        "        info(\"Replies\", hparams)\n",
        "        info(training_target_labels, hparams)\n",
        "        info(training_decoder_inputs, hparams)\n",
        "    return first_tweet, train_encoder_inputs, train_encoder_inputs_lengths, training_target_labels, training_decoder_inputs\n",
        "\n",
        "\n",
        "def test_training(test_hparams, model, infer_model):\n",
        "    if test_hparams.use_attention:\n",
        "        print(\"==== training model[attention] ====\")\n",
        "    else:\n",
        "        print(\"==== training model ====\")\n",
        "    first_tweet, train_encoder_inputs, train_encoder_inputs_lengths, training_target_labels, training_decoder_inputs = make_test_training_data(\n",
        "        test_hparams)\n",
        "    for i in range(test_hparams.num_train_steps):\n",
        "        _ = model.train(train_encoder_inputs,\n",
        "                        train_encoder_inputs_lengths,\n",
        "                        training_target_labels,\n",
        "                        training_decoder_inputs,\n",
        "                        np.ones(test_hparams.batch_size,\n",
        "                                dtype=int) * test_hparams.decoder_length)\n",
        "        if i % 5 == 0 and test_hparams.debug_verbose:\n",
        "            print('.', end='')\n",
        "\n",
        "        if i % 15 == 0:\n",
        "            model.save()\n",
        "\n",
        "    inference_encoder_inputs = np.empty((test_hparams.encoder_length, 1),\n",
        "                                        dtype=np.int)\n",
        "    inference_encoder_inputs_lengths = np.empty(1, dtype=np.int)\n",
        "    for i in range(1):\n",
        "        inference_encoder_inputs[:, i] = first_tweet\n",
        "        inference_encoder_inputs_lengths[i] = len(first_tweet)\n",
        "\n",
        "    # testing \n",
        "    log_prob54 = infer_model.log_prob(inference_encoder_inputs,\n",
        "                                      inference_encoder_inputs_lengths,\n",
        "                                      np.array([5, 4]))\n",
        "    log_prob65 = infer_model.log_prob(inference_encoder_inputs,\n",
        "                                      inference_encoder_inputs_lengths,\n",
        "                                      np.array([6, 5]))\n",
        "    print(\"log_prob for 54\", log_prob54)\n",
        "    print(\"log_prob for 65\", log_prob65)\n",
        "\n",
        "    reward = infer_model.reward_ease_of_answering(test_hparams.encoder_length,\n",
        "                                                  inference_encoder_inputs,\n",
        "                                                  inference_encoder_inputs_lengths,\n",
        "                                                  np.array([[5], [6]]))\n",
        "    print(\"reward=\", reward)\n",
        "\n",
        "    if test_hparams.debug_verbose:\n",
        "        print(inference_encoder_inputs)\n",
        "    replies = infer_model.infer(inference_encoder_inputs,\n",
        "                                inference_encoder_inputs_lengths)\n",
        "    print(\"Infered replies\", replies[0])\n",
        "    print(\"Expected replies\", training_target_labels[0])\n",
        "\n",
        "# is this used?\n",
        "def create_train_infer_models(graph, sess, hparams, force_restore=False):\n",
        "    with graph.as_default():\n",
        "        with tf.variable_scope('root'):\n",
        "            model = ChatbotModel(sess, hparams, model_path=hparams.model_path)\n",
        "\n",
        "        with tf.variable_scope('root', reuse=True):\n",
        "            infer_model = ChatbotInferenceModel(sess, hparams,\n",
        "                                                model_path=hparams.model_path)\n",
        "            restored = model.restore()\n",
        "            if not restored:\n",
        "                if force_restore:\n",
        "                    raise Exception(\"Oops, couldn't restore\")\n",
        "                else:\n",
        "                    sess.run(tf.global_variables_initializer())\n",
        "        return model, infer_model\n",
        "\n",
        "\n",
        "def test_distributed_pattern(hparams):\n",
        "    for d in [hparams.model_path]:\n",
        "        shutil.rmtree(d, ignore_errors=True)\n",
        "        os.makedirs(d, exist_ok=True)\n",
        "\n",
        "    print('==== test_distributed_pattern[{} {}] ===='.format(\n",
        "        'attention' if hparams.use_attention else '',\n",
        "        'beam' if hparams.beam_width > 0 else ''))\n",
        "\n",
        "    first_tweet, train_encoder_inputs, train_encoder_inputs_lengths, training_target_labels, training_decoder_inputs = make_test_training_data(\n",
        "        hparams)\n",
        "\n",
        "    model, infer_model = Trainer().create_models(hparams)\n",
        "\n",
        "    for i in range(hparams.num_train_steps):\n",
        "        _ = model.train(train_encoder_inputs,\n",
        "                        train_encoder_inputs_lengths,\n",
        "                        training_target_labels,\n",
        "                        training_decoder_inputs,\n",
        "                        np.ones(hparams.batch_size,\n",
        "                                dtype=int) * hparams.decoder_length)\n",
        "\n",
        "    model.save()\n",
        "\n",
        "    inference_encoder_inputs = np.empty((hparams.encoder_length, 1),\n",
        "                                        dtype=np.int)\n",
        "    inference_encoder_inputs_lengths = np.empty(1, dtype=np.int)\n",
        "\n",
        "    inference_encoder_inputs[:, 0] = first_tweet\n",
        "    inference_encoder_inputs_lengths[0] = len(first_tweet)\n",
        "\n",
        "    infer_model.restore()\n",
        "    replies = infer_model.infer(inference_encoder_inputs,\n",
        "                                    inference_encoder_inputs_lengths)\n",
        "    print(\"Inferred replies\", replies[0])\n",
        "        \n",
        "    beam_replies = infer_model.infer_beam_search(inference_encoder_inputs,\n",
        "                                                 inference_encoder_inputs_lengths)\n",
        "    print(\"Inferred replies candidate0\", beam_replies[0][:, 0])\n",
        "    print(\"Inferred replies candidate1\", beam_replies[0][:, 1])\n",
        "        \n",
        "    inference_encoder_inputs = np.empty((hparams.encoder_length, hparams.batch_size),\n",
        "                                        dtype=np.int)\n",
        "    inference_encoder_inputs_lengths = np.empty(hparams.batch_size, dtype=np.int)\n",
        "\n",
        "    for i in range(hparams.batch_size):\n",
        "      inference_encoder_inputs[:, i] = first_tweet\n",
        "      inference_encoder_inputs_lengths[i] = len(first_tweet)\n",
        "      \n",
        "    replies = model.sample(inference_encoder_inputs,\n",
        "                                                   inference_encoder_inputs_lengths)\n",
        "    print(\"sample replies\", replies[0])        \n",
        "    print(\"Expected replies\", training_target_labels[0])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9oDXz_ZCNM1F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def test_distributed_one(enable_attention):\n",
        "    hparams = copy.deepcopy(test_hparams).override_from_dict({\n",
        "        'model_path': ModelDirectory.test_distributed.value,\n",
        "        'use_attention': enable_attention,\n",
        "        'beam_width': 2,\n",
        "    })\n",
        "    test_distributed_pattern(hparams)\n",
        "\n",
        "\n",
        "if mode == Mode.Test:\n",
        "    test_distributed_one(enable_attention=False)\n",
        "    test_distributed_one(enable_attention=True)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JrvS_DURF5Pq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def clean_model_path(model_path):\n",
        "    shutil.rmtree(model_path)\n",
        "    os.makedirs(model_path)\n",
        "\n",
        "\n",
        "def print_header(text):\n",
        "    print(\"============== {} ==============\".format(text))\n",
        "\n",
        "\n",
        "def test_tweets_small_swapped(hparams):\n",
        "    replies = [\"@higepon \", \"\", \"\"]\n",
        "    trainer = Trainer()\n",
        "    trainer.train_seq2seq_swapped(hparams, \"tweets_small.txt\", replies)\n",
        "\n",
        "\n",
        "# vocab size \n",
        "tweet_small_hparams = copy.deepcopy(base_hparams).override_from_dict(\n",
        "    {\n",
        "        'batch_size': 6,  # of tweets should be dividable by batch_size\n",
        "        'encoder_length': 8,\n",
        "        'decoder_length': 8,\n",
        "        'num_units': 256,\n",
        "        'num_layers': 2,\n",
        "        'vocab_size': 34,\n",
        "        'embedding_size': 40,\n",
        "        'beam_width': 2,  # for faster iteration, this should be 10\n",
        "        'num_train_steps': 200,\n",
        "        'model_path': ModelDirectory.tweet_small.value,\n",
        "        'learning_rate': 0.05,\n",
        "        'use_attention': True,\n",
        "    })\n",
        "\n",
        "tweet_small_swapped_hparams = copy.deepcopy(\n",
        "    tweet_small_hparams).override_from_dict(\n",
        "    {'model_path': ModelDirectory.tweet_small_swapped.value})\n",
        "\n",
        "if mode == Mode.Test:\n",
        "    tweets_path = \"tweets_small.txt\"\n",
        "    TrainDataGenerator(tweets_path, tweet_small_hparams).remove_generated()\n",
        "    trainer = Trainer()\n",
        "    trainer.train_seq2seq(tweet_small_hparams, tweets_path,\n",
        "                          [\"\", \"\", \"\"])\n",
        "    test_tweets_small_swapped(tweet_small_swapped_hparams)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tzh2rhEPguJ9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def test_tweets_large(hparams):\n",
        "    tweets = [\"\", \"\", \"\",\n",
        "              \"\", \"\", \"\", \"\",\n",
        "              \"\", \"\", \"\"]\n",
        "    trainer = Trainer()\n",
        "    trainer.train_seq2seq(hparams, \"tweets_conversation.txt\", tweets,\n",
        "                          should_clean_saved_model=False)\n",
        "    return trainer.model, trainer.infer_model\n",
        "\n",
        "\n",
        "def test_tweets_large_swapped(hparams):\n",
        "    tweets = [\"\", \"\", \"\", \"\",\n",
        "              \"\", \"\", \"\"]\n",
        "    trainer = Trainer()\n",
        "    trainer.train_seq2seq_swapped(hparams, \"tweets_large.txt\", tweets,\n",
        "                                  should_clean_saved_model=False)\n",
        "    return trainer.model, trainer.infer_model\n",
        "\n",
        "\n",
        "tweet_large_hparams = copy.deepcopy(base_hparams).override_from_dict(\n",
        "    {\n",
        "        # In typical seq2seq chatbot\n",
        "        # num_layers=3, learning_rate=0.5, batch_size=64, vocab=20000-100000, learning_rate decay is 0.99, which is taken care as default parameter in AdamOptimizer.\n",
        "        'batch_size': 64,  # of tweets should be dividable by batch_size\n",
        "        'encoder_length': 28,\n",
        "        'decoder_length': 28,\n",
        "        'num_units': 1024,\n",
        "        'num_layers': 3,\n",
        "        'vocab_size': 60000,\n",
        "    # conversations.txt actually has about 70K uniq words.\n",
        "        'embedding_size': 1024,\n",
        "        'beam_width': 2,  # for faster iteration, this should be 10\n",
        "        'num_train_steps': 1000000,\n",
        "        'model_path': ModelDirectory.tweet_large.value,\n",
        "        'learning_rate': 0.5,\n",
        "    # For vocab_size 50000, num_layers 3, num_units 1024, tweet_large, starting learning_rate 0.05 works well, change it t0 0.01 at perplexity 800, changed it to 0.005 at 200.\n",
        "        'learning_rate_decay': 0.99,\n",
        "        'use_attention': True,\n",
        "        # testing new restore learning rate and no USERNAME TOKEN\n",
        "    })\n",
        "\n",
        "tweet_large_swapped_hparams = copy.deepcopy(\n",
        "    tweet_large_hparams).override_from_dict(\n",
        "    {\n",
        "        'model_path': ModelDirectory.tweet_large_swapped.value\n",
        "    })\n",
        "\n",
        "#Shell.save_model_in_drive(tweet_large_hparams.model_path)\n",
        "\n",
        "if mode == Mode.TrainSeq2Seq:\n",
        "    print(\"train seq2seq\")\n",
        "    test_tweets_large(tweet_large_hparams)\n",
        "elif mode == Mode.TrainSeq2SeqSwapped:\n",
        "    print(\"train seq2seq swapped\")\n",
        "    test_tweets_large_swapped(tweet_large_swapped_hparams)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aAv3l15jQl4z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "small_hparams = copy.deepcopy(tweet_small_hparams).override_from_dict({\n",
        "    'learning_rate': 0.1,\n",
        "    'batch_size': 16,\n",
        "    'num_train_steps': 200,\n",
        "})\n",
        "\n",
        "rl_small_hparams = copy.deepcopy(tweet_small_hparams).override_from_dict({\n",
        "    'learning_rate': 0.1,\n",
        "    'batch_size': 16,\n",
        "    'num_train_steps': 2000,\n",
        "    'model_path': ModelDirectory.tweet_small_rl.value\n",
        "})\n",
        "\n",
        "if mode == Mode.TrainRL:\n",
        "  with memory_util.capture_stderr() as stderr:\n",
        "      try:\n",
        "          trainer = Trainer()\n",
        "          trainer.train_seq2seq_rl(small_hparams, rl_small_hparams, \"tweets_small.txt\", resume=False)\n",
        "      except Exception as e:\n",
        "        print(stderr.getvalue())\n",
        "        raise (e)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4ZaAyc29H9Ww",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if mode == Mode.TrainRL:\n",
        "  Shell.download_logs(rl_small_hparams.model_path)\n",
        "  !rm $small_hparams.model_path/*\n",
        "  !ls -lSh $small_hparams.model_path\n",
        "  !rm $rl_small_hparams.model_path/*\n",
        "  !ls -lSh $rl_small_hparams.model_path"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mmOPEDqwIJWE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with memory_util.capture_stderr() as stderr:\n",
        "    try:\n",
        "        trainer = Trainer()\n",
        "        tweets_path = \"tweets_small.txt\"\n",
        "        val_tweets = [\"\", \"\", \"\"]\n",
        "        model, infer_model, infer_helper = trainer.train_seq2seq(tweet_small_hparams,\n",
        "                                                   tweets_path,\n",
        "                                                   val_tweets)\n",
        "        swapped_val_tweets = [\"@higepon \", \"\",\n",
        "                              \"\"]\n",
        "        b_model, b_infer_model, b_infer_helper = trainer.train_seq2seq_swapped(\n",
        "            tweet_small_swapped_hparams,\n",
        "            tweets_path, swapped_val_tweets)\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(stderr.getvalue())\n",
        "        raise (e)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YLSXZRWCMvx5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with memory_util.capture_stderr() as stderr:\n",
        "    try:\n",
        "#      infer_model.infer_beam_search(encoder_inputs, encoder_inputs_lengths)\n",
        "      print(infer_helper.inferences(\"\"))\n",
        "      \n",
        "\n",
        "    except Exception as e:\n",
        "        print(stderr.getvalue())\n",
        "        raise (e)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WiazM73iU1hK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "large_hparams = copy.deepcopy(tweet_large_hparams).override_from_dict({\n",
        "    'num_train_steps': 40000,\n",
        "})\n",
        "\n",
        "rl_large_hparams = copy.deepcopy(tweet_large_hparams).override_from_dict({\n",
        "    'num_train_steps': 20000,\n",
        "    'model_path': ModelDirectory.tweet_large_rl.value\n",
        "})\n",
        "\n",
        "if mode == Mode.TrainRL:\n",
        "  with memory_util.capture_stderr() as stderr:\n",
        "    try:\n",
        "        trainer = Trainer()\n",
        "        trainer.train_seq2seq_rl(large_hparams, rl_large_hparams, \"tweets_large.txt\", resume=False)\n",
        "    except KeyboardInterrupt:       \n",
        "      print(stderr.getvalue())      \n",
        "    except Exception as e:\n",
        "      print(stderr.getvalue())\n",
        "      raise (e)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GHMFjv2Nbze3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!cp model/tweet_small/* drive/\n",
        "#!ls model/tweet_small\n",
        "#!cp $rl_small_hparams.model_path/* drive/\n",
        "Shell.download_logs(rl_large_hparams.model_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2ZDb62hiz7r4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!rm tweets_large.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yimhfKo9BXG5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if mode == Mode.Test:\n",
        "    Shell.download_logs(ModelDirectory.tweet_small_rl.value)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T_q-Ns9hiHMB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# N.B: This would fail if we try to download logs in the previous cell.\n",
        "# My guess is tflog is somehow locking the log file when running the cell.\n",
        "#download_logs()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hxPwNy-70_9X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class StreamListener(tweepy.StreamListener):\n",
        "    def __init__(self, api, helper):\n",
        "        self.api = api\n",
        "        self.helper = helper\n",
        "\n",
        "    def on_status(self, status):\n",
        "        # done handle @reply only\n",
        "        # done print reply\n",
        "        # add model parameter\n",
        "        # direct reply\n",
        "        # unk reply\n",
        "        # shuffle beam search\n",
        "        print(\"{0}: {1}\".format(status.text, status.author.screen_name))\n",
        "\n",
        "        screen_name = status.author.screen_name\n",
        "        # ignore my tweets\n",
        "        if screen_name == self.api.me().screen_name:\n",
        "            print(\"Ignored my tweet\")\n",
        "            return True\n",
        "        elif status.text.startswith(\"@{0}\".format(self.api.me().screen_name)):\n",
        "\n",
        "            replies = self.helper.inferences(status.text)\n",
        "            reply = random.choice(replies)\n",
        "            reply = \"@\" + status.author.screen_name + \" \" + reply\n",
        "            print(reply)\n",
        "            self.api.update_status(status=reply,\n",
        "                                   in_reply_to_status_id=status.id)\n",
        "\n",
        "            return True\n",
        "\n",
        "    @staticmethod\n",
        "    def on_error(status_code):\n",
        "        print(status_code)\n",
        "        return True\n",
        "\n",
        "\n",
        "def listener(hparams):\n",
        "    Shell.download_model_data_if_necessary(hparams.model_path)\n",
        "\n",
        "    rl_train_graph = tf.Graph()\n",
        "    rl_infer_graph = tf.Graph()\n",
        "    rl_train_sess = tf.Session(graph=rl_train_graph)\n",
        "    rl_infer_sess = tf.Session(graph=rl_infer_graph)\n",
        "\n",
        "    _, infer_model = create_train_infer_models_in_graphs(rl_train_graph,\n",
        "                                                         rl_train_sess,\n",
        "                                                         rl_infer_graph,\n",
        "                                                         rl_infer_sess,\n",
        "                                                         hparams)\n",
        "\n",
        "    source_path = \"tweets_large.txt\"\n",
        "    Shell.download_file_if_necessary(source_path)\n",
        "    generator = TrainDataGenerator(source_path=source_path, hparams=hparams)\n",
        "    _, vocab, rev_vocab = generator.generate()\n",
        "    infer_model.restore()\n",
        "    helper = InferenceHelper(infer_model, vocab, rev_vocab)\n",
        "\n",
        "    config_path = 'config.yml'\n",
        "    Shell.download_file_if_necessary(config_path)\n",
        "    f = open(config_path, 'rt')\n",
        "    cfg = yaml.load(f)['twitter']\n",
        "\n",
        "    consumer_key = cfg['consumer_key']\n",
        "    consumer_secret = cfg['consumer_secret']\n",
        "    access_token = cfg['access_token']\n",
        "    access_token_secret = cfg['access_token_secret']\n",
        "\n",
        "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "    auth.set_access_token(access_token, access_token_secret)\n",
        "    api = tweepy.API(auth)\n",
        "\n",
        "    while True:\n",
        "        #    try:\n",
        "        stream = tweepy.Stream(auth=api.auth,\n",
        "                               listener=StreamListener(api, helper))\n",
        "        print(\"listener starting...\")\n",
        "        stream.userstream()\n",
        "#    except Exception as e:\n",
        "#     print(e.__doc__)\n",
        "\n",
        "\n",
        "tweet_hparams = copy.deepcopy(rl_dst_hparams).override_from_dict(\n",
        "    {'beam_width': 50})\n",
        "if mode == Mode.TweetBot:\n",
        "    listener(tweet_hparams)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}